{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation for POMMES\n",
    "This notebook contains the major routines used to prepare and provide input data for the fundamental power market model _POMMES_ <br>(<b>Po</b>wer <b>M</b>arket <b>M</b>odel <b>E</b>nergy and re<b>S</b>ources).\n",
    "\n",
    "It provides input data for (oemof.solph components used for modeling given in brackets)\n",
    "* conventional power plants (Transformers)\n",
    "* interconnectors (Transformers; formerly Links)\n",
    "* storages (GenericStorages)\n",
    "* electric vehicles (Transformers, GenericStorages, Buses & Sinks)\n",
    "* commodity sources (Sources)\n",
    "* renewable generators (Sources / Transformers)\n",
    "* power demand (Sinks)\n",
    "\n",
    "Output:\n",
    "* A `pommesdispatch` data set for any year between 2017 and 2030 can be created.\n",
    "* Also, a `pommesinvest` data set is created, spanning the time horizon from 2020 to 2050.\n",
    "\n",
    "Time-related information:\n",
    "* For future years, power plant commissionings and decommissionings are taken into account and RES are expanded by a given pathway.\n",
    "* Note that so far, we have included **time series information only for 2017**. For any other year, 2017 time series are used and simply reindexed. To have a comparable simulation year with 8760 hours, for leap years, the last day is simply neglected.\n",
    "* Also, UTC time stamps are used thus ignoring time shift.\n",
    "\n",
    "Overview on the document:\n",
    "> **Outline:** The correspondend elements, named components, used in the framework [oemof.solph](https://github.com/oemof/oemof-solph) determine the outline of the given notebook.\n",
    "> _NOTE: It is recommended to run the notebook sequentially since variables declared earlier may be needed later on, even in later sections._\n",
    "\n",
    "> **Raw data sources:** The raw data is taken from different sources, mostly [OPSD](https://open-power-system-data.org/) and [ENTSO-E](https://transparency.entsoe.eu/). The major sources and assumptions are documented for the respective power system elements (components) in the following. For power plant projections, data from the lastest approved network development plan for electricity for Germany [NEP Strom 2030 as of 2019](https://www.netzentwicklungsplan.de/de/netzentwicklungsplaene/netzentwicklungsplan-2030-2019) as well as data from the [TYNDP 2018](https://tyndp.entsoe.eu/maps-data) of ENTSO-E for are used.<br>\n",
    "An overview on the data licensing information can be found in the repository.\n",
    "\n",
    "> **Tasks:** Besides extraction, cleaning and combination of data from the sources and assumptions, significant parts of the notebooks are routines for transferring the data to a data format that can be handled by the framework [oemof.solph](https://github.com/oemof/oemof-solph) which serves as a basis for the model _POMMES_.\n",
    "\n",
    "> **Authors:** Corresponding authors: Yannick Werner, Johannes Kochems<br>Contributors: Leticia Encinas Rosa, Carla Spiller, Sophie Westphal, Julian Endres, Julien Faist, Timona Ghosh, Johannes Giehl, Christian Fraatz, Robin Claus, Daniel Peschel, Conrad Nicklisch, Benjamin Grosse, Joachim Müller-Kirchenbauer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Package imports and settings\n",
    "\n",
    "## Imports\n",
    "\n",
    "User defined modules used:\n",
    "* `tools`: a set of functions holding assisting routines; comprises functions for calculating (shortest) distances based on given GPS coordinates, for loading bidding zone shapes as well as ENTSO-E data, for assigning efficiencies as well as gradients and minimum loads and for setting NTC values\n",
    "* `eeg_transformers`: functions for modeling RES clusters bidding at the negative market premium (routines for reading in data, data aggregation, clustering and assigning market values)\n",
    "* `transformers_aggregation`: functions for clustering conventional power plants\n",
    "* `hydro`: functions to load, preprocess and upsample hydro generation and filling rate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import seaborn as sns\n",
    "\n",
    "from re import search, IGNORECASE\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import data_prep.tools as tools\n",
    "import data_prep.eeg_transformers as eeg\n",
    "import data_prep.transformer_aggregation as tf_agg\n",
    "import data_prep.hydro as hydro\n",
    "import data_prep.helpers as helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Paths to input files\n",
    "main_path = {\n",
    "    'inputs': '../raw_data_input/',\n",
    "    'outputs': '../prepared_data/'\n",
    "}\n",
    "\n",
    "sub_path = {\n",
    "    'interconnectors': 'interconnectors/',\n",
    "    'pp_public': 'powerplants_public/',\n",
    "    'pp_er': 'powerplants_er/',\n",
    "    'storages': 'storages/',\n",
    "    'timeseries': 'timeseries/',\n",
    "    'hydro': 'hydro/',\n",
    "    'renewables': 'renewables/',\n",
    "    'marketzones': 'market_zones_scandinavia/',\n",
    "    'costs': 'costs/',\n",
    "    'assumptions': 'assumptions/',\n",
    "    'demand': 'demand/',\n",
    "    'electric_vehicles': 'electric_vehicles',\n",
    "}\n",
    "\n",
    "# Filenames of input files\n",
    "input_file = {\n",
    "    'opsd_de': 'KWlisteDE_OPSD_20191002.csv',\n",
    "    'opsd_de_new': 'conventional_power_plants_DE.csv',\n",
    "    'ppmatching_eu': 'powerplants_conventional_eu.csv',\n",
    "    'ppmatching_eu_new': 'powerplants_conventional_eu_20210105.csv',\n",
    "    'tyndp_eu': 'ENTSO Scenario 2018 Generation Capacities.xlsm',\n",
    "    'tyndp_2022': 'ENTSOE_220310_Updated_Electricity_Modelling_Results.xlsx',\n",
    "    'nep_de': 'KWliste_NEP2030_20191022.xlsx',\n",
    "    'techs_de': 'Technologies_ManRes_20170212.csv',\n",
    "    'peschel': 'KWliste_DP_20160304.csv',\n",
    "    'eff_de': 'Efficiencies_ManRes_20180918.csv',\n",
    "    'comm': 'transformers_commissioning.xlsx',\n",
    "    'decomm': 'transformers_decommissioning.csv',\n",
    "    'comm_decomm_BNetzA': 'BNetzA_Veroeff_ZuUndRueckbau_20200404.xlsx',\n",
    "    'KWSAL': 'KWSAL_20200415.csv',\n",
    "    'decomm_hardcoal': 'transformers_decommissioning_hardcoal.csv',\n",
    "    'decomm_lignite': 'transformers_decommissioning_lignite.csv',\n",
    "    'hardcoal_tender_1': 'Liiste_Zuschlaege_01092020_1.xlsx',\n",
    "    'hardcoal_tender_2': 'Liste_Zuschlaege_Jan2021.xlsx',\n",
    "    'hardcoal_tender_3': 'Liste_Zuschlaege_April2021.xlsx',\n",
    "    'hardcoal_tender_4': 'Liste_Zuschlaege_Oktober2021.xlsx',\n",
    "    'hardcoal_tender_5': 'ListeZuschlaegeMaerz2022.xlsx',\n",
    "    'availability': 'availability.csv',\n",
    "    'gradients': 'max_gradient_assumptions.csv',\n",
    "    'new_built': 'transformers_new-built.csv',\n",
    "    'interconnectors_ttc': 'interconnectors_ttc.csv',\n",
    "    'interconnectors_tech_profiles': 'interconnectors_tech_profiles.csv',\n",
    "    'interconnectors_timeseries': 'Interconnectors_timeseries_20191028.csv',\n",
    "    'renewables_nonfluctuating': 'capacity_factors_nonfluc_16082020.csv',\n",
    "    'eeg_powerplants': 'ee_powerplants_29102021.csv',\n",
    "    'market_values': 'netztransparenz_market_values_2017.csv',\n",
    "    'REcap_eu': 'REcapacitiesEU_IRENA_20201120.csv',\n",
    "    'REcap_DK': 'DK_capacities_20191120.csv',\n",
    "    'RES_DE_Prognos': 'Prognos_et_al_2020_RES_data.csv',\n",
    "    'RES_DE_EEG_2021': 'EEG_2021_RES_capacity_targets.csv',\n",
    "    'RES_DE_EEG_2023': 'EEG_2023_RES_capacity_targets.csv',\n",
    "    'onshore_tenders': 'Statistik_Onshore_20220107.xlsx',\n",
    "    'solar_tenders': 'Statistik_Solar_20220107.xlsx',\n",
    "    'solar_tenders2': 'Statistik_Solar2_20220107.xlsx',\n",
    "    'common_tenders': 'Statistik_GemAV_20210716.xlsx',\n",
    "    'PV_2021_01': 'PV_DegressionsVergSaetze_11-01_21.xlsx',\n",
    "    'PV_2021_02': 'PV_DegressionsVergSaetze_02-04_21.xlsx',\n",
    "    'PV_2021_03': 'PV_DegressionsVergSaetze_05-07_21.xlsx',\n",
    "    'RES_cost_ISE': 'RES_cost_projections_ISE.csv',\n",
    "    'RES_market_development' : 'RES_market_development.csv',\n",
    "    'RES_costs_ISE21' : 'RES_cost_projections_ISE21.csv',\n",
    "    'operation_costs_assumptions': 'operation_costs_assumptions.csv',\n",
    "    'markup_assumptions': 'markup_assumptions.csv',\n",
    "    'opsd_timeseries': 'Timeseries_OPSD_20191028.csv',\n",
    "    'entsoe_generation_de': 'entsoe_generation_DE_22072020.csv',\n",
    "    'efficiencies_el': 'efficiencies_el.csv',\n",
    "    'phes_assumptions': 'phes_assumptions.csv',\n",
    "    'tech_assumptions': 'tech_assumptions.csv',\n",
    "    'emf': 'Emissionfactors_20200807.csv',\n",
    "    'GHG_emissions': 'GHG_emissions.csv',\n",
    "    'transformers_investment_options': 'transformers_investment_options.csv',\n",
    "    'storages_el_investment_options': 'storages_el_investment_options.csv',\n",
    "    'ev_fleet_assumptions': 'ev_fleet_assumptions.csv',\n",
    "    'ev_efficiency_assumptions': 'ev_efficiency_assumptions.csv',\n",
    "    'cc_avail': 'vencopy_ChargeAvail_2050.csv',\n",
    "    'soc_lower': 'vencopy_BatMin_2050.csv',\n",
    "    'soc_upper': 'vencopy_BatMax_2050.csv',\n",
    "    'uc_avail': 'vencopy_UncontrCharge_2050.csv',\n",
    "    'driving': 'vencopy_DrivPower_2050.csv',\n",
    "    'energiedaten': 'energiedaten-gesamt-xls-2022.xlsx',\n",
    "    'transportation_costs': 'transportation_cost_assumptions.csv',\n",
    "    'fuel_cost_assumptions': 'fuel_cost_assumptions.csv',\n",
    "    'prices_iea_weo': 'prices_IEA_WEO_2021.xlsx',\n",
    "    'prices_ewi': 'ewi_2022_Energiepreisszenarien.xlsx',\n",
    "    'monthly_price_trends': 'energy-price-trends-xlsx-2022.xlsx',\n",
    "    'emissions_costs': 'emission-spot-primary-market-auction-report',\n",
    "    'emission_costs_projection': 'co2_prices.csv',\n",
    "    'emission_costs_long-term_projection': 'co2_prices_pietzcker_et_al_2021.csv',\n",
    "    'cost_projections_nicklisch': 'cost_projections_nicklisch.xlsx',\n",
    "    'investment_costs_unseen': 'investment_cost_assumptions_UNSEEN_various_sources.csv',\n",
    "    'costs_assumptions_pypsa_eur': 'PyPSA-EUR_costs.csv',\n",
    "    'pietzcker_technology_assumptions': 'Pietzcker_et_al_2021_technology_assumptions.xlsx',\n",
    "    'ise_technology_assumptions': 'ISE_2020_assumptions_technology_costs.xlsx',\n",
    "    'dieterpy_data': 'dieterpy_data_input.xlsx',\n",
    "    'flexmex_data': 'FlexMex_Scalars_ALL.csv',\n",
    "    'unseen_data': 'investment_cost_assumptions_UNSEEN_various_sources.csv',\n",
    "    'unseen_fixed_costs_data': 'fixed_cost_assumptions_UNSEEN_various_sources.csv',\n",
    "    'unseen_variable_costs_data': 'variable_cost_assumptions_UNSEEN_various_sources.csv',\n",
    "    'pypsa_eur': 'PyPSA-EUR_costs.csv',\n",
    "    'wacc': 'wacc_polzin_2021.csv',\n",
    "    'hurdle_rates': 'NERA_2015_hurdle_rates.xlsx',\n",
    "    'natgas_futures': 'OTC-Gas-THE Kalender',\n",
    "    'hardcoal_futures': 'Kohle-API2-',\n",
    "    'oil_futures': 'Brent_Crude_Oil_Futures',\n",
    "    'hydrogen_sprinters': 'hydrogen_sprinters.csv',\n",
    "    'KNS2035_power_consumption': 'Prognos_power_consumption_projection.csv',\n",
    "    'demand_response_baseline_profile': 'sinks_demand_response_el_ts_',\n",
    "    'demand_response_ava_pos': 'sinks_demand_response_el_ava_pos_ts_',\n",
    "    'demand_response_ava_neg': 'sinks_demand_response_el_ava_neg_ts_',\n",
    "    'demand_response_parameters': '_potential_parameters_',\n",
    "    'entsoe_national_demand_estimate': 'Demand_TimeSeries_2030_NationalEstimates.xlsx',\n",
    "    'sources_el_artificial': 'sources_el_artificial',\n",
    "}\n",
    "\n",
    "# Filenames of output files (used as model input)\n",
    "output_file = {\n",
    "    \"buses\": \"buses\",\n",
    "    \"dh_gradients_ts\": \"dh_gradients_ts\",\n",
    "    \"ipp_gradients_ts\": \"ipp_gradients_ts\",\n",
    "    \"remaining_gradients_ts\": \"remaining_gradients_ts\",\n",
    "    \"linking_transformers\": \"linking_transformers\",\n",
    "    \"linking_transformers_ts\": \"linking_transformers_ts\",\n",
    "    \"sources_fluc_res\": \"sources_fluc_res\",\n",
    "    \"sources_commodity\": \"sources_commodity\",\n",
    "    \"emission_limits\": \"emission_limits\",\n",
    "    \"emission_development_factors\": \"emission_development_factors\",\n",
    "    \"sources_shortage\": \"sources_shortage\",\n",
    "    \"sources_shortage_el_add\": \"sources_shortage_el_add\",\n",
    "    \"sources_el_artificial\": \"sources_el_artificial\",\n",
    "    \"sources_renewables\": \"sources_renewables\",\n",
    "    \"sources_renewables_ts\": \"sources_renewables_ts\",\n",
    "    \"sinks_demand_el\": \"sinks_demand_el\",\n",
    "    \"sinks_demand_el_ts\": \"sinks_demand_el_ts\",\n",
    "    \"sinks_excess\": \"sinks_excess\",\n",
    "    \"transformers\": \"transformers\",\n",
    "    \"transformers_minload_ts\": \"transformers_minload_ts\",\n",
    "    \"transformers_gradients\": \"transformers_gradients\",\n",
    "    \"transformers_availability_ts\": \"transformers_availability_ts\",\n",
    "    \"transformers_renewables\": \"transformers_renewables\",\n",
    "    \"transformers_exogenous\": \"transformers_exogenous\",\n",
    "    \"transformers_exogenous_max_ts\": \"transformers_exogenous_max_ts\",\n",
    "    \"transformers_investment_options\": \"transformers_investment_options\",\n",
    "    \"storages_el_investment_options\": \"storages_el_investment_options\",\n",
    "    \"storages_el\": \"storages_el\",\n",
    "    \"storages_el_exogenous\": \"storages_el_exogenous\",\n",
    "    \"storages_el_exogenous_max_ts\": \"storages_el_exogenous_max_ts\",\n",
    "    \"electric_vehicles_ts\": \"electric_vehicles_ts\",\n",
    "    \"components_electric_vehicles\": \"components_electric_vehicles\",\n",
    "    \"costs_operation\": \"costs_operation\",\n",
    "    \"costs_fuel\": \"costs_fuel\",\n",
    "    \"costs_emissions\": \"costs_emissions\",\n",
    "    \"costs_emissions_ts\": \"costs_emissions_ts\",\n",
    "    \"costs_operation_renewables\": \"costs_operation_renewables\",\n",
    "    \"costs_operation_storages\": \"costs_operation_storages\",\n",
    "    \"costs_market_values\": \"costs_market_values\",\n",
    "    \"costs_fuel_ts\": \"costs_fuel_ts\",\n",
    "    \"costs_hydrogen_ts\": \"costs_hydrogen_ts\",\n",
    "    \"wacc\": \"wacc\",\n",
    "    \"interest_rate\": \"interest_rate\",\n",
    "    \"hydrogen_investment_maxima\": \"hydrogen_investment_maxima\",\n",
    "    'demand_response_clusters_eligibility': 'demand_response_clusters_eligibility',\n",
    "    \"demand_response_baseline_profile\": \"sinks_demand_response_el_ts_\",\n",
    "    \"demand_response_ava_pos\": \"sinks_demand_response_el_ava_pos_ts_\",\n",
    "    \"demand_response_ava_neg\": \"sinks_demand_response_el_ava_neg_ts_\",\n",
    "    \"demand_response_parameters\": \"_potential_parameters_\",\n",
    "    \"demand_response_variable_costs_data\": '_variable_costs_parameters_',\n",
    "    \"demand_response_fixed_costs_and_investments_data\": \"_fixed_costs_and_investments_parameters_\",\n",
    "    \"sinks_demand_el_excl_demand_response_ts\": \"sinks_demand_el_excl_demand_response_ts\",\n",
    "    \"sinks_demand_el_excl_demand_response\": \"sinks_demand_el_excl_demand_response\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook workflow settings\n",
    "- The variable `year` determines, for which target year the power plant status shall be evaluated. A value between 2017 and 2030 can be selected.\n",
    "\n",
    "> _Note: **IMPORTANT**: For **investment** data preparation, set **`year = 2030`** to include the latest status of data and to prevent errors in the actual model run occuring because of missing plants._\n",
    "- Specify `inflation_rate` in order to calculate nominal cost values from it.\n",
    "- `sensitivities`: Determine, which sensitivities and respective multipliers to use for studying parameter sensitivities. The following parameter sensitivities are considered in `pommesinvest`:\n",
    "    - fuel and emissions price (combined): Impacts price spreads for flexibility options\n",
    "    - level of inflexible baseline demand: Moves merit order intersection to areas of different steepness\n",
    "    - ratio of PV in German RES mix: Has a generation profile better suited to within-day flexibility\n",
    "- `rounding_precision`: Rounding precision to be applied to the largest (time series) files in order to reduce file size\n",
    "- `wacc_mode` defines the mode to be used for the weighted average costs of capital:\n",
    "    - \"technology_specific\": introduces empirically derived technology-specific WACC values\n",
    "    - \"unique\": Spreads a unique WACC assumption across all technologies\n",
    "- For some plants appearing in the list(s) of power plants to be decommissioned, the decommissioning year is missing. This is defined by `shutdown_assumption`, which imposes a (single) decommissioning year for the respective plants.\n",
    "- `eeg_clusters_per_technology` determines the amount of clusters per eeg technology to use.\n",
    "- `value_exogenous` determines the opportunity cost of exogenous units.\n",
    "- `res_capacity_projection` determines which estimate to use for RES capacity development for Germany until 2030 (\"Prognos\", \"EEG_2021\" or \"EEG_2023\")\n",
    "- `cluster_transformers_DE` determines whether or not to cluster power plants for Germany by their efficiencies. Clustered data can be used to speed up POMMES computations coming at the expense of model result precision.\n",
    "- `use_tyndp_2018`: If set to True, use TYNDP 2018 capacities instead ot those from the latest TYNDP 2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2030  # between 2017 and 2030 for dispatch modelling; 2030 for investment modelling!\n",
    "inflation_rate = 1.02\n",
    "excess_costs = 500  # 500\n",
    "shortage_costs_investment = 3e7\n",
    "artificial_shortage_costs_investment = 500\n",
    "excess_costs_investment = 0.001\n",
    "sensitivities = {\n",
    "    \"-50%\": 0.5,\n",
    "    \"-25%\": 0.75,\n",
    "    \"+25%\": 1.25,\n",
    "    \"+50%\": 1.5,\n",
    "}\n",
    "rounding_precision = 5\n",
    "\n",
    "wacc_mode = \"technology_specific\"  # \"unique\"\n",
    "shutdown_assumption = 2022  # between 2017 and 2030\n",
    "eeg_clusters_per_technology = 20\n",
    "value_exogenous = 50 # in ct/kWh\n",
    "res_capacity_projection = \"EEG_2023\"  # \"Prognos\", \"EEG_2021\", \"EEG_2023\"; for investment model: choose \"EEG_2023\"\n",
    "cluster_transformers_DE = True\n",
    "use_tyndp_2018 = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (year < 2017 or year > 2030):\n",
    "    raise ValueError(f'year must be between 2017 and 2030. You chose {year}.')\n",
    "\n",
    "if use_tyndp_2018:\n",
    "    warnings.warn(\n",
    "        \"Warning! Using TYNDP 2018 is not applicable for preparing investment-related data.\"\n",
    "    )\n",
    "    \n",
    "# Initialze an ExcelWriter to store all data in an .xlsx file\n",
    "writer = pd.ExcelWriter(\n",
    "    main_path[\"outputs\"] + 'input_data_complete_' + str(year) + '.xlsx', \n",
    "    engine='openpyxl'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers\n",
    "\n",
    "In this section, **conventional power plant** data for Germany and Europe is put together.<br>\n",
    "In [oemof.solph](https://github.com/oemof/oemof-solph) conventional power plants (in the most simple approach) can be represented through (generic) so called \"transformers\" which have one input and one or two ouputs (two outputs for CHP).\n",
    "\n",
    "## Data Import and initial preparation\n",
    "\n",
    "**Main data source**:<br>\n",
    "German and European Data on conventional powerplants (PPs) is taken from [Open Power System Data](https://data.open-power-system-data.org/conventional_power_plants/)\n",
    "\n",
    "* Plant status as of 2019:<br>\n",
    "\n",
    "Open Power System Data. 2020. Data Package Conventional power plants. Version 2020-10-01, downloaded on 2021-01-04. https://doi.org/10.25832/conventional_power_plants/2020-10-01\n",
    "\n",
    "* Plant status as of 2017:<br>\n",
    "\n",
    "Open Power System Data. 2018. Data Package Conventional power plants. Version 2018-12-20, downloaded on 2019-10-02. https://doi.org/10.25832/conventional_power_plants/2018-12-20\n",
    "\n",
    "### German Powerplants (OPSD)\n",
    "\n",
    "#### Read in data and do renaming\n",
    "Steps applied:\n",
    "- Data from conventional power plants from OPSD is read in.\n",
    "- Columns not needed are dropped.\n",
    "- A new country index 'DE' is assigned for all power plants, including those that are positioned in the netherlands and austria but feed into the German grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data set with plant status as of 2019\n",
    "if year > 2017:\n",
    "    opsd_de = pd.read_csv(\n",
    "        main_path[\"inputs\"] + sub_path[\"pp_public\"] + input_file[\"opsd_de_new\"],\n",
    "        index_col=0\n",
    "    )\n",
    "\n",
    "    opsd_de.drop(\n",
    "        columns=['capacity_gross_uba',\n",
    "                 'street', 'postcode', 'city', 'network_operator',\n",
    "                 'energy_source_level_1', 'energy_source_level_2', 'energy_source_level_3',\n",
    "                 'merge_comment', 'comment'], \n",
    "        inplace=True)\n",
    "\n",
    "    opsd_de['country2'] = 'DE'\n",
    "    opsd_de.rename(columns={'country': 'country_geographical', \n",
    "                            'country2': 'country',\n",
    "                            'energy_source': 'fuel'}, \n",
    "                   inplace=True)\n",
    "    eu_pp_feedin_de = opsd_de[opsd_de['country_geographical'] != 'DE'].index\n",
    "\n",
    "# Older data set; only used to evaluate plant status as of 2017\n",
    "else:\n",
    "    opsd_de = pd.read_csv(\n",
    "        main_path[\"inputs\"] + sub_path[\"pp_public\"] + input_file[\"opsd_de\"],\n",
    "        sep=\";\", decimal=\",\", index_col=0\n",
    "    )\n",
    "\n",
    "    opsd_de.drop(\n",
    "        columns=['capacity_gross_uba',\n",
    "                 'street', 'postcode', 'city', 'network_operator',\n",
    "                 'energy_source_level_1', 'energy_source_level_2', 'energy_source_level_3',\n",
    "                 'merge_comment', 'comment'], \n",
    "        inplace=True)\n",
    "\n",
    "    opsd_de['country'] = 'DE'\n",
    "    opsd_de.rename(columns={'country_code': 'country_geographical'}, inplace=True)\n",
    "    eu_pp_feedin_de = opsd_de[opsd_de['country_geographical'] != 'DE'].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> _NOTE: There are two duplicated BNetzA ids in the new OPSD data set which are obtained from the original BNetzA power plants list._\n",
    "> * _For the respective power plants, one block (or a share of blocks) is shutdown while the other is still operational_\n",
    "> * _The shutdown power blocks' BNetzA id is manipulated to account for that circumstance._\n",
    "\n",
    "Approach:\n",
    "* Append shutdown column to index to create a MultiIndex\n",
    "* Identify duplicates at index level 0 (BNetzA ID) with shutdown being not NaN.\n",
    "* Get the index location\n",
    "* Get list of original indices, replace the index location and reassign indices, using the string '\\_SD' to indicate plants for shutdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if year > 2017:\n",
    "    duplicated_idx = {}\n",
    "    \n",
    "    opsd_multi = opsd_de.set_index('shutdown', append=True)\n",
    "    \n",
    "    duplicated = opsd_multi[(opsd_multi.index.get_level_values(0).duplicated(keep=False))\n",
    "                             & (opsd_multi.index.get_level_values(1).notna())].index\n",
    "\n",
    "    for el in duplicated:\n",
    "        idx_el = opsd_multi.index.get_loc(el)\n",
    "        duplicated_idx[el[0]] = idx_el\n",
    "        \n",
    "    as_list = opsd_de.index.tolist()\n",
    "    for k, v in duplicated_idx.items():\n",
    "        as_list[v] = k + '_SD'\n",
    "    opsd_de.index = as_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare German conventional PP Raw Data\n",
    "\n",
    "Steps applied:\n",
    "- Exclude pumped storage, run-of-river and (other) storage technologies because they are specifically treated later on <br>&rarr; Create subsets for these technologies for later usage\n",
    "- Exclude non EEG units (i.e. biomass power plants) because biomass within the EEG scheme are treated as EE source later on <br>&rarr; Create a subset for EEG technologies for later usage\n",
    "- Replace fuel names from the OPSD list with the names used in _POMMES_ and rename some columns\n",
    "- Assign last commissioning dates for units which had a retrofit (last commissioning date = retrofitting date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_de = opsd_de[(~opsd_de['technology'].isin(['Pumped storage', 'Run-of-river', \n",
    "                                                'Storage technologies', 'RES', 'Reservoir'])) &\n",
    "                  (opsd_de['eeg'] == 'no')].copy()\n",
    "\n",
    "phes_de = opsd_de.loc[opsd_de['technology'] == 'Pumped storage']\n",
    "ror_de = opsd_de.loc[opsd_de['technology'] == 'Run-of-river']\n",
    "eeg_de = opsd_de.loc[(opsd_de['eeg'] == 'yes') & (opsd_de['technology'] != 'Run-of-river')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_fuels = {'Hard coal': 'hardcoal',\n",
    "              'Nuclear': 'uranium',\n",
    "              'Mixed fossil fuels': 'mixedfuels',\n",
    "              'Lignite': 'lignite',\n",
    "              'Natural gas': 'natgas',\n",
    "              'Oil': 'oil',\n",
    "              'Biomass and biogas': 'biomass',\n",
    "              'Other fossil fuels': 'otherfossil',\n",
    "              'Other fuels': 'otherfossil',\n",
    "              'Waste': 'waste'}\n",
    "\n",
    "conv_de.loc[:,'fuel'].replace(dict_fuels, inplace=True)\n",
    "\n",
    "conv_de.rename(columns={'capacity_net_bnetza': 'capacity',\n",
    "                        'eic_code_plant': 'eic_code',\n",
    "                        'name_bnetza': 'name'}, inplace=True)\n",
    "\n",
    "conv_de.loc[~conv_de['retrofit'].isna(),\n",
    "            'commissioned_last'] = conv_de.loc[~conv_de['retrofit'].isna(), 'retrofit']\n",
    "conv_de.loc[conv_de['retrofit'].isna(), \n",
    "            'commissioned_last'] = conv_de.loc[conv_de['retrofit'].isna(), 'commissioned']\n",
    "conv_de = conv_de.astype(dtype={'commissioned_last': 'object'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### European Powerplants (PyPSA-EUR PP matching)\n",
    "\n",
    "European power plants data is obtained from an existing [power plant matching tool](https://github.com/FRESNA/powerplantmatching) developped by the PyPSA developpers:\n",
    "\n",
    "F. Gotzens, H. Heinrichs, J. Hörsch, and F. Hofmann, Performing energy modelling exercises in a transparent way - The issue of data quality in power plant databases, Energy Strategy Reviews, vol. 23, pp. 1–12, Jan. 2019. <br>[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.3358985.svg)](https://doi.org/10.5281/zenodo.3358985).<br>Copyright 2018-2020 Fabian Gotzens (FZ Jülich), Jonas Hörsch (KIT), Fabian Hofmann (FIAS)<br>\n",
    "powerplantmatching is released as free software under the [GPLv3](http://www.gnu.org/licenses/gpl-3.0.en.html), see [LICENSE](https://github.com/FRESNA/powerplantmatching/blob/master/LICENSE) for further information.\n",
    "\n",
    "Steps applied:\n",
    "- Read in data set supplied as output of the power plant matching tool.\n",
    "- Drop data for countries which are not within the scope of _POMMES_.\n",
    "- Drop powerplants that feed into the German grid and are modeled in Germany.\n",
    "\n",
    "**Update**: Use data set as of end of 2020-26-11.\n",
    "* ids used are unique and consistent.\n",
    "* Keep the naming from the older data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_eu = {\n",
    "    'Netherlands': 'NL', \n",
    "    'Denmark': 'DK', \n",
    "    'France': 'FR', \n",
    "    'Poland': 'PL', \n",
    "    'Switzerland': 'CH',\n",
    "    'Czech Republic': 'CZ', \n",
    "    'Norway': 'NO', \n",
    "    'Sweden': 'SE', \n",
    "    'Austria': 'AT', \n",
    "    'Belgium': 'BE',\n",
    "    'Italy': 'IT'\n",
    "}\n",
    "\n",
    "# Data set with plant status as of 2019\n",
    "if year > 2017:\n",
    "    pp_eu = pd.read_csv(\n",
    "        main_path[\"inputs\"] + sub_path[\"pp_public\"] + input_file[\"ppmatching_eu_new\"],\n",
    "        index_col=0\n",
    "    )\n",
    "    \n",
    "    columns_eu_old = {'DateIn': 'YearCommissioned',\n",
    "                      'DateRetrofit': 'Retrofit'}\n",
    "    \n",
    "    pp_eu.rename(columns=columns_eu_old, inplace=True)\n",
    "    pp_eu.drop(index=pp_eu[~pp_eu['Country'].isin(countries_eu.keys())].index, inplace=True)\n",
    "    pp_eu['Country'].replace(countries_eu, inplace=True)\n",
    "\n",
    "    pp_eu.drop(index=pp_eu[pp_eu['projectID'].str.contains('|'.join(eu_pp_feedin_de))].index, inplace=True)\n",
    "\n",
    "# Older data set; only used to evaluate plant status as of 2017\n",
    "else:\n",
    "    pp_eu = pd.read_csv(\n",
    "        main_path[\"inputs\"] + sub_path[\"pp_public\"] + input_file[\"ppmatching_eu\"],\n",
    "        sep=\";\", decimal=\",\", index_col = 0\n",
    "    )\n",
    "\n",
    "    pp_eu.drop(index=pp_eu[~pp_eu['Country'].isin(countries_eu.keys())].index, inplace=True)\n",
    "    pp_eu['Country'].replace(countries_eu, inplace=True)\n",
    "\n",
    "    pp_eu.drop(index=pp_eu[pp_eu['projectID'].str.contains('|'.join(eu_pp_feedin_de))].index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bidding Zone Allocation\n",
    "For Sweden and Norway shape files are used, that contain the respective bidding zones. Those plants that are not in these multipolygons are allocated based on the bidding zone of their nearest neighbour. This happens quite often for Norwegian hydro power, where the fjords are not perfectly represented by the shapes.\n",
    "\n",
    "Shape data is obtained from the electricity map of Tomorrow. See this [GitHub issue](https://github.com/tmrowco/electricitymap-contrib/pull/1383) for Norwegian and Swedish data.<br>\n",
    "Copyright (c) 2020 Tomorrow<br>\n",
    "License is [MIT License](https://opensource.org/licenses/MIT).\n",
    "\n",
    "Steps applied:\n",
    "- Assign bidding zones for DK based on longitude\n",
    "- Assign bidding zones for Norway and Sweden based on geometry shape data\n",
    "- Assign missing bidding zone information based on smallest geodesic distance to the next bidding zone for Norway and Sweden\n",
    "\n",
    "> _Note: Italy has been introduced to the scope by end 2022. Italy applies market splitting into up to 4 bidding zones in the case of congestions. Nonetheless, since it is not the focus of the analyses and since there is a mechanism inplace to derive a uniform end consumer market price, it will be considered as a single bidding zone for POMMES._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_eu['bidding_zone'] = pp_eu['Country']\n",
    "\n",
    "pp_eu.loc[pp_eu['Country'] == 'DK', 'bidding_zone'] = (\n",
    "    np.where(pp_eu.loc[pp_eu['Country'] == 'DK', 'lon'] <= 10.9, 'DK1', 'DK2'))\n",
    "\n",
    "country_dict = {'NO': 5, 'SE': 4}\n",
    "for country in country_dict.keys():\n",
    "    points = pp_eu.loc[(pp_eu['Country'] == country), ['lat','lon']]\n",
    "    points_df = gpd.GeoDataFrame(points, \n",
    "                                 geometry=gpd.points_from_xy(points.lon, points.lat), \n",
    "                                 crs=\"epsg:4326\")\n",
    "    \n",
    "    zones = pd.concat([tools.load_bidding_zone_shape(\n",
    "        country, country + str(number),\n",
    "        main_path[\"inputs\"] + sub_path[\"marketzones\"]) \n",
    "                       for number in range(1, country_dict[country] + 1)])\n",
    "    \n",
    "    points_df = gpd.sjoin(points_df, zones, how='left')\n",
    "    pp_eu.loc[points_df.index, 'bidding_zone'] = points_df['id'].values\n",
    "    \n",
    "    for idx in pp_eu[(pp_eu['Country'] == country) & (pp_eu['bidding_zone'].isna())].index:\n",
    "        lat_miss, lon_miss = pp_eu.loc[idx, ['lat', 'lon']]\n",
    "\n",
    "        idx_min_dist = pp_eu.loc[(pp_eu['Country'] == country) &\n",
    "                                   (~pp_eu['bidding_zone'].isna()), ['lat', 'lon']].apply(\n",
    "            lambda x: tools.calc_dist(lat_miss, lon_miss, x.lat, x.lon), axis=1).idxmin()\n",
    "\n",
    "        pp_eu.at[idx, 'bidding_zone'] = pp_eu.at[idx_min_dist, 'bidding_zone']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare European conventional PP Raw Data\n",
    "Steps applied:\n",
    "- Rename columns and fuels such that they match the (oemof.solph) terminology used in _POMMES_\n",
    "- Assign technology for hydro plants\n",
    "    - _**Assumption made**: Plants under 10 kW are ROR, those above are hydro storages (reservoir)_\n",
    "- Introduce subsets for some technologies (conventional and renewable ones)\n",
    "- Assign last commissioning dates for units which had a retrofit (last commissioning date = retrofitting date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_eu.rename(\n",
    "    columns={'Name': 'name', \n",
    "             'Fueltype': 'fuel', \n",
    "             'Technology': 'technology', \n",
    "             'Country': 'country',\n",
    "             'Capacity': 'capacity', \n",
    "             'Efficiency': 'efficiency_el', \n",
    "             'Duration': 'duration', \n",
    "             'Volume_Mm3':'volume_m3',\n",
    "             'DamHeight_m': 'damheight_m', \n",
    "             'YearCommissioned': 'commissioned', \n",
    "             'Retrofit': 'retrofit'}, \n",
    "    inplace=True)\n",
    "\n",
    "pp_eu.loc[(pp_eu['fuel'] == 'Hydro') & (pp_eu['technology'].isna()), 'technology'] = (\n",
    "    np.where(pp_eu.loc[(pp_eu['fuel'] == 'Hydro') & (pp_eu['technology'].isna()), 'capacity'] < 10,\n",
    "             'Run-Of-River',\n",
    "             'Reservoir'))\n",
    "\n",
    "conv_eu = pp_eu[(~pp_eu['fuel'].isin(['Wind', 'Solar', 'Hydro']))].copy()\n",
    "pumpedstorage_eu = pp_eu.loc[pp_eu['technology'] == 'Pumped Storage']\n",
    "reservoir_eu = pp_eu.loc[pp_eu['technology'] == 'Reservoir']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_eu['fuel'].replace(\n",
    "    {'Hard Coal': 'hardcoal',\n",
    "     'Nuclear': 'uranium',\n",
    "     'Lignite': 'lignite',\n",
    "     'Natural Gas': 'natgas',\n",
    "     'Oil': 'oil',\n",
    "     'Bioenergy': 'biomass',\n",
    "     'Other': 'otherfossil',\n",
    "     'Geothermal': 'geothermal',\n",
    "     'Waste': 'waste',\n",
    "    }, \n",
    "    inplace=True)\n",
    "\n",
    "conv_eu.loc[~conv_eu['retrofit'].isna(), \n",
    "            'commissioned_last'] = conv_eu.loc[~conv_eu['retrofit'].isna(), 'retrofit']\n",
    "conv_eu.loc[conv_eu['retrofit'].isna(), \n",
    "            'commissioned_last'] = conv_eu.loc[conv_eu['retrofit'].isna(), 'commissioned']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do some garbage collection\n",
    "try:\n",
    "    del (as_list, columns_eu_old, country, country_dict, dict_fuels,\n",
    "         duplicated_idx, duplicated, el, eu_pp_feedin_de, idx, idx_el, idx_min_dist,\n",
    "         k, lat_miss, lon_miss, opsd_de, opsd_multi, points, points_df, v, zones)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add projection for near future power plant development\n",
    "\n",
    "The future estimate for power plants in 2030 for Germany is depicted using the following sources\n",
    "* NEP power plant list: ÜNB / BNetzA (2019): Kraftwerksliste zum ÜNB Entwurf des Szenariorahmens zum NEP 2030,  https://www.netzentwicklungsplan.de/sites/default/files/paragraphs-files/Kraftwerksliste_%C3%9CNB_Entwurf_Szenariorahmen_2030_V2019_2_0_0.pdf, downloaded on 2019-10-22.\n",
    "* Plans for commissioning of new plants put together by Julien Faist (JF). Primary data is combined from\n",
    "    * UBA (2019): Genehmigte oder im Genehmigungsverfahren befindliche Kraftwerksprojekte in Deutschland, as of 01/2019, https://www.umweltbundesamt.de/sites/default/files/medien/384/bilder/dateien/4_tab_genehmigte-in_genehmigung-kraftwerksprojekte_2019-04-04.pdf, accessed 03.11.2020.\n",
    "    * BDEW (2019): BDEW-Kraftwerksliste. In Bau oder Planung befindliche Anlagen ab 20 Megawatt (MW) Leistung, Anlage zur BDEW-Presseinformation vom 1.April 2019 zur Hannover Messe, https://www.bdew.de/media/documents/PI_20190401_BDEW-Kraftwerksliste.pdf, accessed 03.11.2020.\n",
    "    * BNetzA (2019): Kraftwerksliste Bundesnetzagentur zum erwarteten Zu- und Rückbau 2019 bis 2022, https://www.bundesnetzagentur.de/DE/Sachgebiete/ElektrizitaetundGas/Unternehmen_Institutionen/Versorgungssicherheit/Erzeugungskapazitaeten/Kraftwerksliste/kraftwerksliste-node.html, as of 07.03.2019, accessed 03.12.2019.\n",
    "    * company specific sources given in the table itself.\n",
    "* The power plant 'Datteln 4' has been taken into operation meanwhile. It is obtained from a data set on exogeneous decommissioning plans which has been put together by Julien Faist as well.\n",
    "\n",
    "The future capacity projections for Europe in 2030 are derived from the 2022 (previously 2018) version of the TYNDP from ENTSO-E:\n",
    "* ENTSO-E (2022): Updated Electricity Modelling Results, Capacity & Dispatch, TYNDP 2022 Scenario Report – additional Downloads, https://view.officeapps.live.com/op/view.aspx?src=https%3A%2F%2F2022.entsos-tyndp-scenarios.eu%2Fwp-content%2Fuploads%2F2022%2F04%2F220310_Updated_Electricity_Modelling_Results.xlsx&wdOrigin=BROWSELINK, accessed 18.11.2022.\n",
    "* ENTSO-E (2019): ENTSOE Scenario 2018 Generation Capacities, https://tyndp.entsoe.eu/maps-data, accessed 03.11.2020.\n",
    "\n",
    "Updated sources:\n",
    "* BNetzA (2020): Kraftwerksliste Bundesnetzagentur zum erwarteten Zu- und Rückbau 2019 bis 2022, https://www.bundesnetzagentur.de/DE/Sachgebiete/ElektrizitaetundGas/Unternehmen_Institutionen/Versorgungssicherheit/Erzeugungskapazitaeten/Kraftwerksliste/kraftwerksliste-node.html, as of 01.04.2020, accessed 05.01.2021."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine new commissionings for Germany\n",
    "Steps applied:\n",
    "* Read in and use the NEP power plant data set.\n",
    "    * Replace values with unknown commissioning date in NEP list. &rarr; These are planned power plants for which no commissioning date is given; it is (arbitrarily) set to 2030.\n",
    "    * Filter out newly built power plants which are commissioned between 2019 and 2030. Include investment-related assumptions:\n",
    "        * In the investment model, it is assumed that plants with a commissioning date later than 2030 are not built. Investments from 2030 on are purely model-endogenous.\n",
    "        * Furthermore, it is assumed that plants going into operation from 2030 on will be solely fueled with hydrogen instead of natural gas.\n",
    "    * Drop power to heat units since only generation the demand-side is exogeneously fixed in _POMMES_ except for electrical demand response.\n",
    "    * Rename columns that shall be kept so that they match the conventional data set from OPSD.\n",
    "    * Create a new data set containing only the new built power plants.\n",
    "* Combine information on new-built power plants from NEP with the commissionings list put together by JF and the new commissionings information from BNetzA.\n",
    "    * Read in the commissionings lists (JF's list and BNetzA list).\n",
    "    * Do a pairwhise string comparison of the power plants names in order to identify duplicates, i.e. power plants that are listed in two power plants lists. This is necessary for mapping because the commissionings lists in contrary to the other lists does not have the BNA_Id as an index.\n",
    "    * If there are duplicates, assign the missing capacity information from the commissionings lists data set but use the NEP data set for the remainder. The priority of the data sources is as follows:\n",
    "        1. BNetzA list since it contains the most topical information.\n",
    "        2. NEP list since it is one single consistent source.\n",
    "        3. JF's list since it combines different sources.\n",
    "* Append information for the hard coal power plant 'Datteln 4' from the hardcoal shutdown list put together by JF.\n",
    "* The planned power plant Stade-Bützfleth must not be build due to the German coal phase out law (Kohleverstromungsbeendigungsgesetz - KVBG) forbidding new installations of coal-fired power plants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nep = pd.read_excel(\n",
    "    main_path[\"inputs\"] + sub_path[\"pp_public\"] + input_file[\"nep_de\"]\n",
    ")\n",
    "nep.drop_duplicates(subset='BNetzA-ID', inplace=True)\n",
    "nep.set_index('BNetzA-ID', inplace=True)\n",
    "\n",
    "nep.loc[:,'Inbetriebnahme (Jahr)'].replace({'Jahr unbestimmt': 2030}, inplace=True)\n",
    "nep_new = nep.loc[nep['Inbetriebnahme (Jahr)'].isin(range(2019, 2031))]\n",
    "\n",
    "dict_nep_col_names = {'Kraftwerksname': 'name',\n",
    "                      'Betreiber': 'company',\n",
    "                      'Bundesland': 'state',\n",
    "                      'Energieträger': 'fuel',\n",
    "                      'Wärmeauskopplung KWK (Ja/Nein)': 'NEP_chp_bool',\n",
    "                      'Annahmen ÜNB:\\nIndustriekraftwerk (Ja/Nein)': 'NEP_ipp_bool',\n",
    "                      'Inbetriebnahme (Jahr)': 'commissioned',\n",
    "                      'Nettonennleistung B2035 [MW]': 'capacity'}\n",
    "cols_to_drop = [col for col in nep_new.columns \n",
    "                if col not in dict_nep_col_names.keys()]\n",
    "nep_new = nep_new.rename(columns=dict_nep_col_names).drop(cols_to_drop, axis=1)\n",
    "\n",
    "empty_df_new = pd.DataFrame(columns=conv_de.columns)\n",
    "conv_de_new = pd.concat([empty_df_new, nep_new], axis=0)\n",
    "\n",
    "\n",
    "conv_de_new = conv_de_new[~(conv_de_new.name.str.contains('Power to heat', \n",
    "                            flags=IGNORECASE, regex=True))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comm_conv_de = pd.read_excel(\n",
    "    main_path[\"inputs\"] + sub_path[\"pp_er\"] + input_file[\"comm\"],\n",
    "    sheet_name='exo_com_transformers', \n",
    "    index_col='new_built_id',\n",
    "    usecols=\"A:P\")\n",
    "\n",
    "# Exclude hard coal from data set and drop line following column headers (np.nan)\n",
    "comm_conv_de = comm_conv_de[comm_conv_de['Primärenergie-Basis'] != 'Steinkohle'].drop(np.nan)\n",
    "\n",
    "comm_BNetzA = pd.read_excel(\n",
    "    main_path[\"inputs\"] + sub_path[\"pp_public\"] + input_file[\"comm_decomm_BNetzA\"],\n",
    "    sheet_name='Zu und Rückbau Bund Tab', skiprows=5, usecols='B:I', nrows=13\n",
    ")\n",
    "\n",
    "comm_BNetzA = comm_BNetzA[comm_BNetzA['Energieträger'] != 'Steinkohle']\n",
    "comm_BNetzA.loc[comm_BNetzA['Anlagenname'].isna(), 'Anlagenname'] = comm_BNetzA['Blockname']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the preparation for a string comparison of power plants names:\n",
    "* Define a string for some elements to be excluded from the comparison, i.e. general terms, such as 'KW' for 'Kraftwerk' (German for power plants).\n",
    "* Define some inclusion criteria in order not to use relevant information, i.e. 'GuD-Köln' shall be kept while 'GuD' as a general term shall be excluded.\n",
    "* For each data set, the NEP data set, the commissionings list by JF and the commissionings list by BNetzA, take the 'name' column and perform a string split (i.e. separate strings by whitespaces) and store the result to a names DataFrame.\n",
    "* For the names data frame, exclude general terms which are not relevant for the comparison by replacing them with nan values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude = r'GUD|CCPP|AG|.?KW|KWK|GK|in|[0-9]'\n",
    "include = r'GUD-|HKW-'\n",
    "\n",
    "names = conv_de_new['name'].str.split(expand=True).replace({None:np.nan})\n",
    "names_comm = comm_conv_de['Kraftwerksname'].str.split(expand=True).replace({None:np.nan})\n",
    "names_comm_BNetzA = comm_BNetzA['Anlagenname'].str.split(expand=True).replace({None:np.nan})\n",
    "\n",
    "names = names.apply(lambda x: x.str.strip('()') if x.dtype == \"object\" else x)\n",
    "names_comm = names_comm.apply(lambda x: x.str.strip('()') if x.dtype == \"object\" else x)\n",
    "names_comm_BNetzA = names_comm_BNetzA.apply(lambda x: x.str.strip('()') if x.dtype == \"object\" else x)\n",
    "\n",
    "for col in names.columns:\n",
    "    cond1 = names[col].str.match(exclude, flags=IGNORECASE, na=False)\n",
    "    cond2 = ~(names[col].str.match(include, flags=IGNORECASE, na=False))\n",
    "    names.loc[cond1 & cond2, col] = np.nan\n",
    "    \n",
    "for col in names_comm.columns:\n",
    "    cond1 = names_comm[col].str.match(exclude, flags=IGNORECASE, na=False)\n",
    "    cond2 = ~(names_comm[col].str.match(include, flags=IGNORECASE, na=False))\n",
    "    names_comm.loc[cond1 & cond2, col] = np.nan\n",
    "    \n",
    "for col in names_comm_BNetzA.columns:\n",
    "    cond1 = names_comm_BNetzA[col].str.match(exclude, flags=IGNORECASE, na=False)\n",
    "    cond2 = ~(names_comm_BNetzA[col].str.match(include, flags=IGNORECASE, na=False))\n",
    "    names_comm_BNetzA.loc[cond1 & cond2, col] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the actual string comparison of power plants names:\n",
    "* Create sets of unique values from all name DataFrames containing all parts of the power plants names except for the general terms that have been excluded above.\n",
    "* Create the pairwhise intersection of two sets and remove the nan value.\n",
    "* Combine the relevant matches to a regex in order to use this for filtering.\n",
    "* Filter out plants which are contained in two data sets and store them in separate DataFrames.\n",
    "* Do a manual correction: For the term 'Kessel', the matching is not perfectly well-engineered. &rarr; I.e. 'Kessel 13' in Flensburg is detected as a match, though there is only a match for 'Kessel 7' in Köln."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_set = set(names.values.flatten())\n",
    "names_comm_set = set(names_comm.values.flatten())\n",
    "names_comm_BNetzA_set = set(names_comm_BNetzA.values.flatten())\n",
    "\n",
    "# union1: compare JF's list and NEP data\n",
    "union1 = names_comm_set.intersection(names_set)\n",
    "union1.remove(np.nan)\n",
    "filter_plants1 = '|'.join(union1)\n",
    "\n",
    "# union2: compare BNetzA and NEP data\n",
    "union2 = names_comm_BNetzA_set.intersection(names_set)\n",
    "union2.remove(np.nan)\n",
    "filter_plants2 = '|'.join(union2)\n",
    "\n",
    "# union3: compare JF's list and BNetzA data\n",
    "union3 = names_comm_BNetzA_set.intersection(names_comm_set)\n",
    "union3.remove(np.nan)\n",
    "filter_plants3 = '|'.join(union3)\n",
    "\n",
    "# Filter out the plants which are contained in two of the data sets\n",
    "duplicated_comm1 = comm_conv_de[comm_conv_de['Kraftwerksname'].str.contains(filter_plants1)]\n",
    "duplicated_conv1 = conv_de_new[conv_de_new['name'].str.contains(filter_plants1)]\n",
    "\n",
    "duplicated_comm_BNetzA2 = comm_BNetzA[comm_BNetzA['Anlagenname'].str.contains(filter_plants2)]\n",
    "duplicated_conv2 = conv_de_new[conv_de_new['name'].str.contains(filter_plants2)]\n",
    "\n",
    "duplicated_comm_BNetzA3 = comm_BNetzA[comm_BNetzA['Anlagenname'].str.contains(filter_plants3)]\n",
    "duplicated_comm3 = comm_conv_de[comm_conv_de['Kraftwerksname'].str.contains(filter_plants3)]\n",
    "\n",
    "# Do correction: Power plant name matching doesn't cover all possibilities\n",
    "duplicated_comm1 = duplicated_comm1[~(duplicated_comm1['Kraftwerksname'] == 'Kessel 13')]\n",
    "duplicated_comm3 = duplicated_comm3[~(duplicated_comm3['Kraftwerksname'] == 'Kessel 13')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do a capacity matching and assign missing capacity information:\n",
    "* Assign missing capacity information from commissionings lists but use NEP power plants list as a default &rarr; i.e. drop the duplicates from the commissionings list in order to be able to combine the data sources\n",
    "* Check whether power plants capacities are (nearly) the same, i.e. deviate less than 20%, in addition to the plants names being very similar. <br>\n",
    "&rarr; _NOTE: This matching criterion is ignored for now, but might be integrated later. The respective results are stored in a dictionary mapping the indices to each other and containing a boolean indicating a capacity mathc or not._\n",
    "* Assign missing capacity values to NEP data set.<br>\n",
    "&rarr; _NOTE: The 1.2 GW capacity limit mentioned in the NEP data set is violated, but plants within the data set are not build and operated by TSOs but by third parties. Hence, they do not really affect the 1.2 GW capacity limit anyway. For more detailled information on the energy regulatory background, see the [following section](#Assign-missing-values-for-new-commissioned-plants)._\n",
    "\n",
    "> _NOTE: So far, this is not fully integrated and done for the comparison of JF's list and the NEP power plants list only. This seems sufficient, since so far the routine is only used for assigning missing capacity values which can be achieved._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx_mapping: key is the search string\n",
    "# values are indices in data sets and info on whether or not there is a capacity match\n",
    "idx_mapping1 = {}\n",
    "\n",
    "for el in union1:\n",
    "    # Address inaccuracy for string 'Kessel'\n",
    "    if el == 'Kessel':\n",
    "        el = 'Kessel 7'\n",
    "        \n",
    "    idx1 = list(comm_conv_de.loc[comm_conv_de['Kraftwerksname'].str.contains(el)].index)\n",
    "    idx2 = list(conv_de_new.loc[conv_de_new['name'].str.contains(el)].index)\n",
    "    \n",
    "    cap1 = comm_conv_de.loc[comm_conv_de['Kraftwerksname'].str.contains(el), \n",
    "                            'elektrische Nettoleistung (MW)'].values\n",
    "    cap2 = conv_de_new.loc[conv_de_new['name'].str.contains(el), \n",
    "                           'capacity'].values\n",
    "\n",
    "    if len(cap1) > 1:\n",
    "        cap1 = sum(cap1)\n",
    "    else:\n",
    "        cap1 = cap1[0]\n",
    "    if len(cap2) > 1:\n",
    "        cap2 = sum(cap2)\n",
    "    else:\n",
    "        cap2 = cap2[0]\n",
    "    \n",
    "    # Determine whether capacity is nearly the same\n",
    "    match = True\n",
    "    if not isinstance(cap1, str):\n",
    "        interval = [cap1 * 0.8, cap1 * 1.2]\n",
    "        if not isinstance(cap2, str):\n",
    "            match = (cap2 >= interval[0]) & (cap2 <= interval[1])\n",
    "        else:\n",
    "            cap2 = cap1\n",
    "            conv_de_new.loc[idx2, 'capacity'] = cap1\n",
    "\n",
    "    idx_mapping1[el] = (idx1, idx2, match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjust BNetzA commissionings list:\n",
    "* Add CHP information from JF's list.\n",
    "* Add IPP information based on company names.\n",
    "* Add unique IDs for the plants from the BNetzA list, building up on the scheme JF introduced.\n",
    "* Set CHP and IPP information for known industry power plants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comm_BNetzA[['KWK (falls bekannt)', 'IPP']] = 'nein'\n",
    "for el in union3:\n",
    "    # Address inaccuracy for string 'Kessel'\n",
    "    if el == 'Kessel':\n",
    "        el = 'Kessel 7'\n",
    "    \n",
    "    comm_BNetzA.loc[comm_BNetzA['Anlagenname'].str.contains(el), \n",
    "                    ['KWK (falls bekannt)', 'IPP', 'Fernwärme Leistung (MW)']] = (\n",
    "        comm_conv_de.loc[comm_conv_de['Kraftwerksname'].str.contains(el)\n",
    "                         & ~comm_conv_de['Kraftwerksname'].str.contains('West\\n'),\n",
    "                         ['KWK (falls bekannt)', 'IPP', 'Fernwärme Leistung (MW)']].values)\n",
    "    comm_BNetzA.rename(index={\n",
    "        k: comm_conv_de.loc[\n",
    "               comm_conv_de['Kraftwerksname'].str.contains(el)\n",
    "               & ~comm_conv_de['Kraftwerksname'].str.contains('West\\n')].index.values[0]\n",
    "           for k in comm_BNetzA.loc[comm_BNetzA['Anlagenname'].str.contains(el)].index}, \n",
    "                       inplace=True)\n",
    "    \n",
    "last_idx_comm = int(comm_conv_de.index.values[-1].split('_')[-1])\n",
    "\n",
    "keys_BNetzA = [el for el in comm_BNetzA.index if isinstance(el, int)]\n",
    "values_BNetzA = ['new_built_' + '{:03d}'.format(el) \n",
    "                 for el in range(last_idx_comm + 1, last_idx_comm + 1 + len(keys_BNetzA))]\n",
    "comm_BNetzA_newidx = dict(zip(keys_BNetzA, values_BNetzA))\n",
    "comm_BNetzA.rename(index=comm_BNetzA_newidx, inplace=True)\n",
    "\n",
    "comm_BNetzA.loc[comm_BNetzA['Anlagenname'].str.contains('HKW'), 'KWK (falls bekannt)'] = 'yes'\n",
    "comm_BNetzA.loc[comm_BNetzA['Unternehmen'].str.contains(\"|\".join('Volkswagen, Ineos')), 'IPP'] = 'yes'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the different data sets after the string matching of power plants names:\n",
    "* Drop the duplicates from the commissionings lists\n",
    "* Adjust the column names and some entries for consistency\n",
    "* Combine the commissionings lists with the new-built power plants obtained from the NEP power plants list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comm_conv_de.drop(duplicated_comm1.index.union(duplicated_comm3.index), inplace=True)\n",
    "conv_de_new.drop(duplicated_conv2.index, inplace=True)\n",
    "\n",
    "dict_comm_col_names = {\n",
    "    'Kraftwerksname': 'name',\n",
    "    'Unternehmen': 'company',\n",
    "    'Primärenergie-Basis': 'fuel',\n",
    "    'KWK (falls bekannt)': 'NEP_chp_bool',\n",
    "    'IPP': 'NEP_ipp_bool',\n",
    "    'Fernwärme Leistung (MW)': 'chp_capacity_uba',\n",
    "    'Anlagenart': 'technology',\n",
    "    'geplante Inbetriebnahme': 'commissioned',\n",
    "    'elektrische Nettoleistung (MW)': 'capacity'}\n",
    "\n",
    "dict_comm_BNetzA_names = {\n",
    "    'Anlagenname': 'name',\n",
    "    'Unternehmen': 'company',\n",
    "    'Energieträger': 'fuel',\n",
    "    'KWK (falls bekannt)': 'NEP_chp_bool',\n",
    "    'IPP': 'NEP_ipp_bool',\n",
    "    'Fernwärme Leistung (MW)': 'chp_capacity_uba',\n",
    "    'Voraussichtliche Aufnahme der kommerziellen Strom-einspeisung': 'commissioned',\n",
    "    'Geplante Netto-Nennleistung (elektrisch) der Investition in MW (Pumpspeicher: Turbinenbetrieb)': 'capacity'}\n",
    "\n",
    "cols_to_drop = [col for col in comm_conv_de.columns \n",
    "                if col not in dict_comm_col_names.keys()]\n",
    "cols_to_drop_BNetzA = [col for col in comm_BNetzA.columns \n",
    "                       if col not in dict_comm_BNetzA_names.keys()]\n",
    "\n",
    "comm_conv_de = comm_conv_de.rename(columns=dict_comm_col_names).drop(cols_to_drop, axis=1)\n",
    "comm_BNetzA = comm_BNetzA.rename(columns=dict_comm_BNetzA_names).drop(cols_to_drop_BNetzA, axis=1)\n",
    "\n",
    "# Replace CHP values in order to be able to use the same data prep steps\n",
    "comm_conv_de.loc[:, ['NEP_chp_bool', 'NEP_ipp_bool']].replace({'ja': 'Ja', 'nein': 'Nein'}, inplace=True)\n",
    "comm_BNetzA.loc[:, ['NEP_chp_bool', 'NEP_ipp_bool']].replace({'ja': 'Ja', 'nein': 'Nein'}, inplace=True)\n",
    "\n",
    "conv_de_new = pd.concat([conv_de_new, comm_conv_de, comm_BNetzA], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Append information on individual plants ('Datteln 4' & 'HKW Dieselstr.'):\n",
    "* Read in decommissioning data set for hardcoal\n",
    "* Access plant Datteln 4 and do some renaming\n",
    "* Append it to the new-built power plants data set for Germany\n",
    "* Assign CHP information (based on information from the plant operator Uniper: https://www.uniper.energy/de/datteln-4, accessed 03.11.2020)\n",
    "* Update shutdown date to 2030 due to the advanced coal exit strategy (2030 instead of 2035 resp. 2038).\n",
    "* Add missing capacity information for HKW Dieselstr. Halle from plant operators website:\n",
    "https://evh.de/privatkunden/unternehmen/energiepark/dieselstra%C3%9Fe, accessed 03.11.2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decomm_hardcoal_de = pd.read_csv(\n",
    "    main_path[\"inputs\"] + sub_path[\"pp_er\"] + input_file[\"decomm_hardcoal\"],\n",
    "    index_col=0, sep=\";\", decimal=\",\", usecols=list(range(13)))\n",
    "\n",
    "decomm_hardcoal_col_names = {\n",
    "    'Unternehmen': 'company',\n",
    "    'Kraftwerksname': 'name',\n",
    "    'Energieträger': 'fuel',\n",
    "    'Aufnahme der kommerziellen Stromerzeugung der derzeit in Betrieb befindlichen Erzeugungseinheit\\r\\n(Datum/Jahr)':\n",
    "    'commissioned',\n",
    "    'Geplante Ausserbetriebnahme': 'shutdown',\n",
    "    'Netto-Nennleistung (elektrische Wirkleistung) in MW': 'capacity',\n",
    "}\n",
    "\n",
    "datteln4 = decomm_hardcoal_de[decomm_hardcoal_de['Kraftwerksname'] == 'Datteln 4'].rename(\n",
    "    columns=decomm_hardcoal_col_names).drop(\n",
    "    columns=[col for col in decomm_hardcoal_de.columns if col not in decomm_hardcoal_col_names.keys()])\n",
    "\n",
    "conv_de_new = pd.concat([conv_de_new, datteln4])\n",
    "conv_de_new.loc[conv_de_new['name'] == 'Datteln 4', \n",
    "                ['NEP_chp_bool', 'NEP_ipp_bool', 'chp_capacity_uba', 'shutdown']] = ('Ja', 'Nein', 380, 2030)\n",
    "\n",
    "conv_de_new.loc[conv_de_new['name'] == 'HKW Dieselstr.\\n(Modernisierung)', ['capacity', 'chp_capacity_uba']] = (47, 34)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add so-called hydrogen sprinters:\n",
    "* § 28e of EEG 2023 introduces tender procedures for new-built hydrogen plants.\n",
    "* Capacities are assumed to be built and operational one year after the tender has been held.\n",
    "\n",
    "Steps applied:\n",
    "* Read in capacities per year and assign remaining parameters\n",
    "* Assume technology type to be gas turbine and assign complete power to one single plant. It is likely to be multiple plants, but since they are modelled as clusters anyways, this does not play a major role.\n",
    "\n",
    "Source:\n",
    "* EEG 2023: Gesetz zu Sofortmaßnahmen für einen beschleunigten Ausbau der erneuerbaren Energien und weiteren Maßnahmen im Stromsektor from July 20, 2022, https://www.clearingstelle-eeg-kwkg.de/sites/default/files/2022-07/23_bgbl122s1237_81095.pdf, accessed 15.09.2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hydrogen_sprinters = pd.read_csv(\n",
    "    main_path[\"inputs\"] + sub_path[\"pp_public\"] + input_file[\"hydrogen_sprinters\"], index_col=0, sep=\";\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hydrogen_sprinters.reset_index(drop=False, inplace=True)\n",
    "hydrogen_sprinters.rename(columns={\n",
    "    \"Jahr\": \"commissioned\",\n",
    "    \"Leistung\": \"capacity\"\n",
    "}, inplace=True)\n",
    "hydrogen_sprinters[\"commissioned\"] += 1\n",
    "hydrogen_sprinters[\"name\"] = [\n",
    "    f\"DE_transformer_hydrogen_new_built_{sprinter['commissioned']}\" \n",
    "    for index, sprinter in hydrogen_sprinters.iterrows()\n",
    "]\n",
    "hydrogen_sprinters[\"fuel\"] = \"hydrogen\"\n",
    "hydrogen_sprinters[\"technology\"] = \"GT\"\n",
    "hydrogen_sprinters.set_index(\"name\", drop=False, inplace=True)\n",
    "hydrogen_sprinters.index.name = None\n",
    "\n",
    "# Add hydrogen sprinters to data set for exogenous units\n",
    "conv_de_new = pd.concat([conv_de_new, hydrogen_sprinters])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign missing values for new commissioned plants\n",
    "\n",
    "* Assign new values / missing values in the data set containing new plants:\n",
    "    * energy sources &rarr; Use names consistent with OPSD naming conventions\n",
    "    * country &rarr; Assign Germany (DE) for all plants\n",
    "    * status &rarr; Set status to operating (for 2030 consideration)\n",
    "    * eeg status &rarr; Assign no, i.e. no RES power plants under the German Renewable Energies Act (EEG)\n",
    "    * commissioning year &rarr; commissioned_last is the same as initial commissioning year\n",
    "    * technology &rarr; See below for the logic used\n",
    "    * capacity &rarr; Has to be distributed for some units (see below)\n",
    "* Split data sets for power plants and pumped hydro as well as run of river plants since they are treated separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_de_new.loc[:,'fuel'].replace(\n",
    "    {'Pumpspeicher': 'Pumped storage',\n",
    "     'Erdgas': 'natgas',\n",
    "     'Kuppelgas': 'otherfossil',\n",
    "     'Sonstige': 'otherfossil',\n",
    "     'Mineralölprodukte': 'oil',\n",
    "     'Wasser': 'Run-of-river',\n",
    "     'Biomasse': 'biomass',\n",
    "     'Steinkohle': 'hardcoal',\n",
    "     'Mehrere Energieträger': 'natgas',\n",
    "     'Sonstige Energieträger\\n(nicht erneuerbar)': 'otherfossil'},\n",
    "    inplace=True)\n",
    "\n",
    "conv_de_new.loc[:,['country_geographical', 'country']] = 'DE'\n",
    "conv_de_new['status'] = 'operating'\n",
    "conv_de_new['eeg'] = 'no'\n",
    "conv_de_new['commissioned_last'] = conv_de_new['commissioned'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Technology is added according to the following logic:\n",
    "* If technology information does exist (new-built plants from the UBA list), assign a name that is consistent with the convention used here later on\n",
    "* If plant is a gas-fired combined cycle plant &rarr; Assign combined cycle (CC)\n",
    "* Else if plant is gas-fired &rarr; Assing gas turbine (GT)\n",
    "* Else &rarr; assign steam turbine (ST) for all remaining plants\n",
    "\n",
    "For some plants, no capacity values are given, instead a message _'be aware of 1.2 GW capacity limit'_ is stated.\n",
    "\n",
    "Energy regulatory background:\n",
    "* § 13k EnWG formerly allowed TSOs to build and operate gas-fired back up power plants up to an overall capacity of 2 GW resp. up to the amount of capacity which the German regulatory body (BNetzA) assumed to be necessary.\n",
    "* The BNetzA determined a maximum amount of 1.2 GW in 2017, see: https://www.bundesnetzagentur.de/SharedDocs/Downloads/DE/Sachgebiete/Energie/Unternehmen_Institutionen/Versorgungssicherheit/Berichte_Fallanalysen/BNetzA_Netzstabilitaetsanlagen13k.pdf?__blob=publicationFile&v=3 (accessed 22.10.2020)\n",
    "* These 1.2 GW served as a basis for the NEP 2030 (version 2019, 2nd draft) which is used here.\n",
    "* § 13k does no longer exist and is substituted by § 11 (3) EnWG which explicitly forbids TSOs to build and operate power plants by their own which would be not in line with the unbundling regulations.\n",
    "\n",
    "Approach applied here: The overall capacity of 1.2 GW for new-built gas power plants is equally split between the different plants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_de_new.loc[:, 'technology'].replace(\n",
    "    {'GuD': 'CC',\n",
    "     'GM': 'M',\n",
    "     'G/AK': 'CC',\n",
    "     'BGT': 'GT',\n",
    "     'BST': 'ST'}, \n",
    "    inplace=True)\n",
    "\n",
    "conditions = [(conv_de_new.name.str.contains('GUD|CCPP', \n",
    "                                          flags=IGNORECASE, \n",
    "                                          regex=True)) & (conv_de_new.fuel == 'natgas'),\n",
    "             (~conv_de_new.name.str.contains('GUD|CCPP', \n",
    "                                          flags=IGNORECASE, \n",
    "                                          regex=True)) & (conv_de_new.fuel == 'natgas')]\n",
    "choices = ['CC', 'GT']\n",
    "\n",
    "conv_de_new['technology_temp'] = np.select(conditions, choices, default='ST')\n",
    "\n",
    "conv_de_new.loc[conv_de_new['technology'].isna(), 'technology'] = conv_de_new['technology_temp']\n",
    "conv_de_new.drop(columns=['technology_temp'], inplace=True)\n",
    "\n",
    "amount = conv_de_new.loc[conv_de_new['capacity'] == '1,2-GW-Beschränkung beachten!'].shape[0]\n",
    "try:\n",
    "    abs_value = round(1200 / amount)\n",
    "    conv_de_new.loc[conv_de_new['capacity'] == '1,2-GW-Beschränkung beachten!', \n",
    "                'capacity'] = abs_value\n",
    "except ZeroDivisionError:\n",
    "    # For proper garbage collection\n",
    "    abs_value = None\n",
    "\n",
    "conv_de_new['capacity'] = conv_de_new['capacity'].astype(float)\n",
    "\n",
    "phes_de_new = conv_de_new.loc[conv_de_new['fuel'] == 'Pumped storage']\n",
    "ror_de_new = conv_de_new.loc[conv_de_new['fuel'] == 'Run-of-river']\n",
    "conv_de_new = conv_de_new.loc[~(conv_de_new.index.isin(phes_de_new.index.append(ror_de_new.index)))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume gas switch to hydrogen:\n",
    "* In light of the ongoing gas crisis, it is assumed that we won't see much natural gas installations anymore.\n",
    "* It is assumed that all power plants going operational after 2023 will already be fired with hydrogen.\n",
    "* Furthermore, it is assumed that from 2024 on, a gradual switch of existing units to hydrogen will be performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_de_new.loc[(conv_de_new.fuel == \"natgas\") & (conv_de_new.commissioned_last > 2023), \"fuel\"] = \"hydrogen\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### European power plant development\n",
    "\n",
    "For the European power plant park development, the TYNDP data is used.\n",
    "\n",
    "Steps applied:\n",
    "* Read in the TYNDP generation data (aggregated at the level of bidding zones):\n",
    "    * Use scenario data from scenario _Distributed Generation_ which has shares of 51% RES for electricity and 3.6% RES for gas in 2030\n",
    "    * Filter out the countries for closer consideration (using a regular expression)\n",
    "* Aggeregate capacities for country data set they belong to (for DK and FR)\n",
    "* Adjust naming of bidding zones to be consistent with the one used here\n",
    "* Rename column names such that they match the names for the energy carriers used here\n",
    "* Distribute aggregated data for Norway across the different bidding zones\n",
    "    * Capacities for Norway are aggregated for the bidding zones NO1, NO2 and NO5, but capacity information is needed at the level of individual bidding zones.\n",
    "    * As an assumption, capacities for the bidding zones are distributed in the same manner as capacities are distributed in the status quo. Therefore, the capacity shares in the status quo are determined in the first place.\n",
    "* Split the data set into conventional and RES as well as ROR data and do some renaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NO_dict = {}\n",
    "NO_zones = ['NO1', 'NO2', 'NO5']\n",
    "\n",
    "for country in NO_zones:\n",
    "    NO_dict[country] = conv_eu[\n",
    "        conv_eu['bidding_zone'] == country].capacity.sum()\n",
    "NO_cap = sum(NO_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_eu_2025_BEST = tools.extract_tyndp_capacities(\n",
    "    countries=countries_eu, no_dict=NO_dict, scenario=\"2025 BEST\",\n",
    "    path=main_path[\"inputs\"] + sub_path[\"pp_public\"] + input_file[\"tyndp_eu\"])\n",
    "\n",
    "pp_eu_2030_DG = tools.extract_tyndp_capacities(\n",
    "    countries=countries_eu, no_dict=NO_dict, scenario=\"2030 DG\",\n",
    "    path=main_path[\"inputs\"] + sub_path[\"pp_public\"] + input_file[\"tyndp_eu\"])\n",
    "\n",
    "RES_to_sep = ['solarPV', 'windonshore', 'windoffshore']\n",
    "Hydro_to_sep = ['PHES_capacity_pump', 'PHES_capacity_turbine']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_eu_2025_BEST = pp_eu_2025_BEST[(~pp_eu_2025_BEST['fuel'].isin(RES_to_sep + Hydro_to_sep))].copy()\n",
    "renewables_eu_2025_BEST = pp_eu_2025_BEST[pp_eu_2025_BEST['fuel'].isin(RES_to_sep)]\n",
    "pumpedstorage_eu_2025_BEST = pp_eu_2025_BEST.loc[pp_eu_2025_BEST['fuel'].isin(Hydro_to_sep)]\n",
    "\n",
    "conv_eu_2030_DG = pp_eu_2030_DG[(~pp_eu_2030_DG['fuel'].isin(RES_to_sep + Hydro_to_sep))].copy()\n",
    "renewables_eu_2030_DG = pp_eu_2030_DG[pp_eu_2030_DG['fuel'].isin(RES_to_sep)]\n",
    "pumpedstorage_eu_2030_DG = pp_eu_2030_DG.loc[pp_eu_2030_DG['fuel'].isin(Hydro_to_sep)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do some garbage collection\n",
    "try:\n",
    "    del (Hydro_to_sep, NO_cap, NO_dict, NO_zones, abs_value, amount, \n",
    "         cap1, cap2, choices, col, cols_to_drop, cols_to_drop_BNetzA,\n",
    "         comm_BNetzA, comm_BNetzA_newidx, comm_conv_de, cond1, cond2, conditions, countries_eu, country,\n",
    "         datteln4, decomm_hardcoal_col_names, dict_comm_BNetzA_names, dict_comm_col_names,\n",
    "         dict_nep_col_names, duplicated_comm1, duplicated_comm3, duplicated_comm_BNetzA2, duplicated_comm_BNetzA3,\n",
    "         duplicated_conv1, duplicated_conv2, el, empty_df_new, exclude, \n",
    "         filter_plants1, filter_plants2, filter_plants3,\n",
    "         idx1, idx2, idx_mapping1, include, interval, keys_BNetzA, last_idx_comm, match, \n",
    "         names, names_comm, names_comm_BNetzA, names_comm_BNetzA_set, names_comm_set, names_set,\n",
    "         nep_new, pp_eu, pp_eu_2025_BEST, pp_eu_2030_DG,\n",
    "         union1, union2, union3, values_BNetzA\n",
    "         )\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### European power plant development (UPDATE)\n",
    "Extract capacities from TYNDP using the following specifications:\n",
    "* Climate year: 2009; reasoning: relatively low wind, but not as extreme as 2010 &rarr; seeking to find a rather robust system configuration\n",
    "* Scenario: Distributed Energy; reasoning: chooses decentralised technologies over centralised ones; less imports &rarr; Less imports favoured in the context of geopolitical threads (Ukraine war, tensions in relations with China)\n",
    "* Countries considered: Electrical neighbours of Germany + Italy; reasoning: include exchange perspective while keeping system limited and complexity manageable.\n",
    "\n",
    "Steps applied:\n",
    "* Read in and slice data\n",
    "* Do some renaming\n",
    "* Collect capacity information for 2030, 2040, 2050 to be combined with status quo data set(s) in the latter course\n",
    "* Distribute aggregated data for Norway across the different bidding zones\n",
    "    * Capacities for Norway are aggregated for the bidding zones NO1, NO2 and NO5, but capacity information is needed at the level of individual bidding zones.\n",
    "    * As an assumption, capacities for the bidding zones are distributed in the same manner as capacities are distributed in the status quo. Therefore, the capacity shares in the status quo are determined in the first place.\n",
    "* Split pumped hydro and RES data from data set, hereby excluding otherres from the data set due to trouble in attribution.\n",
    "\n",
    "Sources:\n",
    "* ENTSO-E (2022): TYNDP 2022 Scenario Report. Version April 2022, https://2022.entsos-tyndp-scenarios.eu/wp-content/uploads/2022/04/TYNDP2022_Joint_Scenario_Full-Report-April-2022.pdf, accessed 18.11.2022.\n",
    "* ENTSO-E (2022): Updated Electricity Modelling Results, Capacity & Dispatch, TYNDP 2022 Scenario Report – additional Downloads, https://view.officeapps.live.com/op/view.aspx?src=https%3A%2F%2F2022.entsos-tyndp-scenarios.eu%2Fwp-content%2Fuploads%2F2022%2F04%2F220310_Updated_Electricity_Modelling_Results.xlsx&wdOrigin=BROWSELINK, accessed 18.11.2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NO_dict = {}\n",
    "NO_zones = ['NO1', 'NO2', 'NO5']\n",
    "\n",
    "for country in NO_zones:\n",
    "    NO_dict[country] = conv_eu[\n",
    "        conv_eu['bidding_zone'] == country].capacity.sum()\n",
    "NO_cap = sum(NO_dict.values())\n",
    "\n",
    "NO_cap_shares_dict = {}\n",
    "for k, v in NO_dict.items():\n",
    "    NO_cap_shares_dict[k] = float(v / sum(NO_dict.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tyndp2022_capacities = pd.read_excel(\n",
    "    f\"{main_path['inputs']}{sub_path['pp_public']}{input_file['tyndp_2022']}\",\n",
    "    sheet_name=\"Capacity & Dispatch\"\n",
    ")\n",
    "\n",
    "tyndp2022_nodes = {\n",
    "    \"AT00\": \"AT\",\n",
    "    \"BE00\": \"BE\",\n",
    "    \"CH00\": \"CH\",\n",
    "    \"CZ00\": \"CZ\",\n",
    "    \"DKE1\": \"DK2\",\n",
    "    \"DKKF\": \"DK1\",\n",
    "    \"DKW1\": \"DK1\",\n",
    "    \"FR00\": \"FR\",\n",
    "    \"FR15\": \"FR\",  # Corsica attributed to France\n",
    "    \"IT00\": \"IT\",\n",
    "    \"NL00\": \"NL\",\n",
    "    \"NOM1\": \"NO3\",\n",
    "    \"NON1\": \"NO4\",\n",
    "    \"NOS0\": \"NO1\",  # Split to NO1, NO2, NO5 according to current capacity shares\n",
    "    \"PL00\": \"PL\",\n",
    "    \"SE01\": \"SE1\",\n",
    "    \"SE02\": \"SE2\",\n",
    "    \"SE03\": \"SE3\",\n",
    "    \"SE04\": \"SE4\",\n",
    "}\n",
    "\n",
    "tyndp2022_fuels = {\n",
    "    \"Gas\": \"natgas\",\n",
    "    \"Hydro\": \"PHES\",\n",
    "    \"Oil\": \"oil\",\n",
    "    \"Other Non RES\": \"otherfossil\",\n",
    "    \"Other RES\": \"otherres\",\n",
    "    \"Solar\": \"solarPV\",\n",
    "    \"Wind Onshore\": \"windonshore\",\n",
    "    \"Wind Offshore\": \"windoffshore\",\n",
    "    \"Nuclear\": \"uranium\",\n",
    "    \"Coal & Lignite\": \"hardcoal\",\n",
    "    \"Solar_stand alone\": \"solarPV\",\n",
    "    \"Offshore Wind_stand alone\": \"windoffshore\"\n",
    "}\n",
    "\n",
    "tydnp2022_res_to_sep = [\n",
    "    \"solarPV\", \"windonshore\", \"windoffshore\"\n",
    "]\n",
    "\n",
    "tyndp2022_capacities = tyndp2022_capacities.loc[\n",
    "    (tyndp2022_capacities[\"Climate Year\"] == \"CY 2009\")\n",
    "    & (tyndp2022_capacities[\"Scenario\"] == \"Distributed Energy\")\n",
    "    & (tyndp2022_capacities[\"Node\"].isin(tyndp2022_nodes.keys()))\n",
    "    & (tyndp2022_capacities[\"Parameter\"] == \"Capacity (MW)\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do some renaming and reindexing\n",
    "tyndp2022_capacities[\"Country\"] = tyndp2022_capacities[\"Node\"].replace(tyndp2022_nodes)\n",
    "tyndp2022_capacities[\"Fuel\"] = tyndp2022_capacities[\"Fuel\"].replace(tyndp2022_fuels)\n",
    "tyndp2022_capacities.set_index([\"Country\", \"Fuel\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine capacity estimates:\n",
    "* Collect capacity values for 2030, 2040, 2050\n",
    "* Assign values to DataFrame holding installed power per year\n",
    "\n",
    "There are only few technology distinctions made, thus\n",
    "* Assume \"Hydro\" to be pumped hydro throughout since the previously used TYNDP 2018 considered pumped hydro only as well.\n",
    "* Assume for Poland - the only relevant used of lignite besides Germany - that half of the coal plants in 2030 will be lignite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# introduce new DataFrame to save capacity installed for a given year\n",
    "eu_power_plants = pd.DataFrame(\n",
    "    columns=[f\"{iter_year}-01-01\" for iter_year in range(2020, 2051)]\n",
    ")\n",
    "\n",
    "for iter_year in [2030, 2040, 2050]:\n",
    "    eu_power_plants[f\"{iter_year}-01-01\"] = (\n",
    "        tyndp2022_capacities.loc[tyndp2022_capacities[\"Year\"] == iter_year, \"Value\"].groupby(\n",
    "            tyndp2022_capacities.loc[tyndp2022_capacities[\"Year\"] == iter_year, \"Value\"].index\n",
    "        ).sum()\n",
    "    )\n",
    "\n",
    "eu_power_plants.index = pd.MultiIndex.from_tuples(eu_power_plants.index)\n",
    "\n",
    "eu_power_plants.loc[(\"PL\", \"hardcoal\")] = eu_power_plants.loc[(\"PL\", \"hardcoal\")] / 2\n",
    "eu_power_plants.loc[(\"PL\", \"lignite\"), :] = eu_power_plants.loc[(\"PL\", \"hardcoal\"), :]\n",
    "\n",
    "eu_power_plants[[\"2030-01-01\", \"2040-01-01\", \"2050-01-01\"]] = (\n",
    "    eu_power_plants[[\"2030-01-01\", \"2040-01-01\", \"2050-01-01\"]].fillna(0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle capacity shares for Norwegian market zones\n",
    "no_plants = eu_power_plants.loc[\n",
    "    eu_power_plants.index.get_level_values(0) == \"NO1\"\n",
    "]\n",
    "no2_plants = no_plants.rename(index={\"NO1\": \"NO2\"}).mul(NO_cap_shares_dict[\"NO2\"])\n",
    "no5_plants = no_plants.rename(index={\"NO1\": \"NO5\"}).mul(NO_cap_shares_dict[\"NO5\"])\n",
    "\n",
    "# No generation capacity in NO1\n",
    "eu_power_plants = pd.concat([\n",
    "    eu_power_plants, \n",
    "    no2_plants,\n",
    "    no5_plants\n",
    "]).drop(index=\"NO1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data sets for RES, pumped hydro and conventionals\n",
    "renewables_eu_tyndp2022 = eu_power_plants.loc[\n",
    "    eu_power_plants.index.get_level_values(1).isin(tydnp2022_res_to_sep)\n",
    "].astype(\"float64\")\n",
    "pumpedstorages_eu_tyndp2022 = eu_power_plants.loc[\n",
    "    eu_power_plants.index.get_level_values(1) == \"PHES\"\n",
    "].astype(\"float64\")\n",
    "conv_eu_tyndp2022 = eu_power_plants.loc[\n",
    "    ~(eu_power_plants.index.get_level_values(1).isin(tydnp2022_res_to_sep + [\"PHES\", \"otherres\"]))\n",
    "].astype(\"float64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign technology\n",
    "\n",
    "- Steps applied for Germany\n",
    "    - Use data with results from manual research / matching of department (ER_man)<br>\n",
    "      _Caution: Data set contains duplicates &rarr; take the first occurence and drop the rest_\n",
    "    - Fill missing technology data from OPSD\n",
    "    \n",
    "- Steps applied for Europe\n",
    "    - Take technology data from OPSD\n",
    "    - Further missing values are assigned by energy carrier in the following way:<br>\n",
    "      natgas &rarr; GT<br>\n",
    "      rest &rarr; ST\n",
    "    \n",
    "- Additional steps taken for both\n",
    "    - Rename original technology name\n",
    "    - If plants technology is 'CC' and fuel is not in natgas or oil, 'ST' is assigned instead\n",
    "    - A technology and fuel combination is specified in order to assign efficiencies on this basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "techs = pd.read_csv(\n",
    "    main_path[\"inputs\"] + sub_path[\"pp_er\"] + input_file[\"techs_de\"],\n",
    "    sep=\";\", decimal=\",\"\n",
    ")\n",
    "dict_techs_ER_man = {'T': 'GT', 'SPP': 'ST'}\n",
    "techs = techs.drop_duplicates(subset='bnaID').set_index('bnaID').replace(dict_techs_ER_man)\n",
    "\n",
    "dict_techs_ospd = {'Steam turbine': 'ST', \n",
    "                   'Gas turbine': 'GT', \n",
    "                   'Combined cycle': 'CC', \n",
    "                   'Combustion Engine': 'M'}\n",
    "\n",
    "conv_de['technology'].replace(dict_techs_ospd, inplace=True)\n",
    "\n",
    "conv_de = conv_de.join(techs['ERman'], how='left')\n",
    "conv_de.loc[~conv_de['ERman'].isna(), 'technology'] = conv_de.loc[~conv_de['ERman'].isna(), 'ERman']\n",
    "conv_de.drop(columns=['ERman'], inplace=True)\n",
    "\n",
    "conv_de.loc[(conv_de['technology'] == 'CC')\n",
    "            & (~conv_de['fuel'].isin(['natgas', 'oil'])), 'technology'] = 'ST'\n",
    "\n",
    "conv_de['tech_fuel'] = conv_de['technology'] + '_' + conv_de['fuel']\n",
    "\n",
    "dict_techs_ppmatch = {\n",
    "    'Steam Turbine': 'ST', \n",
    "    'OCGT': 'GT', \n",
    "    'CCGT': 'CC',\n",
    "    'CCGT, Thermal': 'CC',\n",
    "}\n",
    "\n",
    "conv_eu['technology'].replace(dict_techs_ppmatch, inplace=True)\n",
    "\n",
    "dict_tech_eu = {\n",
    "    'natgas': 'GT', \n",
    "    'hardcoal': 'ST', \n",
    "    'uranium': 'ST', \n",
    "    'otherfossil': 'ST',\n",
    "    'oil': 'ST',  # efficiency later assigned on is worse than for GT\n",
    "    'lignite': 'ST', \n",
    "    'biomass': 'ST',\n",
    "    'waste': 'ST',\n",
    "}\n",
    "\n",
    "conv_eu.loc[conv_eu['technology'].isna(), 'technology'] = (\n",
    "    conv_eu.loc[conv_eu['technology'].isna(), 'fuel'].replace(dict_tech_eu))\n",
    "\n",
    "conv_eu.loc[(conv_eu['technology'] == 'CC')\n",
    "            & (~conv_eu['fuel'].isin(['natgas', 'oil'])), 'technology'] = 'ST'\n",
    "\n",
    "conv_eu['tech_fuel'] = conv_eu['technology'] + '_' + conv_eu['fuel']\n",
    "\n",
    "conv_de_new['tech_fuel'] = conv_de_new['technology'] + '_' + conv_de_new['fuel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do some garbage collection\n",
    "try:\n",
    "    del (dict_techs_ER_man, dict_techs_ospd, dict_techs_ppmatch, techs)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign electrical efficiencies\n",
    "\n",
    "Data sources used and steps applied:\n",
    "- Basic data source is the given electrical efficiency information of OPSD\n",
    "- Missing values for efficiency are filled based on manual research by ER department. Several data sources are combined in the following priority order:\n",
    "    - manual researched efficiencies by Robin Claus (RC), 18.09.2018\n",
    "    - manual researched efficiencies from master thesis of Daniel Peschel (DP), 04.03.2016\n",
    "- Remaining missing values are filled by a linear regression approach from [DIW Data Documentation No. 72 from 2014](https://www.diw.de/documents/publikationen/73/diw_01.c.440963.de/diw_datadoc_2014-072.pdf) (see citation below)\n",
    "- For power plants with a latest commissioning date before 1950, the electrical efficiency value for the respective fuel / technology combination of 1990 is (arbitrarily) chosen.\n",
    "* For oil plants, a minimum efficiency of 30% is introduced.\n",
    "\n",
    "Egerer, Jonas, Gerbaulet, Clemens, Ihlenburg, Richard, Kunz, Friedrich, Reinhard, Benjamin, Hirschhausen, Christian von, Weber, Alexander, Weibezahn, Jens (2014): Electricity Sector Data for Policy-Relevant Modeling: Data Documentation and Applications to the German and European Electricity Markets, DIW and TU Berlin, WIP, DIW Data Documentation 72, Berlin, March 2014. © DIW Berlin, 2014.\n",
    "\n",
    "DIW regression / interpolation:\n",
    "- The efficiencies are based on the \"assumption table\" which includes the electrical efficiency information based on linear regressions by DIW.<br>\n",
    "- It includes values for: uranium (U/ST), lignite (L/ST), hard coal (H/ST), natural gas (NG/GT), oil (O/CB)\n",
    "- All other values were derived from research by Leticia Encinas Rosa (LER) and are time invariant (own assumptions)\n",
    "    \n",
    "For new-built power plants, the efficiency estimates from DIW are projected into the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data from manual research\n",
    "eff_RC = pd.read_csv(\n",
    "    main_path[\"inputs\"] + sub_path[\"pp_er\"] + input_file[\"eff_de\"], \n",
    "    index_col=0, sep=\";\", decimal=\",\").rename(\n",
    "        columns={'eta_el': 'efficiency_el_RC'})\n",
    "conv_de = conv_de.join(eff_RC['efficiency_el_RC'])\n",
    "\n",
    "chp_DP = pd.read_csv(\n",
    "    main_path[\"inputs\"] + sub_path[\"pp_er\"] + input_file[\"peschel\"], \n",
    "    index_col=1, sep=\";\", decimal=\",\"\n",
    ")\n",
    "chp_DP['efficiency_el_DP'] = chp_DP['Wirkungsgrad'].replace(0, np.nan)*0.01\n",
    "conv_de = conv_de.join(chp_DP['efficiency_el_DP'])\n",
    "\n",
    "conv_de.rename(columns = {'efficiency_data': 'efficiency_el_OPSD'}, inplace=True)\n",
    "\n",
    "conv_de['efficiency_el'] = np.nan\n",
    "\n",
    "# Fill the data gaps\n",
    "dict_eff_ordinv = ['efficiency_el_RC', 'efficiency_el_DP', 'efficiency_el_OPSD']\n",
    "for eff in dict_eff_ordinv:\n",
    "    conv_de.loc[(conv_de['efficiency_el'].isna()) & (~conv_de[eff].isna()), 'efficiency_el'] = (\n",
    "        conv_de.loc[(conv_de['efficiency_el'].isna()) & (~conv_de[eff].isna()), eff])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use DIW interpolation for efficiencies\n",
    "# For the geothermal plants in IT, 12% world wide average efficiency is used as a rough proxy,\n",
    "# see: https://doi.org/10.1016/j.geothermics.2013.11.001, acceseed 18.11.2022.\n",
    "assumptions_eff_el = pd.read_csv(\n",
    "    main_path[\"inputs\"] + sub_path[\"assumptions\"] + input_file[\"efficiencies_el\"],\n",
    "    index_col=0, sep=\";\", decimal=\",\"\n",
    ").astype(\"float64\")\n",
    "\n",
    "# Existing plants\n",
    "arr = conv_de.loc[conv_de['efficiency_el'].isna(), ['commissioned_last', 'tech_fuel']].to_numpy()\n",
    "conv_de.loc[conv_de['efficiency_el'].isna(), 'efficiency_el'] = (\n",
    "    [tools.assign_eff_el_interpol(yr, tf, assumptions_eff_el) \n",
    "     for yr, tf in arr])\n",
    "\n",
    "arr = conv_eu.loc[conv_eu['efficiency_el'].isna(), ['commissioned_last', 'tech_fuel']].to_numpy()\n",
    "conv_eu.loc[conv_eu['efficiency_el'].isna(), 'efficiency_el'] = (\n",
    "    [tools.assign_eff_el_interpol(yr, tf, assumptions_eff_el) \n",
    "     for yr, tf in arr])\n",
    "\n",
    "conv_de.drop(columns=['efficiency_el_OPSD', 'efficiency_source', 'efficiency_estimate',\n",
    "                      'efficiency_el_RC', 'efficiency_el_DP'], inplace=True)\n",
    "\n",
    "conv_de.loc[(conv_de['fuel'] == 'oil') & (conv_de['efficiency_el'] < 0.3), 'efficiency_el'] = 0.3\n",
    "\n",
    "# New built plants\n",
    "arr = conv_de_new.loc[:, ['commissioned_last', 'tech_fuel']].to_numpy()\n",
    "conv_de_new.loc[:, 'efficiency_el'] = (\n",
    "    [tools.assign_eff_el_interpol(yr, tf, assumptions_eff_el) \n",
    "     for yr, tf in arr])\n",
    "\n",
    "conv_de_new.drop(columns=['efficiency_data', 'efficiency_source', 'efficiency_estimate'], \n",
    "                 inplace=True)\n",
    "\n",
    "# Cap efficiency for lignite plants and otherfossil given unrealistically high values\n",
    "efficiency_cap = 0.45\n",
    "conv_de.loc[\n",
    "    (conv_de.fuel.isin([\"lignite\", \"otherfossil\"])) \n",
    "    & (conv_de.efficiency_el > efficiency_cap), \"efficiency_el\"] = efficiency_cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do some garbage collection\n",
    "try:\n",
    "    del (arr, assumptions_eff_el, chp_DP, dict_eff_ordinv, eff, eff_RC)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign CHP information\n",
    "\n",
    "Data sources used and steps applied:\n",
    "- The following CHP categories are distincted:\n",
    "    - industrial power plants (IPP)\n",
    "    - district heating power plants (CHP) and\n",
    "    - power plants that are dispatched solely on a electricity market basis, i.e. that do not have a minimum load profile to serve heat demand (EMB)\n",
    "- The following sources are evaluated in order to obtain information on CHP eligibility and type:\n",
    "    - OPSD power plant list (information if a plant uses CHP is from BNetzA; the type, i.e., IPP, CHP (district heating) is from UBA)\n",
    "    - NEP 2030 power plant list\n",
    "    - manual research information from master thesis of DP, 04.03.2016\n",
    "\n",
    "> _Note: CHP information is used in POMMES to assign minimum load values resp. profiles for these plants.<br>Heat usage in turn is out of the current scope of POMMES_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if year != 2017:\n",
    "    # Add missing IPP information for new power plants in OPSD list (ones not in list as of 2017)\n",
    "    companies = '|'.join(['BMW', 'K\\+S', 'Papierfabrik'])\n",
    "    diff_ix = ['BNA0083_SD', 'BNA0085b_SD', 'BNA0804', 'BNA1911', 'BNA1925', 'BNA1926',\n",
    "               'BNA1927', 'BNA1934', 'BNA1935', 'BNA1936', 'BNA1937', 'BNA1938',\n",
    "               'BNA1939', 'BNA1942', 'BNA1944', 'BNA1945', 'BNA1946', 'BNa1947']\n",
    "    conv_de.loc[conv_de.index.isin(diff_ix) & \n",
    "                conv_de['company'].str.contains(companies), 'type'] = 'IPP'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from OPSD\n",
    "conv_de.rename(columns={'chp': 'OPSD_chp_bool', \n",
    "                        'type': 'OPSD_chp_type'}, inplace=True)\n",
    "chp_type = conv_de[['OPSD_chp_bool', 'OPSD_chp_type']].copy()\n",
    "chp_type['OPSD_ipp_bool'] = np.where(chp_type['OPSD_chp_type'] == 'IPP', 'yes', 'no')\n",
    "\n",
    "# from NEP 2030\n",
    "chp_type = chp_type.join(nep[['Wärmeauskopplung KWK (Ja/Nein)', \n",
    "                              'Annahmen ÜNB:\\nIndustriekraftwerk (Ja/Nein)']]).rename(\n",
    "    columns={'Wärmeauskopplung KWK (Ja/Nein)': 'NEP_chp_bool',\n",
    "             'Annahmen ÜNB:\\nIndustriekraftwerk (Ja/Nein)': 'NEP_ipp_bool'})\n",
    "\n",
    "chp_type_new = conv_de_new[['NEP_chp_bool', 'NEP_ipp_bool']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Existing plants\n",
    "chp_type['chp_bool'] = np.where((chp_type['OPSD_chp_bool'] == 'yes') | (chp_type['NEP_chp_bool'] == 'Ja'),\n",
    "                                'yes', 'no')\n",
    "\n",
    "conditions = [(chp_type['chp_bool'] == 'yes') \n",
    "              & (chp_type['OPSD_ipp_bool'] != 'yes') & (chp_type['NEP_ipp_bool'] != 'Ja'),\n",
    "              (chp_type['OPSD_ipp_bool'] == 'yes') | (chp_type['NEP_ipp_bool'] == 'Ja')]\n",
    "choices = ['chp', 'ipp']\n",
    "\n",
    "chp_type['chp_type'] = np.select(conditions, choices, default='emb')\n",
    "\n",
    "conv_de.drop(columns=['OPSD_chp_bool', 'OPSD_chp_type'], inplace=True)\n",
    "conv_de = conv_de.join(chp_type[['chp_bool', 'chp_type']], how='left').rename(columns={'chp_type': 'type'})\n",
    "\n",
    "# New built plants\n",
    "chp_type_new['chp_bool'] = np.where(chp_type_new['NEP_chp_bool'] == 'Ja',\n",
    "                                    'yes', 'no')\n",
    "\n",
    "conditions = [(chp_type_new['chp_bool'] == 'yes') & (chp_type_new['NEP_ipp_bool'] == 'Nein'), \n",
    "              chp_type_new['NEP_ipp_bool'] == 'Ja']\n",
    "choices = ['chp', 'ipp']\n",
    "\n",
    "chp_type_new['chp_type'] = np.select(conditions, choices, default='emb')\n",
    "\n",
    "conv_de_new.drop(columns=['NEP_chp_bool', 'NEP_ipp_bool', 'type', 'chp'], inplace=True)\n",
    "conv_de_new = conv_de_new.join(chp_type_new[['chp_bool', 'chp_type']], how='left').rename(\n",
    "    columns={'chp_type': 'type'})\n",
    "\n",
    "# By assumption all pps in EU are emb\n",
    "conv_eu['type'] = 'emb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do some garbage collection\n",
    "try:\n",
    "    del (choices, chp_type, chp_type_new, companies, conditions, diff_ix, nep)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop information for power plants not needed\n",
    "\n",
    "Steps applied:\n",
    "- Remove plants that were shutdown prior to 2017 from the power plants list &rarr; _NOTE: 2017 is the base year used for historical backcasting. In priciple, any year up to 2018 (state of the data set) can be used here withouth affecting the future power plants data set that is created afterwards._\n",
    "- Drop all information not needed for the model run in _POMMES_ (for the German and the European power plants list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# German power plants\n",
    "conv_de.drop(index=conv_de[conv_de['shutdown'] < year].index, inplace=True)\n",
    "conv_de.drop(index=conv_de[~((conv_de['status']=='shutdown') |\n",
    "                             (conv_de['status']=='operating'))].index, inplace=True)\n",
    "\n",
    "conv_de.drop(columns=['name', 'block_bnetza', 'name_uba', 'company', 'state', 'lat', 'lon',\n",
    "                      'country_geographical',  'technology', 'chp_capacity_uba',\n",
    "                      'commissioned', 'commissioned_original', 'retrofit', 'eic_code',\n",
    "                      'eic_code_block', 'eeg', 'network_node', 'voltage', 'chp_bool'], inplace=True)\n",
    "\n",
    "# European power plants\n",
    "conv_eu = conv_eu[['fuel', 'capacity', 'efficiency_el', 'bidding_zone', 'tech_fuel', 'type']]\n",
    "conv_eu.rename(columns={'bidding_zone': 'country'}, inplace=True)\n",
    "\n",
    "# New built German power plants\n",
    "conv_de_new.drop(columns=['name', 'block_bnetza', 'name_uba', 'company', 'state', 'lat', 'lon',\n",
    "                      'country_geographical',  'technology', 'chp_capacity_uba',\n",
    "                      'commissioned_original', 'retrofit', 'eic_code',\n",
    "                      'eic_code_block', 'eeg', 'network_node', 'voltage', 'chp_bool'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign commissioning and decomissioning estimates\n",
    "\n",
    "Steps applied:\n",
    "- For plants which don't have a commissioning date, assign 1990\n",
    "- Calculate year when plant should be commissioned based on a lifetime calculation\n",
    "- Create a table which is indexed by power plants (index) and years (columns)\n",
    "- Fill in commissioning / decommissioning information:\n",
    "    - If a plant is commissioned in a certain year, the respective cell contains its capacity with a positive sign\n",
    "    - If a plant is decommissioned in a certain year, the respective cell contains its capacity with a negative sign\n",
    "\n",
    "**Logic for determining decommissioning years:**\n",
    "* Check if decommissioning year is prior to the starting time of the simulation run\n",
    "* If so, check what type of plant is given:\n",
    "    * For some plants, shutdown information has been put together by Julien Faist in a decommissionings list. Primary data sources are:\n",
    "        * BNetzA (2019): Kraftwerksstilllegungsanzeigenliste, as of 01.04.2019, https://www.bundesnetzagentur.de/DE/Sachgebiete/ElektrizitaetundGas/Unternehmen_Institutionen/Versorgungssicherheit/Erzeugungskapazitaeten/KWSAL/KWSAL_node.html, accessed prior to 01.04.2020.\n",
    "        * BNetzA (2019): Kraftwerksliste zum erwarteten Zu. und Rückbau 2019 bis 2022, as of 01.04.2019, https://www.bundesnetzagentur.de/DE/Sachgebiete/ElektrizitaetundGas/Unternehmen_Institutionen/Versorgungssicherheit/Erzeugungskapazitaeten/Kraftwerksliste/kraftwerksliste-node.html, accessed prior to 01.04.2020.\n",
    "        * AtG: https://www.gesetze-im-internet.de/atg/, accessed 03.11.2020.\n",
    "    * For coal power plans, a shutdown plan is derived from the KVBG: https://www.gesetze-im-internet.de/kvbg/, accessed 03.11.2020, which in turn is based on the recommendations made by the so-called \"Kohlekommission\" (Abschlussbericht der Kommission „Wachstum, Strukturwandel und Beschäftigung“ 2019). The more ambitious strategy is used, i.e. a shutdown of all power plants until 2035 already instead of 2038. The data has been put together by Julien Faist in a decommissionings list for coal power plants.\n",
    "    * For the remaining plants, decommissionings occur in the x years (default: x=10) following a number of years (offset; default=5) after the start year, i.e. the one for which no OPSD data is available anymore. In each year, around 100/x% of the overall capacity to be decommissioned is actually decommissioned, whereby only entire blocks or plants are decommissioned. The order is determined by plant age and electrical efficiency.\n",
    "* Else set decommissioning year to commissioning year + unit lifetime\n",
    "\n",
    "> _NOTE:_\n",
    "> _For decommissionings based on unit age, the five years after the start year are chosen since_\n",
    "> * _Some plants have already exceeded their lifetime by far and_\n",
    "> * _Plant operators are obliged to report shutdowns planned within the next couple of years._\n",
    "\n",
    "Updated sources:\n",
    "* BNetzA (2020): Kraftwerksstilllegungsanzeigenliste, as of 15.04.2020, https://www.bundesnetzagentur.de/DE/Sachgebiete/ElektrizitaetundGas/Unternehmen_Institutionen/Versorgungssicherheit/Erzeugungskapazitaeten/KWSAL/KWSAL_node.html, accessed 05.01.2021.\n",
    "* BNetzA (2020): Kraftwerksliste Bundesnetzagentur zum erwarteten Zu- und Rückbau 2019 bis 2022, https://www.bundesnetzagentur.de/DE/Sachgebiete/ElektrizitaetundGas/Unternehmen_Institutionen/Versorgungssicherheit/Erzeugungskapazitaeten/Kraftwerksliste/kraftwerksliste-node.html, as of 01.04.2020, accessed 05.01.2021."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add exogeneous decommissioning info (KWSAL, AtG, other)\n",
    "\n",
    "Steps applied:\n",
    "* Include exogenous decommissioning information which has been put together by JF.\n",
    "* Include exogeneous decommissioning information provided by the BNetzA from the power plants list on expected commissionings and decommissionings as well as the power plants list for reported shutdown plans (KWSAL).\n",
    "* Handle duplicates in the BNetzA lists.\n",
    "* Exclude system relevant power plants (which are assumed to be kept online).\n",
    "* Determine the plants in the decommissioning data set which are still included in the OPSD data set.\n",
    "* Assign shutdown dates either from Julien's research or from the updated BNetzA list for those plants.\n",
    "* Assing decommissioning assumptions for the remaining plants:\n",
    "    * Filter for plants in the KWSAL which are not system-relevant and which do neither appear in the decommissionings lists already evaluated (JF's list and the BNetzA commissionings and decommissionings list) nor in the data set of the conventional plants for Germany.\n",
    "    * Assign a (single) shutdown year by assumption.\n",
    "\n",
    "> _NOTE:_\n",
    "> * _The system relevance decision has to be renewed at least every two years by the TSOS in agreement with the BNetzA. So the respective plants might be decommissioned later on._\n",
    "> * _There are some plants which are not included in the conv_de dataset anymore, but still appear in the KWSAL. This is due to the fact that their status is not 'operational' anymore. Hence, these plants are already some kinds of special cases outside the EOM such as security reserve of lignite units and thus do not (explicitly) need to be considered in POMMES._\n",
    "> _The remainder of plants is either dropped from the conventional data set due to its shutdown status or already covered by the other decommissionings lists._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Research by JF\n",
    "decomm_conv_de = pd.read_csv(\n",
    "    main_path[\"inputs\"] + sub_path[\"pp_er\"] + input_file[\"decomm\"],\n",
    "    index_col=0, sep=\";\"\n",
    ")\n",
    "\n",
    "decomm_conv_de = decomm_conv_de[decomm_conv_de['Systemrelevanz'].isna()]\n",
    "\n",
    "decomm_idx = decomm_conv_de.index.intersection(conv_de.index)\n",
    "\n",
    "conv_de.loc[decomm_idx,'shutdown'] = decomm_conv_de.loc[decomm_idx, 'Geplante Ausserbetriebnahme']\n",
    "\n",
    "# BNetzA list\n",
    "decomm_BNetzA = pd.read_excel(\n",
    "    main_path[\"inputs\"] + sub_path[\"pp_public\"] + input_file[\"comm_decomm_BNetzA\"],\n",
    "    sheet_name='Zu und Rückbau Bund Tab', skiprows=24, nrows=28, index_col=0\n",
    ")\n",
    "\n",
    "trimmed_idx = decomm_BNetzA.index.str.strip('*').str.split(', ')\n",
    "decomm_ids = [item for sublist in list(trimmed_idx) for item in sublist]\n",
    "\n",
    "# Create a new entry for each power plant in BNetzA list\n",
    "for idx in list(trimmed_idx):\n",
    "    if len(idx) > 1:\n",
    "        for subidx in idx:\n",
    "            decomm_BNetzA.loc[subidx] = decomm_BNetzA.loc[decomm_BNetzA.index.str.contains(subidx)].values[0]\n",
    "            \n",
    "conv_de.loc[[el for el in decomm_ids \n",
    "             if el in conv_de.index], 'shutdown'] = decomm_BNetzA[\n",
    "    'Voraussichtlicher Zeitpunkt der endgültigen Aufgabe (Jahr) gemäß Unternehmensplanung']\n",
    "\n",
    "# Do some corrections\n",
    "conv_de.at['BNA0969b', 'shutdown'] = decomm_BNetzA.at[\n",
    "    'BNA0969b*', \n",
    "    'Voraussichtlicher Zeitpunkt der endgültigen Aufgabe (Jahr) gemäß Unternehmensplanung']\n",
    "\n",
    "conv_de['shutdown'].replace({'2020 bis 2022': '2021',\n",
    "                             '2021 bis 2023': '2022'}, inplace=True)\n",
    "\n",
    "# KWSAL list (BDEW)\n",
    "decomm_KWSAL = pd.read_csv(\n",
    "    main_path[\"inputs\"] + sub_path[\"pp_public\"] + input_file[\"KWSAL\"],\n",
    "    encoding=\"latin_1\", index_col=0, delimiter=';', decimal=\",\"\n",
    ")\n",
    "\n",
    "unique_KWSAL_idx = decomm_KWSAL.index.str.split('\\n')\n",
    "unique_KWSAL_id_flat = [item for sublist in unique_KWSAL_idx for item in sublist]\n",
    "\n",
    "for idx in list(unique_KWSAL_idx):\n",
    "    if len(idx) > 1:\n",
    "        for subidx in idx:\n",
    "            decomm_KWSAL.loc[subidx] = decomm_KWSAL.loc[decomm_KWSAL.index.str.contains(subidx)].values[0]\n",
    "\n",
    "# Handle different approaches for HKW Wilmersdorf (blocks 2 and 3 are aggregated, while block 1 received a new index)\n",
    "duplicated_KWSAL_idx = {}\n",
    "\n",
    "decomm_KWSAL_multi = decomm_KWSAL.set_index('Kraftwerksblock', append=True)\n",
    "duplicated_KWSAL = decomm_KWSAL_multi[(decomm_KWSAL_multi.index.get_level_values(0).duplicated(keep=False))\n",
    "                         & (decomm_KWSAL_multi.index.get_level_values(1) == 'HKW Wilmersdorf GT 1')].index\n",
    "\n",
    "for el in duplicated_KWSAL:\n",
    "    idx_el = decomm_KWSAL_multi.index.get_loc(el)\n",
    "    duplicated_KWSAL_idx[el[0]] = idx_el\n",
    "\n",
    "as_list = decomm_KWSAL.index.tolist()\n",
    "for k, v in duplicated_KWSAL_idx.items():\n",
    "    as_list[v] = k + '_SD'\n",
    "decomm_KWSAL.index = as_list\n",
    "\n",
    "decomm_KWSAL = decomm_KWSAL.groupby(decomm_KWSAL.index).agg({\n",
    "    'Kraftwerksbetreiber': 'first', \n",
    "    'Kraftwerksblock': sum,\n",
    "    'Netto-Nennleistung in MW laut KW-Liste': sum,\n",
    "    'Stilllegungsanzeigentyp': 'first',\n",
    "    'Systemrelevanz von zur Stilllegung angezeigten KW-Blöcken gemäß ÜNB': 'first'})\n",
    "\n",
    "sd_plants = decomm_KWSAL[\n",
    "    ~(decomm_KWSAL['Systemrelevanz von zur Stilllegung angezeigten KW-Blöcken gemäß ÜNB'] == 'x')].index\n",
    "\n",
    "conv_de.loc[conv_de.shutdown.isna() & \n",
    "            conv_de.index.isin(conv_de.index.intersection(sd_plants)), 'shutdown'] = shutdown_assumption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add exogenous decommissioning for coal (KVBG)\n",
    "* Advance the coal phase-out to 2030 from 2035.\n",
    "    * For hardcoal, include phase out based on tenders. For the remainder, advance the phase out based on assumptions.\n",
    "    * For lignite, include phase out of small plants from tenders. Base the phase out of the remainder on assumptions.\n",
    "    * Also take into account the current critical gas supply situations leading to a prolonged operation until 2023 for plants to be shutdown in the near future.\n",
    "\n",
    "Steps applied:\n",
    "* Include exogenous decommissioning information which has been put together by Julien Faist from the German Kohleverstromungsbeendigungsgesetz (KVBG) based on the Kohlekommission's recommendations for a coal phase out plan until 2035.\n",
    "* Add plants from hardcoal shutdown tenders. BNetzA identifiers have been manually added to avoid complicated string mapping.\n",
    "* Advance the lignite and hardcoal phase out to 2030 according to the following logic:\n",
    "    * In principle, shutdown of plants that were succesful in shutdown tenders are assigned the next full year when they are no longer operational. But due to the ongoing gas shortage and the so-called Ersatzkraftwerkebereithaltungsgesetz, they are not assumed to be shut down prior to 2024.\n",
    "    * Shutdown planned in 2025: Shutdown is made one year earlier.\n",
    "    * Shutdown planned in 2026-2027: Shutdown is made two years earlier.\n",
    "    * Shutdown planned in 2028-2030: Shutdown is made three years earlier.\n",
    "    * Shutdown planned in between 2030 and 2031: Shutdown is made in 2028.\n",
    "    * Shutdown planned in between 2032 and 2033: Shutdown is made in 2029.\n",
    "    * Shutdown planned in between 2033 and 2035: Shutdown is made in 2030.\n",
    "* Determine the plants from coal phase out list occuring in the conventional power plants list (from OPSD) but not in the already evaluated decommissioning list from above.\n",
    "\n",
    "Sources:\n",
    "* Tender results for the phase out of hard coal:\n",
    "BNetzA (2022): Kohleausstieg, Ausschreibungen, https://www.bundesnetzagentur.de/DE/Fachthemen/ElektrizitaetundGas/Kohleausstieg/start.html, accessed 27.05.2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_tenders_to_shutdown_year = {\n",
    "    'hardcoal_tender_1': 2022+2,\n",
    "    'hardcoal_tender_2': 2022+2,\n",
    "    'hardcoal_tender_3': 2023+1,\n",
    "    'hardcoal_tender_4': 2024,\n",
    "    'hardcoal_tender_5': 2025,\n",
    "}\n",
    "to_concat = []\n",
    "for key in map_tenders_to_shutdown_year.keys():\n",
    "    tender_plants = pd.read_excel(\n",
    "        main_path[\"inputs\"] + sub_path[\"pp_public\"] + input_file[key],\n",
    "        skiprows=8, index_col=0, usecols=list(range(4))\n",
    "    )\n",
    "    tender_plants[\"shutdown\"] = map_tenders_to_shutdown_year[key]\n",
    "    to_concat.append(tender_plants)\n",
    "plants_to_decommission = pd.concat(to_concat)\n",
    "\n",
    "# Combine tenders and research from JF for hardcoal\n",
    "decomm_hardcoal_de = pd.merge(\n",
    "    decomm_hardcoal_de, plants_to_decommission, \n",
    "    left_index=True, \n",
    "    right_index=True, \n",
    "    how=\"left\"\n",
    ")\n",
    "decomm_hardcoal_de.loc[decomm_hardcoal_de[\"shutdown\"].notna(), \"Geplante Ausserbetriebnahme\"] = decomm_hardcoal_de[\"shutdown\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in and combine data sets for lignite and hardcoal\n",
    "decomm_lignite_de = pd.read_csv(\n",
    "    main_path[\"inputs\"] + sub_path[\"pp_er\"] + input_file[\"decomm_lignite\"],\n",
    "    index_col=0, sep=\";\", decimal=\",\", usecols=list(range(13))\n",
    ")\n",
    "\n",
    "# Combine tenders and research from JF for hardcoal\n",
    "decomm_lignite_de = pd.merge(\n",
    "    decomm_lignite_de, plants_to_decommission, \n",
    "    left_index=True, \n",
    "    right_index=True, \n",
    "    how=\"left\"\n",
    ")\n",
    "decomm_lignite_de.loc[decomm_lignite_de[\"shutdown\"].notna(), \"Geplante Ausserbetriebnahme\"] = decomm_lignite_de[\"shutdown\"]\n",
    "\n",
    "# Advance lignite and hardcoal phase out\n",
    "for coal_decomm_df in [decomm_hardcoal_de, decomm_lignite_de]:\n",
    "    coal_decomm_df.loc[coal_decomm_df[\"Geplante Ausserbetriebnahme\"] == 2025, \"Geplante Ausserbetriebnahme\"] -= 1\n",
    "    coal_decomm_df.loc[coal_decomm_df[\"Geplante Ausserbetriebnahme\"].isin(range(2026, 2028)), \"Geplante Ausserbetriebnahme\"] -= 2\n",
    "    coal_decomm_df.loc[coal_decomm_df[\"Geplante Ausserbetriebnahme\"].isin(range(2028, 2031)), \"Geplante Ausserbetriebnahme\"] -= 3\n",
    "    coal_decomm_df.loc[coal_decomm_df[\"Geplante Ausserbetriebnahme\"].isin(range(2030, 2032)), \"Geplante Ausserbetriebnahme\"] = 2028\n",
    "    coal_decomm_df.loc[coal_decomm_df[\"Geplante Ausserbetriebnahme\"].isin(range(2032, 2034)), \"Geplante Ausserbetriebnahme\"] = 2029\n",
    "    coal_decomm_df.loc[coal_decomm_df[\"Geplante Ausserbetriebnahme\"] > 2033, \"Geplante Ausserbetriebnahme\"] = 2030\n",
    "\n",
    "decomm_coal_de = pd.concat([decomm_lignite_de, decomm_hardcoal_de])\n",
    "\n",
    "# Ensure only existing units to be shut down; new-built one(s), i.e. only Datteln 4, is treated separately\n",
    "decomm_coal_idx = pd.Index([el for el in decomm_coal_de.index \n",
    "                            if el in conv_de.index])\n",
    "\n",
    "# Assign shutdown dates from Julien's research and updated advancing of coal phase out (possible path derived from KVBG)\n",
    "conv_de.loc[decomm_coal_idx,'shutdown'] = decomm_coal_de.loc[decomm_coal_idx, 'Geplante Ausserbetriebnahme']\n",
    "\n",
    "all_decomm_idx = decomm_coal_idx.union(decomm_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add exogeneous decommissionings based on unit lifetime or assumption; Include technology assumptions\n",
    "\n",
    "Steps applied:\n",
    "* Combine data sets for existing and new-built units with some technology assumptions given in a separate file.\n",
    "* Unit lifetime is determined based on assumptions. &rarr; _NOTE: Unit lifetimes given here seem to be rather conservative estimates. It is assumed that unit lifetime estimates from the literature can be prolongued by 20 years through retrofits._\n",
    "* Decommissioning information for new-built plants &rarr; Solely based on unit age except for the one coal plant 'Datteln 4'; no extension of lifetimes is allowed through retrofits.\n",
    "* Plants which have exceeded their lifetime are to be decomissioned based on unit age: Decommissionings occur in the x years (default: x=10) following a number of years (offset; default=5) after the start year, i.e. the one for which no OPSD data is available anymore. In each year, around 100/x% of the overall capacity to be decommissioned is actually decommissioned, whereby only entire blocks or plants are decommissioned. The order is determined by plant age and electrical efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assumptions = pd.read_csv(\n",
    "    main_path[\"inputs\"] + sub_path[\"assumptions\"] + input_file[\"tech_assumptions\"], \n",
    "    index_col=0, sep=\";\", decimal=\",\"\n",
    ")\n",
    "assumptions.drop(columns=['min_load_factor'], inplace=True)\n",
    "\n",
    "conv_de = conv_de.merge(assumptions, left_on='tech_fuel', right_index=True, how='left')\n",
    "conv_de_new = conv_de_new.merge(assumptions, left_on='tech_fuel', right_index=True, how='left')\n",
    "\n",
    "# Assumption: power plants without commissioning year were commissioned in 1990\n",
    "conv_de.loc[conv_de['commissioned_last'].isna(), 'commissioned_last'] = 1990\n",
    "\n",
    "# start_year is the one following the latest OPSD data state (end of 2020)\n",
    "# unit lifetimes are assumed to be prolongued by 20 years through retroffiting\n",
    "start_year = 2021\n",
    "conv_de['decommissioned_calc'] = conv_de['commissioned_last'] + conv_de['unit_lifetime'] + 20\n",
    "conv_de.loc[conv_de['shutdown'].isna(), 'shutdown'] = conv_de['decommissioned_calc']\n",
    "\n",
    "plants_to_decomm = conv_de.loc[~(conv_de['shutdown'].isna()) \n",
    "                               & (conv_de['decommissioned_calc'] <= start_year) \n",
    "                               & ~(conv_de['fuel'] == 'uranium')]\n",
    "\n",
    "# New plants: Decommissioned based on unit age; no extension through retrofits\n",
    "conv_de_new['decommissioned_calc'] = conv_de_new['commissioned_last'] + conv_de_new['unit_lifetime']\n",
    "conv_de_new.loc[conv_de_new['shutdown'].isna(), 'shutdown'] = conv_de_new['decommissioned_calc']\n",
    "\n",
    "# Handle plants that already exceeded their calculated lifetimes\n",
    "plants_to_decomm = plants_to_decomm.sort_values(by=['commissioned_last','efficiency_el'], \n",
    "                                                ascending=True)\n",
    "\n",
    "# x is the number of years over which the decommissionings shall be spread\n",
    "# offset is the number of years after the start year to start with the decommissionings\n",
    "x = 10\n",
    "offset = 5\n",
    "number_plants = plants_to_decomm.shape[0]\n",
    "plants_per_year = round(number_plants / x)\n",
    "counter = 0\n",
    "\n",
    "for iter_year in range(start_year+offset, start_year+offset+x-1):    \n",
    "    idx = plants_to_decomm.iloc[counter:counter+plants_per_year].index\n",
    "    conv_de.loc[idx, 'shutdown'] = iter_year\n",
    "    counter += plants_per_year\n",
    "\n",
    "idx = plants_to_decomm.iloc[counter:].index\n",
    "conv_de.loc[idx, 'shutdown'] = year+offset+x-1\n",
    "\n",
    "conv_de['shutdown'] = conv_de['shutdown'].astype(int)\n",
    "conv_de_new['shutdown'] = conv_de_new['shutdown'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data sets for investment and dispatch modelling\n",
    "\n",
    "* Dispatch data set\n",
    "    * Drop the already decommissioned plants as of the target year for dispatch modelling\n",
    "    * Drop the new plants going into operation earlier than selected target year for dispatch modelling\n",
    "* Investment data set\n",
    "    * Keep all capacities in the investment data set (make a copy of data set)\n",
    "    * Calculate the capacity available for each single year later on in the units' clustering (see [section below](#Transformers-for-usage-in-the-investment-model))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decomm_idx = conv_de[conv_de['shutdown'] < year].index\n",
    "cap_for_decommissioning = conv_de.loc[decomm_idx, \"capacity\"].sum()\n",
    "decomm_new_idx = conv_de_new[conv_de_new['shutdown'] < year].index\n",
    "\n",
    "# Separate existing data sets\n",
    "all_conv_de = conv_de.copy()\n",
    "conv_de = conv_de.drop(decomm_idx)\n",
    "remaining_capacity_conv = conv_de.capacity.sum()\n",
    "\n",
    "# Separate new-built data sets\n",
    "all_conv_de_new = conv_de_new.copy()\n",
    "conv_de_new = conv_de_new.loc[conv_de_new[\"commissioned\"] < year]\n",
    "\n",
    "print(f\"Summary statistics on capacity development for Germany until {year}:\")\n",
    "print(66 * \"-\")\n",
    "print(f'Overall capacity to be decommissioned:\\t\\t{round(cap_for_decommissioning):,} MW')\n",
    "print(f'Remaining capacity (excluding new-built units):\\t{round(remaining_capacity_conv):,} MW')\n",
    "print(f'Overall capacity of new-built power plants:\\t{round(conv_de_new.capacity.sum()):,} MW')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do some garbage collection\n",
    "try:\n",
    "    del (all_decomm_idx, as_list, cap_for_decommissioning, counter,\n",
    "         decomm_BNetzA, decomm_KWSAL, decomm_KWSAL_multi, \n",
    "         decomm_coal_de, decomm_coal_idx, decomm_conv_de,\n",
    "         decomm_hardcoal_de, decomm_ids, decomm_idx, decomm_lignite_de,\n",
    "         decomm_new_idx, duplicated_KWSAL, duplicated_KWSAL_idx, el, idx, idx_el,\n",
    "         iter_year, k, number_plants, offset, plants_per_year, plants_to_decomm,\n",
    "         sd_plants, shutdown_assumption, start_year, subidx, \n",
    "         trimmed_idx, unique_KWSAL_id_flat, unique_KWSAL_idx, v, x)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add new-built plants to dispatch data set by assumption for preventing loss of load\n",
    "\n",
    "Please refer to [this section](#Transformers-and-commodity-sources-for-the-target-year) below since data for RES is needed for proper adequacy evaluation.\n",
    "\n",
    "> _NOTE:_\n",
    "> * _The overall capacity for the target year derived from plans for plants to be commissioned might not be sufficient to meet the demand._\n",
    "> * _Hence, if necessary, the capacity gaps are filled up according to the scheme of the capacity balance (sheet) for Germany._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign minimum load profiles\n",
    "Steps applied:\n",
    "- Non-CHP and elictricity market-based CHP ('emb') units are assigned a minimum load value of zero, hence, no minimum load is applicable for these units and they can be switched off.\n",
    "- District heating ('chp') plants and IPPs are assigned a time variant minimum load, depending on heating demand and industry shift profiles, respectively. This is done by using the *disaggregator*, which was developed in the project [DemandRegio](https://www.er.tu-berlin.de/menue/forschung/abgeschlossene_forschungsvorhaben/demandregio/) and is available in the corresponding [GitHub repository](https://github.com/DemandRegioTeam/disaggregator). Those load profiles are preprocessed using another notebook. If a powerplant has an individual load profile is finally checked in the _pommesdispatch_ model\n",
    "- Powerplants that were not able to be assigned with this procedure are assigned using selected load profiles of single power plants from 2017 according to their fuel type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_de['min_load_LP'] = 0\n",
    "conv_de_new['min_load_LP'] = 0\n",
    "all_conv_de['min_load_LP'] = 0\n",
    "all_conv_de_new['min_load_LP'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_loads_dh = pd.read_csv(\n",
    "    main_path[\"inputs\"] + sub_path[\"timeseries\"] + 'min_loads/min_loads_dh.csv',\n",
    "    index_col=0,\n",
    "    parse_dates=[0])\n",
    "min_loads_dh = tools.reindex_time_series(min_loads_dh, year)\n",
    "min_loads_dh.to_csv(\n",
    "    main_path[\"outputs\"] + 'min_loads_dh' + \"_\" + str(year) + \".csv\")\n",
    "\n",
    "min_loads_ipp = pd.read_csv(\n",
    "    main_path[\"inputs\"] + sub_path[\"timeseries\"] + 'min_loads/min_loads_ipp.csv',\n",
    "    index_col=0,\n",
    "    parse_dates=[0])\n",
    "min_loads_ipp = tools.reindex_time_series(min_loads_ipp, year)\n",
    "min_loads_ipp.to_csv(\n",
    "    main_path[\"outputs\"] + 'min_loads_ipp' + \"_\" + str(year) + \".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "when2heat = pd.read_csv(\n",
    "    main_path[\"inputs\"] + sub_path[\"timeseries\"] + 'when2heat.csv', encoding='cp1252', sep=';',\n",
    "    usecols=['utc_timestamp', 'DE_heat_demand_total'], parse_dates=['utc_timestamp'],\n",
    "    index_col='utc_timestamp'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_entsoe_chp(name):\n",
    "    load = pd.read_csv('../raw_data_input/timeseries/min_loads/'+name+'.csv', index_col=0, parse_dates=[0])\n",
    "    df = pd.DataFrame(\n",
    "        index=pd.date_range(start='2017-01-01 00:00:00', end='2017-12-31 23:00:00', freq='H', tz='UTC')\n",
    "    )\n",
    "    df = df.join(load)\n",
    "    df = df.interpolate('linear')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "district_heat_demand = when2heat['DE_heat_demand_total'].loc['2013-01-01T00:00:00Z':'2013-12-31T23:00:00Z']\n",
    "district_heat_demand = district_heat_demand/district_heat_demand.max()\n",
    "\n",
    "min_load_ts = district_heat_demand.to_frame().rename(columns={'DE_heat_demand_total': 'chp'})\n",
    "# following only applies for oil other fossils etc.\n",
    "min_load_ts.loc[:, 'chp'] = np.maximum(min_load_ts.loc[:, 'chp'] - 0.5, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It seemed liked min loads are too high, so a slight adjustment is made here\n",
    "min_load_ts['chp_natgas'] = (\n",
    "    load_entsoe_chp('BerlinMitte_BNA0073') \n",
    "    / conv_de.loc['BNA0073', 'capacity']\n",
    ").values - 0.1\n",
    "min_load_ts['chp_lignite'] = 0.5 * (\n",
    "    (load_entsoe_chp('Lippendorf_BNA0115') / all_conv_de.loc['BNA0115', 'capacity']).values \n",
    "    + (load_entsoe_chp('Lippendorf_BNA0116') / all_conv_de.loc['BNA0116', 'capacity']).values\n",
    ") - 0.1\n",
    "min_load_ts['chp_hardcoal'] = 0.5 * (\n",
    "    (load_entsoe_chp('ReuterWest_BNA0086') / all_conv_de.loc['BNA0086', 'capacity']).values \n",
    "    + (load_entsoe_chp('ReuterWest_BNA0087') / all_conv_de.loc['BNA0087', 'capacity']).values\n",
    ") - 0.1\n",
    "\n",
    "min_load_ts['ipp'] = np.where((min_load_ts.index.hour > 6) & (min_load_ts.index.hour < 22), 0.9, 0.7)\n",
    "min_load_ts[min_load_ts < 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FR = tools.load_entsoe_generation_data('FR')\n",
    "AT = tools.load_entsoe_generation_data('AT')\n",
    "\n",
    "min_load_ts['FR_natgas'] = np.round(\n",
    "    FR['Fossil Gas  - Actual Aggregated [MW]']/\n",
    "        FR['Fossil Gas  - Actual Aggregated [MW]'].max(), rounding_precision).values\n",
    "min_load_ts['AT_natgas'] = np.round(\n",
    "    AT['Fossil Gas  - Actual Aggregated [MW]']/\n",
    "        AT['Fossil Gas  - Actual Aggregated [MW]'].max(), rounding_precision).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_load_ts.index= pd.date_range(start='2017-01-01 00:00:00', end='2017-12-31 23:00:00', freq='H')\n",
    "\n",
    "min_load_ts = min_load_ts.round(rounding_precision)\n",
    "min_load_ts_reindexed = tools.reindex_time_series(min_load_ts, year)\n",
    "min_load_ts_reindexed.to_csv(\n",
    "    main_path[\"outputs\"] + output_file[\"transformers_minload_ts\"] + \"_\" + str(year) + \".csv\"\n",
    ")\n",
    "min_load_ts_reindexed.to_excel(writer, output_file[\"transformers_minload_ts\"] + \"_\" + str(year))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multi-year time series for investment modelling\n",
    "to_concat = []\n",
    "for iter_year in range(2020, 2051):\n",
    "    to_concat.append(tools.reindex_time_series(min_load_ts, iter_year))\n",
    "    \n",
    "overall_min_load_ts = pd.concat(to_concat)\n",
    "\n",
    "overall_min_load_ts.to_csv(\n",
    "    main_path[\"outputs\"] + output_file[\"transformers_minload_ts\"] + \"_hourly.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do some garbage collection\n",
    "try:\n",
    "    del (FR, district_heat_demand, when2heat)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign gradients, minimum and maximum loads\n",
    "\n",
    "Steps applied:\n",
    "- Drop columns not needed anymore from data set and do some column renaming\n",
    "- Add estimate for gradients from assumption table and convert values (from %/min to MW/hour)\n",
    "- For the dispatch model, scale maximum load values for Germany based on historic generation data from ENTSO-E\n",
    "- If the year of interest is a future year use 2021 maximum values, except for natural gas where an increase in its market based availbility (to 92% in 2030) is assumed\n",
    "- Adjust gradients of plants based on historical generation data from ENTSO-E\n",
    "- Assume the flexibility of gas, hardcoal and lignite power plants to increase over time:\n",
    "    - natural gas / hydrogen: from 26% of installed capacity in 2022 to 70% in 2030 an 100% in 2045\n",
    "    - hardcoal: from 22% of installed capacity in 2020 to 40% of installed capacity in 2030 (phase-out year)\n",
    "    - lignite: from 23% of installed capacity in 2030 to 35% of installed capacity in 2030 (phase-out year)\n",
    "- Assign minimum load values per technology for German plants\n",
    "- Introduce margins between minimum and maximum loads of foreign power plants (natgas for FR and AT).\n",
    "- For the investment model, use installed capacities as they actually are / were and only assign minimum load assumptions.\n",
    "\n",
    "Data source:\n",
    "\n",
    "ENTSO-E (2022): Transparency Platform, Actual Generation per Production Type, https://transparency.entsoe.eu/generation/r2/actualGenerationPerProductionType/show, acessed 04.02.2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_keep = ['country', 'fuel', 'tech_fuel', 'capacity', \n",
    "                'efficiency_el', 'type', 'min_load_LP']\n",
    "cols_to_keep.extend(assumptions.columns)\n",
    "\n",
    "conv_de.drop(columns=conv_de.columns[~conv_de.columns.isin(cols_to_keep)], \n",
    "             inplace=True)\n",
    "\n",
    "# Assign gradients for Europe separately\n",
    "conv_eu = conv_eu.merge(assumptions, left_on='tech_fuel', right_index=True, how='left')\n",
    "\n",
    "conv_eu['grad_pos'] = np.minimum(1, 60 * conv_eu['load_grad_relative'])\n",
    "conv_eu['grad_neg'] = np.minimum(1, 60 * conv_eu['load_grad_relative'])\n",
    "\n",
    "conv_eu.drop(columns=['load_grad_relative'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use historical data if available and else data for last year available (except for natgas)\n",
    "fuels_to_use = ['lignite', 'hardcoal', 'uranium', 'natgas']\n",
    "if year < 2022:\n",
    "    generation_DE = tools.load_entsoe_german_generation_data(\n",
    "        path=main_path[\"inputs\"] + sub_path[\"timeseries\"],\n",
    "        year=year\n",
    "    )\n",
    "else:    \n",
    "    generation_DE = tools.load_entsoe_german_generation_data(\n",
    "        path=main_path[\"inputs\"] + sub_path[\"timeseries\"],\n",
    "        year=2021\n",
    "    )\n",
    "generation_DE_2020 = tools.load_entsoe_german_generation_data(\n",
    "    path=main_path[\"inputs\"] + sub_path[\"timeseries\"],\n",
    "    year=2020\n",
    ")\n",
    "fuels_to_use.remove('uranium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save original aggregated capacites to estimate fixed costs later on\n",
    "aggregated_capacities_de = conv_de.groupby('fuel').agg({'capacity':sum})\n",
    "\n",
    "# Empirically observed maximum loads as shares of installed capatity\n",
    "max_load_dict = {\n",
    "    fuel: min(\n",
    "        1,\n",
    "        generation_DE[fuel].max() \n",
    "        / aggregated_capacities_de[\"capacity\"].loc[fuel]\n",
    "    ) for fuel in fuels_to_use\n",
    "}\n",
    "\n",
    "# For natural gas, assume an increase in (market-based) availability\n",
    "natgas_max_load = pd.Series(index=range(2022, 2031), dtype=\"float64\")\n",
    "natgas_max_load[2022] = 0.68  # maximum observed historic value for 2020\n",
    "natgas_max_load[2030] = 0.92  # increase assumed due to decommissionings of other plants\n",
    "natgas_max_load = natgas_max_load.interpolate()\n",
    "\n",
    "if year >= 2022:\n",
    "    max_load_dict['natgas'] = natgas_max_load.loc[year]\n",
    "\n",
    "for fuel in max_load_dict.keys():\n",
    "     conv_de.loc[(conv_de['country'] == 'DE') \n",
    "                 & (conv_de['fuel'] == fuel), 'capacity'] *= max_load_dict[fuel]\n",
    "\n",
    "min_load_dict = {\n",
    "    'lignite': 0.4, \n",
    "    'hardcoal': 0.2, \n",
    "    'uranium': 0.5, \n",
    "    'natgas': 0.15,\n",
    "    'hydrogen': 0.1,\n",
    "    'oil': 0,\n",
    "    'otherfossil': 0.2,\n",
    "    'biomass': 0.2\n",
    "}\n",
    "\n",
    "# Convert gradients from %/min to MW/hour and assign minimum loads (relative values)\n",
    "conv_de = tools.assign_gradients_and_min_loads(conv_de, min_load_dict)\n",
    "all_conv_de = tools.assign_gradients_and_min_loads(all_conv_de, min_load_dict)\n",
    "\n",
    "conv_de_new.drop(columns=conv_de_new.columns[~conv_de_new.columns.isin(cols_to_keep)], \n",
    "                 inplace=True)\n",
    "all_conv_de_new.drop(columns=conv_de_new.columns[~conv_de_new.columns.isin(cols_to_keep)], \n",
    "                 inplace=True)\n",
    "\n",
    "try:\n",
    "    conv_de_new = tools.assign_gradients_and_min_loads(conv_de_new, min_load_dict)\n",
    "    all_conv_de_new = tools.assign_gradients_and_min_loads(all_conv_de_new, min_load_dict) \n",
    "# For status quo analyses, there are no new-built plants to be considered\n",
    "except ValueError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce flexibility to real market-based observed values\n",
    "fuels_to_use.append(\"uranium\")\n",
    "\n",
    "for fuel in fuels_to_use:\n",
    "    conv_de.loc[conv_de[\"fuel\"] == fuel, [\"grad_pos\", \"grad_neg\"]] = (\n",
    "        generation_DE[fuel].diff().max() / generation_DE[fuel].max()\n",
    "    )\n",
    "    all_conv_de.loc[all_conv_de[\"fuel\"] == fuel, [\"grad_pos\", \"grad_neg\"]] = (\n",
    "        generation_DE_2020[fuel].diff().max() / generation_DE_2020[fuel].max()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add approximization for development of gradients\n",
    "Add approximation on how *market-induced* gradient limitations will evolve.\n",
    "\n",
    "Steps applied:\n",
    "* Read estimates from file and interpolate in between years.\n",
    "* Write interpolated gradients to file again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For natural gas and hardcoal, assume an increase in (market-based) flexibility\n",
    "max_gradients = pd.DataFrame(index=range(2020, 2051), columns=fuels_to_use, dtype=\"float64\")\n",
    "\n",
    "gradient_assumptions = pd.read_csv(\n",
    "    main_path[\"inputs\"] + sub_path[\"assumptions\"] + input_file[\"gradients\"],\n",
    "    sep=\";\", decimal=\".\", index_col=0, header=0\n",
    ")\n",
    "\n",
    "for col in gradient_assumptions.columns:\n",
    "    max_gradients[col] = gradient_assumptions[col]\n",
    "    \n",
    "max_gradients = max_gradients.interpolate()\n",
    "\n",
    "if year >= 2022:\n",
    "    conv_de.loc[conv_de[\"fuel\"] == \"natgas\", [\"grad_pos\", \"grad_neg\"]] = max_gradients.at[year, \"natgas\"]\n",
    "    conv_de.loc[conv_de[\"fuel\"] == \"hardcoal\", [\"grad_pos\", \"grad_neg\"]] = max_gradients.at[year, \"hardcoal\"]\n",
    "    \n",
    "max_gradients.to_csv(\n",
    "    main_path[\"outputs\"] + output_file[\"transformers_gradients\"] + \".csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine availabilities for conventionals\n",
    "\n",
    "Steps applied:\n",
    "- Read in and use availability time series assumption from AMIRIS Examples repository to resolve maximum loads in time\n",
    "- Scale to a maximum of 1 to allow for historically observed resp. assumed market-based maximum output\n",
    "- Assign max_load_factor < 1 to account for availability\n",
    "\n",
    "\n",
    "> _Note: Availabilities are used for all conventional transformers in_ `pommesdispatch`\n",
    "\n",
    "Data source:\n",
    "\n",
    "AMIRIS Germany Example: https://gitlab.com/dlr-ve/esy/amiris/examples/-/tree/main/Germany2019, accessed 01.03.2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "availability_ts = pd.read_csv(\n",
    "    main_path[\"inputs\"] + sub_path[\"pp_public\"] + input_file[\"availability\"],\n",
    "    sep=\";\", decimal=\",\", index_col=0, header=None\n",
    ")\n",
    "\n",
    "# Reindex and reformat\n",
    "availability_ts.index = availability_ts.index.str.replace(\"2019\", \"2017\")\n",
    "availability_ts.index = pd.DatetimeIndex(availability_ts.index.str.replace(\"_\", \" \"), freq=\"MS\")\n",
    "availability_ts.loc[pd.Timestamp(\"2018-01-01\")] = availability_ts.iloc[-1]\n",
    "availability_ts.index.name = \"time stamp\"\n",
    "availability_ts.columns = [\"values\"]\n",
    "availability_ts = availability_ts.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize, interpolate and store time series\n",
    "availability_ts = availability_ts.div(availability_ts.max()).round(rounding_precision)\n",
    "availability_ts = availability_ts.resample(\"H\").interpolate()[:-1]\n",
    "\n",
    "availability_ts_reindexed = tools.reindex_time_series(availability_ts, year)\n",
    "\n",
    "availability_ts_reindexed.to_csv(\n",
    "    main_path[\"outputs\"] + output_file[\"transformers_availability_ts\"] + \"_\" + str(year) + \".csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_concat = []\n",
    "for iter_year in range(2020, 2051):\n",
    "    to_concat.append(tools.reindex_time_series(availability_ts, iter_year))\n",
    "    \n",
    "overall_availability_ts = pd.concat(to_concat)\n",
    "\n",
    "overall_availability_ts.to_csv(\n",
    "    main_path[\"outputs\"] + output_file[\"transformers_availability_ts\"] + \"_hourly.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do some garbage collection\n",
    "try:\n",
    "    del (aggregated_capacities_de, assumptions, availability_ts, capacity_increase,\n",
    "         fuel, max_load_dict, generation_DE, natgas_max_load, natgas_max_gradient, fuels_to_use, values)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Prepare conventional power plant data for usage in oemof.solph\n",
    "\n",
    "After finalizing the conventional power plants data set, this section serves to prepare the data for the usage in the oemof.solph terminology used by _POMMES_. Therefore, European power plant data is aggregated and identifiers as well as electricity buses are created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate European data and create a common data basis for the status quo\n",
    "\n",
    "Steps applied:\n",
    "- Aggregate European power plant data by country and fuel\n",
    "- Derive maximum, minimum and gradient values from historical ENTSO-E data. Maximum gradients are obtained from the 99% percentile of differences in power output between consecutive hours\n",
    "- Create a common data basis for Germany and Europe by concatenating the detailed German and the aggregated European power plant data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wm = lambda x: np.average(x, weights=conv_eu.loc[x.index, 'capacity'])\n",
    "agg_dict = dict([(key, (wm)) for key in conv_eu.columns if key not in ['fuel', 'country', 'tech_fuel']])\n",
    "for key in ['capacity']:\n",
    "    agg_dict[key] = 'sum'\n",
    "agg_dict['type'] = 'first'\n",
    "agg_dict[\"tech_fuel\"] = \"first\"\n",
    "\n",
    "conv_eu_agg = conv_eu.groupby(['country', 'fuel'], as_index=False).aggregate(agg_dict)\n",
    "\n",
    "conv_eu_agg['min_load_factor'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get ENTSOE max, min and max gradient values for conventionals and adjust accordingly\n",
    "path = '../raw_data_input/hydro/inputs/'\n",
    "countries = ['AT', 'CH', 'FR', 'NO1', 'NO2', 'NO3', 'NO4', 'NO5', 'NL', 'CZ', 'DK1', 'DK2', 'PL']\n",
    "generation = {country: tools.load_entsoe_generation_data(country) for country in countries}\n",
    "\n",
    "fuels = {\n",
    "    'lignite': 'Fossil Brown coal/Lignite  - Actual Aggregated [MW]',\n",
    "    'natgas': 'Fossil Gas  - Actual Aggregated [MW]',\n",
    "    'hardcoal': 'Fossil Hard coal  - Actual Aggregated [MW]',\n",
    "    'oil': 'Fossil Oil  - Actual Aggregated [MW]',\n",
    "    'uranium': 'Nuclear  - Actual Aggregated [MW]'\n",
    "        }\n",
    "data = []\n",
    "for c in countries:\n",
    "    for fuel in fuels.keys():\n",
    "        try:\n",
    "            data.append(\n",
    "                [c,\n",
    "                 fuel,\n",
    "                 generation[c][fuels[fuel]].min(),\n",
    "                 generation[c][fuels[fuel]].max(),\n",
    "                 generation[c][fuels[fuel]].diff().quantile(0.99)])\n",
    "        except (KeyError, TypeError): \n",
    "            pass        \n",
    "        \n",
    "conv_data = pd.DataFrame(columns=['country', 'fuel', 'minimum', 'maximum', 'gradient_max'], data=data)\n",
    "conv_data['gradient_max'] /= conv_data['maximum']\n",
    "conv_data['minimum'] /= conv_data['maximum']\n",
    "# oil has zero maximum infeed\n",
    "conv_data.drop(index=conv_data[(conv_data['country'] == 'AT')\n",
    "                               & (conv_data['fuel'] == 'oil')].index, inplace=True)\n",
    "\n",
    "# overwrite data\n",
    "for i in conv_data.index:\n",
    "    c, f = conv_data.loc[i, ['country', 'fuel']]\n",
    "    conv_eu_agg.loc[(conv_eu_agg['country']==c) & (conv_eu_agg['fuel']==f),\n",
    "                ['capacity', 'min_load_factor', 'grad_pos']] = (\n",
    "        conv_data.loc[i, ['maximum', 'minimum', 'gradient_max']].values)\n",
    "conv_eu_agg['grad_neg'] = conv_eu_agg['grad_pos']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjust gradients to fit minimum loads\n",
    "* In order to reflect (market-based) power plant inflexibilities, gradients have been reduced to minimum observable ones of *power plant fleets*.\n",
    "* Since for deriving minimum load profiles, data for *individual* power plants has been used, it may be the case that a power plant is assigned a gradient that is below the maximum gradient required by its minimum load profile.\n",
    "* This would lead to an infeasible model configuration. Therefore, the gradient is adjusted here to at least equal the maximum gradient of the minimum load profile.\n",
    "* Gradients therefore are defined as time series and only the values below the maximum gradient of the minimum load profile are increased.\n",
    "* Gradient values are assigned for plants occuring in the min loads time series data sets for district heating resp. industrial power pants first.\n",
    "* Then, gradient values for the remaining district heating resp. industrial plants are assigned based on fuels / countries.\n",
    "* In the last step, gradient values are compared with minimum load time series and increased wherever they are below the minimum load time series, also including a small margin for numerical reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh_gradients_ts = pd.DataFrame(index=min_loads_dh.index, columns=min_loads_dh.columns)\n",
    "ipp_gradients_ts = pd.DataFrame(index=min_loads_ipp.index, columns=min_loads_ipp.columns)\n",
    "remaining_gradients_ts = pd.DataFrame(index=min_load_ts_reindexed.index, columns=min_load_ts_reindexed.columns)\n",
    "\n",
    "gradients_min_loads = {\n",
    "    \"chp\": {\n",
    "        \"min_loads\": min_loads_dh,\n",
    "        \"gradients\": dh_gradients_ts\n",
    "    },\n",
    "    \"ipp\": {\n",
    "        \"min_loads\": min_loads_ipp,\n",
    "        \"gradients\": ipp_gradients_ts\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for value in gradients_min_loads.values():\n",
    "    gradient_ts = value[\"gradients\"]\n",
    "    for col in gradient_ts.columns:\n",
    "        try:\n",
    "            gradient_ts[col] = conv_de.at[col, \"grad_pos\"]\n",
    "        except KeyError:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fuels_to_consider= {}\n",
    "for plant_type in gradients_min_loads.keys():\n",
    "    fuels_to_consider[plant_type] = conv_de.loc[conv_de[\"type\"] == plant_type, \"fuel\"].unique()\n",
    "\n",
    "for plant_type, fuels in fuels_to_consider.items():\n",
    "    for fuel in fuels:\n",
    "        grad = conv_de.loc[(conv_de[\"type\"] == plant_type) & (conv_de[\"fuel\"] == fuel), \"grad_pos\"].unique()\n",
    "        remaining_gradients_ts[f\"{plant_type}_{fuel}\"] = grad[0]\n",
    "        if len(grad) > 1:\n",
    "            print(\"WARNING! Gradients are not set uniformly by fuel. Consider either changing them or this implementation!\")\n",
    "\n",
    "remaining_gradients_ts.drop(columns=[\"chp\", \"ipp\"], inplace=True)\n",
    "\n",
    "for country in [\"FR\", \"AT\"]:\n",
    "    remaining_gradients_ts[f\"{country}_natgas\"] = conv_eu_agg.loc[\n",
    "        (conv_eu_agg[\"country\"] == country) & (conv_eu_agg[\"fuel\"] == \"natgas\"), \"grad_pos\"\n",
    "    ].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for value in gradients_min_loads.values():\n",
    "    gradient_ts = value[\"gradients\"]\n",
    "    minimum_loads = value[\"min_loads\"]\n",
    "\n",
    "    for col in gradient_ts.columns:\n",
    "        gradient_ts[col] = np.where(minimum_loads[col].diff().abs() >= gradient_ts[col], minimum_loads[col].diff().abs() * 1.01, gradient_ts[col])\n",
    "        gradient_ts[col] = np.where(gradient_ts[col] > 1, 1, gradient_ts[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in remaining_gradients_ts.columns:\n",
    "    if col.startswith(\"chp\"):\n",
    "        if col.endswith((\"natgas\", \"lignite\")):\n",
    "            remaining_gradients_ts[col] = np.where(\n",
    "                min_load_ts_reindexed[col].diff().abs() >= remaining_gradients_ts[col], \n",
    "                min_load_ts_reindexed[col].diff().abs() * 1.01, \n",
    "                remaining_gradients_ts[col]\n",
    "            )   \n",
    "        else:\n",
    "            remaining_gradients_ts[col] = np.where(\n",
    "                min_load_ts_reindexed[\"chp\"].diff().abs() >= remaining_gradients_ts[col], \n",
    "                min_load_ts_reindexed[\"chp\"].diff().abs() * 1.01, \n",
    "                remaining_gradients_ts[col]\n",
    "            )\n",
    "    elif col.startswith(\"ipp\"):\n",
    "        remaining_gradients_ts[col] = np.where(\n",
    "            min_load_ts_reindexed[\"ipp\"].diff().abs() >= remaining_gradients_ts[col], \n",
    "            min_load_ts_reindexed[\"ipp\"].diff().abs() * 1.01, \n",
    "            remaining_gradients_ts[col]\n",
    "        )\n",
    "    elif col.startswith((\"FR\", \"AT\")):\n",
    "        remaining_gradients_ts[col] = np.where(\n",
    "            min_load_ts_reindexed[col].diff().abs() >= remaining_gradients_ts[col], \n",
    "            min_load_ts_reindexed[col].diff().abs() * 1.01, \n",
    "            remaining_gradients_ts[col]\n",
    "        )\n",
    "         \n",
    "    remaining_gradients_ts[col] = np.where(\n",
    "        remaining_gradients_ts[col] > 1, \n",
    "        1, \n",
    "        remaining_gradients_ts[col]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use values for natgas as a proxy for hydrogen\n",
    "remaining_gradients_ts[\"chp_hydrogen\"] = remaining_gradients_ts[\"chp_natgas\"]\n",
    "remaining_gradients_ts[\"ipp_hydrogen\"] = remaining_gradients_ts[\"ipp_natgas\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh_gradients_ts.to_csv(\n",
    "    main_path[\"outputs\"] + output_file[\"dh_gradients_ts\"] + \"_\" + str(year) + \".csv\"\n",
    ")\n",
    "ipp_gradients_ts.to_csv(\n",
    "    main_path[\"outputs\"] + output_file[\"ipp_gradients_ts\"] + \"_\" + str(year) + \".csv\"\n",
    ")\n",
    "remaining_gradients_ts.to_csv(\n",
    "    main_path[\"outputs\"] + output_file[\"remaining_gradients_ts\"] + \"_\" + str(year) + \".csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a European data projection for the future\n",
    "\n",
    "Steps applied:\n",
    "* Take the capacity information obtained from ENTSOE's TYNDP\n",
    "* Extract the capacity values for 2025 and 2030 and the aggregated status quo data\n",
    "* Interpolate in between and add the remainder of the data from the status quo data set\n",
    "* Add missing information and fill data gaps by mean values per fuel\n",
    "* Include some assumption on (slight) efficiency improvement due to decommissionings / new commissionings over time (efficiency is assumed to be 0.5% p.a. (not percent points) higher for all fuels)\n",
    "* Combine all data sets for the target year to be considered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_eu_2025_BEST['technology'] = conv_eu_2025_BEST['fuel'].replace(dict_tech_eu)\n",
    "conv_eu_2030_DG['technology'] = conv_eu_2030_DG['fuel'].replace(dict_tech_eu)\n",
    "conv_eu_2025_BEST['tech_fuel'] = conv_eu_2025_BEST['technology'] + '_' + conv_eu_2025_BEST['fuel']\n",
    "conv_eu_2030_DG['tech_fuel'] = conv_eu_2030_DG['technology'] + '_' + conv_eu_2030_DG['fuel']\n",
    "\n",
    "# Set index only for matching purposes\n",
    "conv_eu_2025_BEST.set_index(['country', 'fuel'], inplace=True)\n",
    "conv_eu_2030_DG.set_index(['country', 'fuel'], inplace=True)\n",
    "conv_eu_agg.set_index(['country', 'fuel'], inplace=True)\n",
    "\n",
    "# Do the Italian job (bidding zone renaming)\n",
    "italian_market_areas = {\n",
    "    \"ITcn\": \"IT\",\n",
    "    \"ITn\": \"IT\",\n",
    "    \"ITs\": \"IT\",\n",
    "    \"ITcs\": \"IT\",\n",
    "    \"ITsar\": \"IT\",\n",
    "    \"ITsic\": \"IT\",\n",
    "}\n",
    "\n",
    "conv_eu_2025_BEST.rename(index=italian_market_areas, inplace=True)\n",
    "conv_eu_2030_DG.rename(index=italian_market_areas, inplace=True)\n",
    "\n",
    "conv_eu_2025_BEST = conv_eu_2025_BEST.groupby(conv_eu_2025_BEST.index).sum()\n",
    "conv_eu_2030_DG = conv_eu_2030_DG.groupby(conv_eu_2030_DG.index).sum()\n",
    "\n",
    "conv_eu_2025_BEST.index = pd.MultiIndex.from_tuples(conv_eu_2025_BEST.index, names=[\"country\", \"fuel\"])\n",
    "conv_eu_2030_DG.index = pd.MultiIndex.from_tuples(conv_eu_2030_DG.index, names=[\"country\", \"fuel\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_eu_capacities = pd.DataFrame(index=conv_eu_agg.index.union(conv_eu_2025_BEST.index).union(conv_eu_2030_DG.index),\n",
    "                                  columns=list(range(2019, 2031)), dtype=\"float64\")\n",
    "\n",
    "conv_eu_capacities[2019] = conv_eu_agg[\"capacity\"]\n",
    "conv_eu_capacities[2025] = conv_eu_2025_BEST[\"capacity\"]\n",
    "conv_eu_capacities[2030] = conv_eu_2030_DG[\"capacity\"]\n",
    "conv_eu_capacities.loc[:, [2019, 2025, 2030]] = conv_eu_capacities.loc[:, [2019, 2025, 2030]].fillna(0)\n",
    "conv_eu_capacities = conv_eu_capacities.interpolate(axis=\"columns\").round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_tyndp_2018:\n",
    "\n",
    "    cols_to_concat = [col for col in conv_eu_agg.columns\n",
    "                      if col not in conv_eu_capacities.columns]\n",
    "\n",
    "    conv_eu = pd.merge(conv_eu_capacities, conv_eu_agg[cols_to_concat], \n",
    "                       left_index=True, right_index=True, how='left')\n",
    "\n",
    "    if year > 2019:\n",
    "        conv_eu[\"capacity\"] = conv_eu_capacities[year]\n",
    "    else:\n",
    "        conv_eu = conv_eu.loc[conv_eu[\"capacity\"].notna()]\n",
    "\n",
    "    conv_eu.drop(conv_eu_capacities.columns, axis=\"columns\", inplace=True)\n",
    "    conv_eu.reset_index(drop=False, inplace=True)\n",
    "\n",
    "    eu_fuels = conv_eu['fuel'].unique()\n",
    "\n",
    "    for fuel in eu_fuels:\n",
    "        conv_eu.loc[conv_eu['fuel'] == fuel, \n",
    "                    :] = conv_eu.loc[conv_eu['fuel'] == fuel, \n",
    "                                     :].fillna(conv_eu.loc[conv_eu['fuel'] == fuel, :].mean())\n",
    "\n",
    "    conv_eu['type'] = 'emb'\n",
    "    conv_eu.fillna(conv_eu.mean(), inplace=True)\n",
    "\n",
    "    if year > 2019:\n",
    "        conv_eu['efficiency_el'] = (1 + (year - 2019) * 0.005) * conv_eu['efficiency_el']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a European data projection for the future (UPDATE) \n",
    "\n",
    "Steps applied:\n",
    "* Take the capacity information obtained from ENTSOE's TYNDP\n",
    "* Add the remainder of the data from the status quo data set and interpolate\n",
    "* Add missing information and fill data gaps by mean values per fuel\n",
    "* Include some assumption on (slight) efficiency improvement due to decommissionings / new commissionings over time (efficiency is assumed to be 0.5% p.a. (not percent points) higher for all fuels)\n",
    "* Combine all data sets for the target year to be considered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not use_tyndp_2018:\n",
    "    \n",
    "    conv_eu_tyndp2022[\"2020-01-01\"] = conv_eu_agg[\"capacity\"]\n",
    "    conv_eu_tyndp2022[\"2020-01-01\"] = conv_eu_tyndp2022[\"2020-01-01\"].fillna(0)\n",
    "    conv_eu_tyndp2022 = conv_eu_tyndp2022.interpolate(axis=\"columns\").round(1)\n",
    "    conv_eu_tyndp2022.index.names = [\"country\", \"fuel\"]\n",
    "    \n",
    "    cols_to_concat = [col for col in conv_eu_agg.columns\n",
    "                      if col not in conv_eu_tyndp2022.columns]\n",
    "\n",
    "    conv_eu = pd.merge(conv_eu_tyndp2022, conv_eu_agg[cols_to_concat], \n",
    "                       left_index=True, right_index=True, how='left')\n",
    "    \n",
    "    # Fill gaps for tech_fuel\n",
    "    conv_eu.reset_index(drop=False, inplace=True)\n",
    "    conv_eu[\"technology\"] = np.nan\n",
    "    conv_eu.loc[conv_eu['tech_fuel'].isna(), 'technology'] = (\n",
    "        conv_eu.loc[conv_eu['technology'].isna(), 'fuel'].replace(dict_tech_eu)\n",
    "    )\n",
    "    conv_eu.loc[conv_eu['tech_fuel'].isna(), \"tech_fuel\"] = (\n",
    "        conv_eu[\"technology\"] + \"_\" + conv_eu[\"fuel\"]\n",
    "    )\n",
    "    conv_eu.drop(columns=\"technology\", inplace=True)\n",
    "    conv_eu.set_index([\"country\", \"fuel\"], inplace=True)\n",
    "    \n",
    "    # Separate investment data\n",
    "    conv_eu_investment = conv_eu.copy()\n",
    "    \n",
    "    if year > 2019:\n",
    "        conv_eu[\"capacity\"] = conv_eu_tyndp2022[f\"{year}-01-01\"]\n",
    "    else:\n",
    "        conv_eu = conv_eu.loc[conv_eu[\"capacity\"].notna()]\n",
    "\n",
    "    conv_eu.drop(conv_eu_tyndp2022.columns, axis=\"columns\", inplace=True)\n",
    "    conv_eu.reset_index(drop=False, inplace=True)\n",
    "\n",
    "    eu_fuels = conv_eu['fuel'].unique()\n",
    "\n",
    "    for fuel in eu_fuels:\n",
    "        conv_eu.loc[conv_eu['fuel'] == fuel, \n",
    "                    :] = conv_eu.loc[conv_eu['fuel'] == fuel, \n",
    "                                     :].fillna(conv_eu.loc[conv_eu['fuel'] == fuel, :].mean())\n",
    "\n",
    "    conv_eu['type'] = 'emb'\n",
    "    conv_eu.fillna(conv_eu.mean(), inplace=True)\n",
    "\n",
    "    if year > 2019:\n",
    "        conv_eu['efficiency_el'] = (1 + (year - 2019) * 0.005) * conv_eu['efficiency_el']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not use_tyndp_2018:\n",
    "    \n",
    "    conv_eu_investment.reset_index(drop=False, inplace=True)\n",
    "\n",
    "    for fuel in eu_fuels:\n",
    "        conv_eu_investment.loc[conv_eu_investment['fuel'] == fuel, \n",
    "                    :] = conv_eu_investment.loc[conv_eu_investment['fuel'] == fuel, \n",
    "                                     :].fillna(conv_eu_investment.loc[conv_eu_investment['fuel'] == fuel, :].mean())\n",
    "\n",
    "    conv_eu_investment['type'] = 'emb'\n",
    "    conv_eu_investment.fillna(conv_eu_investment.mean(), inplace=True)\n",
    "    \n",
    "    # Prepare for usage with oemof.solph\n",
    "    tools.nodes_to_oemof(conv_eu_investment, component=\"_transformer_\", component_suffix_var=\"fuel\")\n",
    "    conv_eu_investment.rename(columns={\"to\": \"to_el\"}, inplace=True)\n",
    "    conv_eu_investment[\"from\"] = conv_eu_investment[\"country\"] + \"_source_\" + conv_eu_investment[\"fuel\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = pd.concat([conv_de, conv_de_new, conv_eu])\n",
    "conv = conv.reset_index().rename(columns={'index': 'identifier'})\n",
    "conv.loc[conv['country'] != 'DE', 'identifier'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract all commodity sources of European power plants to be integrated in investment model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_conv_eu_sources = pd.DataFrame()\n",
    "if use_tyndp_2018:\n",
    "    conv_eu_2025_BEST.reset_index(drop=False, inplace=True)\n",
    "    conv_eu_2030_DG.reset_index(drop=False, inplace=True)\n",
    "    combined = pd.concat([conv_eu, conv_eu_2025_BEST, conv_eu_2030_DG])\n",
    "else:\n",
    "    combined = conv_eu_investment.copy()\n",
    "all_conv_eu_sources[\"country\"] = combined[\"country\"]\n",
    "all_conv_eu_sources[\"fuel\"] = combined[\"fuel\"]\n",
    "all_conv_eu_sources[\"label\"] = combined[\"country\"] + \"_source_\" + combined[\"fuel\"]\n",
    "all_conv_eu_sources[\"from\"] = combined[\"country\"] + \"_bus_\" + combined[\"fuel\"]\n",
    "all_conv_eu_sources = all_conv_eu_sources.drop_duplicates(keep=\"first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduce a power plant identifier and include connection to oemof.solph elements\n",
    "Steps applied:\n",
    "- Introduce an identifier consisting of country, the string 'transformer', fuel type and BNetzA-ID (the latter for Germany only)\n",
    "- Add a connection to the respective country fuel bus\n",
    "- Add a connection to the respective country electricity bus\n",
    "- Write the resulting data set to a csv and and Excel file\n",
    "\n",
    "> _Note: See the [documentation of oemof.solph](https://oemof.readthedocs.io/en/latest/index.html) for further information on buses which are basically some kind of (balanced) bus bars to connect the elements of an energy system._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv['label'] = np.where(conv['country'] == 'DE', \n",
    "                         conv['country'] + '_transformer_' + conv['fuel'] + '_' + conv['identifier'],\n",
    "                         conv['country'] + '_transformer_' + conv['fuel'])\n",
    "conv.set_index('label', inplace=True)\n",
    "\n",
    "# Connection to buses\n",
    "conv['from'] = conv['country'] + '_bus_' + conv['fuel']\n",
    "conv['to_el'] = conv['country'] + '_bus_' + 'el'\n",
    "\n",
    "all_conv_de['from'] = all_conv_de['country'] + '_bus_' + all_conv_de['fuel']\n",
    "all_conv_de['to_el'] = all_conv_de['country'] + '_bus_' + 'el'\n",
    "\n",
    "all_conv_de_new['from'] = all_conv_de_new['country'] + '_bus_' + all_conv_de_new['fuel']\n",
    "all_conv_de_new['to_el'] = all_conv_de_new['country'] + '_bus_' + 'el'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **_NOTE:_** The power plants data for the target year is written later (see: [this section](#Write-transformer-data-for-2030)) since it may include some adequacy considerations and assumptions about new-built plants to meet the security of supply requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduce electricity buses\n",
    "Introduce an electrical bus for every bidding zone modelled. Also see [this section](#Buses) where all buses are written to csv and Excel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_modeled = ['DE', 'AT', 'BE', 'CH', 'CZ', 'DK1', 'DK2', 'FR', 'NL', \n",
    "                     'NO1', 'NO2', 'NO3', 'NO4', 'NO5', 'PL', 'SE1', 'SE2', 'SE3', 'SE4', 'IT']\n",
    "\n",
    "buses = [country+'_bus_el' for country in countries_modeled]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do some garbage collection\n",
    "try:\n",
    "    del (agg_dict, c, cols_to_concat, cols_to_keep, conv_data, conv_eu,\n",
    "         conv_eu_2025_BEST, conv_eu_2030_DG, conv_eu_agg, conv_eu_capacities,\n",
    "         countries, data, eu_fuels, \n",
    "         f, filter_rule, fuel, fuels, fuels_to_evaluate, \n",
    "         generation, i, individual_chp, individual_ipp, key, \n",
    "         min_load_dict, min_load_ts, min_loads_dh, min_loads_ipp, path,\n",
    "         remaining_chp, remaining_ipp, too_low_gradients_idx, type_to_ts)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linking Transformers (Interconnectors)\n",
    "\n",
    "In this section, **interconnector** data for cross-border power exchange modelling is put together.<br>\n",
    "In [oemof.solph](https://github.com/oemof/oemof-solph) interconnectors for cross-border exchange can be represented by simple transformers elements with one input and one ouput as well as (transportation) losses.\n",
    "> Note: A former version of _POMMES_ used the custom link component instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in interconnector data\n",
    "\n",
    "A NTC based approach is used in _POMMES_ which considers time-dependent availability of NTCs by applying technical profiles for cross-border exchanges.\n",
    "\n",
    "The following data is read in:\n",
    "- Available resp. total transfer capacities for the interconnectors:\n",
    "    - Data is obtained from Timona Ghosh's master thesis based on the year 2016.\n",
    "    - Data for exchanges not considered by Timona Ghosh is added from the following sources:\n",
    "        - Scandinavian countries: [Nordpool](https://www.nordpoolgroup.com/Market-data1/Dayahead/Capacities1/Capacities/KEY/Norway/?view=table), accessed 2018-02-23\n",
    "        - Belgium: [Elia](https://www.elia.be/en/electricity-market-and-system/electricity-market-facilitation/capacity-calculation)\n",
    "        - Other countries: ENTSO-E (Forcasted Transfer Capacities - Day Ahead)\n",
    "    - Prognosis are given up to the year 2030. It is assumed that capacities remain constant after 2030.\n",
    "- Technical profiles:\n",
    "    - Technical profiles are derived from Timona Ghosh's master thesis. The original source are publications of the European TSOs and/or correspondence with TSOs.\n",
    "    - The technical profiles used are those prior to the introduction of FBMC.\n",
    "- Interconnector timeseries:\n",
    "    - The time series data contains the timeseries for the parameters used in the technical profiles.\n",
    "    - The time series are taken from Timona Gosh's master thesis and given for the year 2016. The original source is ENTSO-E resp. TSO transparency information.\n",
    "\n",
    "> _Note: Technical profiles determine the available NTC capacity which is dependent on wind power generation and/or load. The approach was used prior to flow-based market coupling (FBMC) and is still applied for the exchange with bidding zones which did not yet introduce FBMC. Since the domains for NTC and FBMC are quite similar, NTC still remains a good proxy for cross-border exchange._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interconn_TTC = pd.read_csv(\n",
    "    main_path[\"inputs\"] + sub_path[\"interconnectors\"] + input_file[\"interconnectors_ttc\"],\n",
    "    sep=\";\", decimal=\",\"\n",
    ")\n",
    "interconn_TTC.columns = list(interconn_TTC.columns)[:3] + list(range(2015, 2031))\n",
    "\n",
    "interconn_techprofiles = pd.read_csv(\n",
    "    main_path[\"inputs\"] + sub_path[\"interconnectors\"] + input_file[\"interconnectors_tech_profiles\"],\n",
    "    sep=\";\", decimal=\",\", index_col = 0\n",
    ")\n",
    "interconn_techprofiles.columns = list(interconn_techprofiles.columns)[:4] + list(range(2015, 2031))\n",
    "\n",
    "interconn_ts = pd.read_csv(\n",
    "    main_path[\"inputs\"] + sub_path[\"interconnectors\"] + input_file[\"interconnectors_timeseries\"],\n",
    "    parse_dates=True, index_col=0, sep=\";\", decimal=\",\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create data for linking transformer elements\n",
    "\n",
    "Provide data for transformer elements which model transshipment lines between the country electricity buses.\n",
    "\n",
    "Steps applied:\n",
    "-  Create labelling information:\n",
    "    - name of the linking transformer\n",
    "    - country bus, the line starts from (exporting country)\n",
    "    - country bus, the line leads to (importing country)\n",
    "- Apply a conversion factor of 0.95 in order to prevent excessive exchanges and model losses\n",
    "- Assign type of link:\n",
    "    - 'AC': applied for all connections for which technical profiles exist in order to depict time-dependent NTC values\n",
    "    - 'DC': applied for all other connections for which a fixed value is applied\n",
    "- Write the resulting data set into a csv and an Excel file\n",
    "\n",
    "> Example for naming convention used: DE &rarr; AT; name: DE_link_AT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "interconn_TTC['link'] = interconn_TTC['from'] + '_link_' + interconn_TTC['to'] \n",
    "interconn_TTC['from'] = interconn_TTC['from'] + '_bus_el'\n",
    "interconn_TTC['to'] = interconn_TTC['to'] + '_bus_el'\n",
    "\n",
    "interconn_TTC.drop(columns = 'action', inplace = True)\n",
    "interconn_TTC['conversion_factor'] = 1  # Do not include efficiencies to prevent roundshift \"burning\" energy\n",
    "interconn_TTC.set_index('link', inplace = True)\n",
    "interconn_TTC.loc[interconn_TTC.index.isin(interconn_techprofiles.index), 'type'] = 'AC'\n",
    "interconn_TTC.loc[interconn_TTC['type'] != 'AC', 'type'] = 'DC'\n",
    "\n",
    "# Ensure limitless capacity to display a single market zone\n",
    "interconn_TTC.loc['DE_link_AT', [2016, 2017, 2018]] = 100000\n",
    "interconn_TTC.loc['AT_link_DE', [2016, 2017, 2018]] = 100000\n",
    "\n",
    "# Add projection for the time frame 2030 to 2050\n",
    "for iter_year in range(2031, 2051):\n",
    "    interconn_TTC[iter_year] = interconn_TTC[2030]\n",
    "\n",
    "interconn_TTC.to_csv(\n",
    "    main_path[\"outputs\"] + output_file[\"linking_transformers\"] + \"_\" + str(year) + \".csv\"\n",
    ")\n",
    "interconn_TTC.to_excel(writer, sheet_name='interconn_TTC'  + \"_\" + str(year))\n",
    "\n",
    "# investment model\n",
    "interconn_TTC.to_csv(\n",
    "    main_path[\"outputs\"] + output_file[\"linking_transformers\"] + \".csv\"\n",
    ")\n",
    "interconn_TTC_transposed = interconn_TTC.T.loc[list(range(2020, 2051))]\n",
    "interconn_TTC_transposed[\"new_index\"] = interconn_TTC_transposed.index.astype(str) + \"-01-01\"\n",
    "interconn_TTC_transposed.set_index(\"new_index\", inplace=True, drop=True)\n",
    "interconn_TTC_transposed = interconn_TTC_transposed.div(interconn_TTC_transposed.loc[\"2050-01-01\"])\n",
    "\n",
    "interconn_TTC_transposed.to_csv(\n",
    "    main_path[\"outputs\"] + output_file[\"linking_transformers\"] + \"_annual_ts.csv\"\n",
    ")\n",
    "\n",
    "# Use year 2050 for indexation since some connections are not yet there in 2020\n",
    "normalized_interconn_TTC = interconn_TTC.copy()\n",
    "for iter_year in range(2015, 2051):\n",
    "    normalized_interconn_TTC.loc[:, iter_year] = (\n",
    "        normalized_interconn_TTC.loc[:, iter_year].div(normalized_interconn_TTC.loc[:, 2050])\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate time-dependent NTCs for AC interconnectors\n",
    "\n",
    "As stated above, the NTC values for AC interconnectors are determined based on technical profiles.\n",
    "\n",
    "Steps applied:\n",
    "- A DataFrame holding actual time-dependent NTC values is instanciated\n",
    "- A factor is introduced to account for interconnector expansion. Example:\n",
    "    - Max. NTC is 5,000 MW at the moment and it is planned to install another 1,000 MW\n",
    "    - factor 'network_expansion' has to be set to 6,000 / 5,000 = 1.2\n",
    "- Functions are introduced for determining the actual NTC value based on the technical profiles\n",
    "- The actual NTC time series values are set based on the technical profiles, i.e.\n",
    "    - based on German wind infeed for all German borders except for DE / DK1\n",
    "    - based on wind infeed and demand in TenneTs control area for DE / DK1\n",
    "- The NTC timeseries values are written to a csv and an Excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NTC_actual = pd.DataFrame(columns=interconn_techprofiles.index.unique(), index=interconn_ts.index)\n",
    "network_expansion = 1\n",
    "\n",
    "iter_year = interconn_ts.index.year\n",
    "for connector in NTC_actual.columns:\n",
    "    intercon = interconn_techprofiles.loc[connector].reset_index()\n",
    "    \n",
    "    if connector != 'DK1_link_DE' and connector != 'DE_link_DK1':\n",
    "        wind = interconn_ts['Wind_DE'].to_numpy()\n",
    "        \n",
    "        actual_capacity = (\n",
    "            np.array(\n",
    "                [tools.find_ntc_wind(intercon, wind, yr) \n",
    "                 for yr, wind in np.array([iter_year, wind]).T]) /\n",
    "            (interconn_TTC.at[connector,2016] * network_expansion))\n",
    "            \n",
    "    else:\n",
    "        wind = interconn_ts['Wind_TenneT'].to_numpy()\n",
    "        dem = interconn_ts['Demand_TenneT'].to_numpy()\n",
    "        \n",
    "        actual_capacity = (\n",
    "            np.array(\n",
    "                [tools.find_ntc_load_and_wind(intercon, wind, dem, yr) \n",
    "                 for yr, wind, dem in np.array([iter_year, wind, dem]).T]) / \n",
    "            (interconn_TTC.at[connector,2016] * network_expansion))                 \n",
    "\n",
    "    NTC_actual[connector] = actual_capacity\n",
    "    \n",
    "NTC_actual_2017 = NTC_actual.copy()\n",
    "NTC_actual = tools.reindex_time_series(NTC_actual, year)\n",
    "\n",
    "NTC_actual.to_csv(\n",
    "    main_path[\"outputs\"] + output_file[\"linking_transformers_ts\"] + \"_\" + str(year) + \".csv\"\n",
    ")\n",
    "NTC_actual.tz_localize(None).to_excel(writer, sheet_name='NTC_actual' + \"_\" + str(year))\n",
    "\n",
    "to_concat = []\n",
    "for iter_year in range(2020, 2051):\n",
    "    NTC_actual_tz = (\n",
    "        (\n",
    "            NTC_actual_2017 \n",
    "            * normalized_interconn_TTC.loc[[idx for idx in NTC_actual_2017.columns], iter_year]\n",
    "        ).loc[\"2017\"]\n",
    "    )\n",
    "    to_concat.append(tools.reindex_time_series(NTC_actual_tz, iter_year))\n",
    "    \n",
    "NTC_actual = pd.concat(to_concat)\n",
    "NTC_actual.to_csv(\n",
    "    main_path[\"outputs\"] + output_file[\"linking_transformers_ts\"] + \".csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do some garbage collection\n",
    "try:\n",
    "    del (NTC_actual, actual_capacity, connector, dem, intercon, interconn_TTC,\n",
    "         interconn_techprofiles, interconn_ts, iter_year, network_expansion, wind)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storages\n",
    "\n",
    "In this section, **power storages** data is put together.<br>\n",
    "In [oemof.solph](https://github.com/oemof/oemof-solph) storages can be represented through (generic) so called \"GenericStorage\" elements which have specific parameters.\n",
    "\n",
    "The following types of storages are covered in _POMMES_:\n",
    "- hydro reservoir storage\n",
    "- pumped hydro storage\n",
    "\n",
    "## Reservoir storages\n",
    "\n",
    "For modelling reservoir energy storages, generation data as well as data for the current (weekly) filling level is used.\n",
    "\n",
    "Steps applied for preparation:\n",
    "- Determine path where raw data can be found\n",
    "- Obtain all files stored in that folder (to read in based on subsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = main_path[\"inputs\"] + sub_path[\"hydro\"] +'inputs/'\n",
    "files = [f for f in listdir(path) if isfile(join(path, f))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain electricity generation from reservoir storages\n",
    "Steps applied:\n",
    "- Read in and prepare generation information from ENTSO-E per bidding zone<br>\n",
    "(see function `hydro.load_hydro_generation_data` for details)\n",
    "- Join data from all hydro generation data files and create subsets for reservoir resp. run of river (the latter will be treated later on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_generation = [f for f in files if 'entsoe_generation' in f]\n",
    "files_generation = [f for f in files_generation if search(('AT|CH|FR|IT|NO'), f)]\n",
    "\n",
    "generation_df = pd.DataFrame(index=pd.date_range(start='2017-01-01 00:00:00',\n",
    "                                                 end='2017-12-31 23:00:00', \n",
    "                                                 freq='H'))\n",
    "\n",
    "for file in files_generation:\n",
    "    bidding_zone = file.partition('generation_')[2][0:2]\n",
    "    \n",
    "    if bidding_zone == 'NO':\n",
    "        bidding_zone = file.partition('generation_')[2][0:3]    \n",
    "              \n",
    "    generation_df = generation_df.join(\n",
    "        hydro.load_hydro_generation_data(bidding_zone=bidding_zone, path=path, filename=file), \n",
    "        how='left')\n",
    "    \n",
    "generation_reservoir = generation_df[generation_df.columns[generation_df.columns.str.contains('Reservoir')]]\n",
    "generation_ror = generation_df[generation_df.columns[generation_df.columns.str.contains('ROR')]]\n",
    "\n",
    "reservoir_load_factors = pd.DataFrame(data={'max_load_factor': generation_reservoir.max(),\n",
    "                                            'min_load_factor': generation_reservoir.min()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derive an hourly filling rate (time series)\n",
    "\n",
    "Steps applied:\n",
    "- Read in and prepare filling rate information from ENTSO-E per bidding zone<br>\n",
    "(see function `hydro.load_hydro_generation_data` for details)\n",
    "- Join data from all hydro reservoir data files\n",
    "- Set first value for filling rate of 2018 as last value for 2017\n",
    "- Transform weekly values for the filling rate to hourly ones\n",
    "- Determine minimum and maximum filling rates per country / bidding zone\n",
    "- Determine inflow of reservoir as:\n",
    "$$inflow = \\Delta filling \\space rate + losses + \\frac{electricity \\space generation}{efficiency}$$\n",
    "- Determine weekly average of inflow and assign this value for each hour of the week\n",
    "- Normalize the values by dividing through usable storage energy (which is defined as the difference between max. and min. filling rate)\n",
    "- Currently, there is no generation data for Sweden, therefore, average data from Norway's bidding zones is taken for the inflow for all four Swedish bidding zones\n",
    "- Rename columns of DataFrame eventually containing the weekly average inflow on an hourly basis\n",
    "\n",
    "> *Findings:*\n",
    "> - *Austria has a small storage capacity, but an extremely high production of hydro. However, the total amount should fit.*\n",
    "> - *Results are quite sensible to the evaporation rate, and turbine efficiency.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hydro_storage_loss_rate = 0.0000\n",
    "efficiency_turbine = 0.85\n",
    "\n",
    "files_reservoir = [f for f in files if 'entsoe_hydro_reservoir' in f]\n",
    "\n",
    "reservoir_df = pd.DataFrame(index=np.arange(0,52,1))\n",
    "\n",
    "for file in files_reservoir:\n",
    "    bidding_zone = file.partition('reservoir_')[2][0:2]\n",
    "    \n",
    "    if bidding_zone in ['NO', 'SE']:\n",
    "        bidding_zone = file.partition('reservoir_')[2][0:3]\n",
    "    \n",
    "    reservoir_df = reservoir_df.join(\n",
    "        hydro.load_hydro_reservoir_data(bidding_zone=bidding_zone, \n",
    "                                        years=['2017', '2018'], \n",
    "                                        path=path, filename=file),\n",
    "        how='left')\n",
    "\n",
    "end_values = reservoir_df.loc[0, reservoir_df.columns[reservoir_df.columns.str.contains('2018')]].values\n",
    "reservoir_df.drop(columns=reservoir_df.columns[reservoir_df.columns.str.contains('2018')], inplace=True)\n",
    "reservoir_df.index = pd.date_range(start='01-01-2017 00:00:00', periods=52, freq='7D')\n",
    "reservoir_df.loc[pd.Timestamp('2018-01-01 00:00:00', freq='7D')] = end_values\n",
    "\n",
    "# Resample to hourly values and obtain max and min values\n",
    "reservoir_df = reservoir_df.resample('H').interpolate('linear')\n",
    "min_storageable_energy, max_storageable_energy = (reservoir_df.min(), reservoir_df.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_SE = reservoir_df.columns[reservoir_df.columns.str.contains('SE')]\n",
    "cols_nonSE = reservoir_df.columns[~reservoir_df.columns.isin(cols_SE)]\n",
    "\n",
    "inflow_df = (reservoir_df.loc[:, cols_nonSE].diff()[1:].values \n",
    "    + reservoir_df[:-1][cols_nonSE] * hydro_storage_loss_rate \n",
    "    + generation_reservoir.values / efficiency_turbine)\n",
    "\n",
    "# Do interpolation of inflow (weekly averages)\n",
    "inflow_df_averaged = inflow_df.apply(lambda x: hydro.upsample_inflow(x))\n",
    "\n",
    "usable_storage = max_storageable_energy - min_storageable_energy\n",
    "reservoir_init_level = (reservoir_df.iloc[0] - min_storageable_energy) / usable_storage\n",
    "\n",
    "rel_avg_hourly_inflow = inflow_df_averaged / usable_storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "_ = rel_avg_hourly_inflow[[\n",
    "    \"AT_hydro_storage_fillrate_2017\", \n",
    "    \"CH_hydro_storage_fillrate_2017\", \n",
    "    \"IT_hydro_storage_fillrate_2017\"\n",
    "]].plot(ax=ax)\n",
    "_ = plt.legend(loc=\"center left\", bbox_to_anchor=[1, 0.7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derive correction factors and estimate Sweden\n",
    "Steps applied:\n",
    "* Determine aggregated (theoretical) electricty output of estimated inflow\n",
    "* For all except for SE, calculate difference of theoretical and real output, and divide by total storage to get a correction for the (hourly) natural inflow\n",
    "* Subtract this difference from the inflow values\n",
    "* Use the mean values for Norway as an estimate for Sweden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate aggregated (theoretical) electricty output os estimated inflow\n",
    "calculated_energy = (rel_avg_hourly_inflow * usable_storage.values).sum() * efficiency_turbine\n",
    "calculated_energy.drop(index=calculated_energy.loc[calculated_energy.index.str.contains('SE')].index, inplace=True)\n",
    "\n",
    "correction_factor = ((calculated_energy - generation_reservoir.sum().values) /\n",
    "    usable_storage.loc[usable_storage.index.str.contains('AT|CH|FR|IT|NO')].values / 8760)\n",
    "\n",
    "rel_avg_hourly_inflow.loc[:, rel_avg_hourly_inflow.columns.str.contains('AT|CH|FR|IT|NO')] -= correction_factor.values\n",
    "\n",
    "cols_NO = reservoir_df.columns[reservoir_df.columns.str.contains('NO')]\n",
    "rel_avg_hourly_inflow[cols_SE] = (\n",
    "    np.repeat(rel_avg_hourly_inflow[cols_NO].mean(axis=1).values.reshape((8760,1)), [4], axis=1))\n",
    "\n",
    "rel_avg_hourly_inflow.columns = [_[0] + '_source_hydro' for _ in rel_avg_hourly_inflow.columns.str.split('_')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data for usage in oemof.solph\n",
    "Determine parameters for reservoirs such that they can be used to create the respective oemof.solph storages.\n",
    "\n",
    "Steps applied:\n",
    "- Put data for usable storage energy amount (nominal_storable_energy) and initial storage state into a DataFrame\n",
    "- Add loss rate and efficiency for turbine based on assumptions\n",
    "- Create the hydro sources and buses per country / bidding zone needed for reservoir modelling\n",
    "- Add sink components for modelling hydro spillage\n",
    "- Group data per bidding zone\n",
    "- PL, BE, CZ hydro storages are excluded due to missing date for the filling rates\n",
    "- Add inflow and outflow bus information\n",
    "- Set storage type to 'reservoir'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hydro_countries = [_[0] for _ in usable_storage.index.str.split('_')]\n",
    "hydro_reservoir_data = pd.DataFrame(\n",
    "    index=[c+'_storage_el_reservoir' for c in hydro_countries],\n",
    "    columns=['country','nominal_storable_energy', 'initial_storage_level'],\n",
    "    data=np.array([hydro_countries, usable_storage.values, reservoir_init_level.values]).T)\n",
    "hydro_reservoir_data['loss_rate'] = hydro_storage_loss_rate\n",
    "hydro_reservoir_data.loc['CH_storage_el_reservoir', 'loss_rate'] = 0.00001\n",
    "\n",
    "hydro_reservoir_data['efficiency_turbine'] = efficiency_turbine\n",
    "hydro_reservoir_data['efficiency_pump'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Hydro sources and buses\n",
    "sources_hydro = pd.DataFrame(columns=['country', 'capacity'],\n",
    "                             data=np.array([hydro_countries, usable_storage.values]).T)\n",
    "sources_hydro = tools.nodes_to_oemof(sources_hydro, to='_bus_hydro', component='_source_hydro')\n",
    "\n",
    "buses.extend(sources_hydro['to'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sinks_reservoir_spillage = pd.DataFrame(data={'country': hydro_countries,\n",
    "                                              'from': sources_hydro['to'].to_list()})\n",
    "\n",
    "sinks_reservoir_spillage['label'] = sinks_reservoir_spillage['country'] + '_sink_hydro_excess'\n",
    "sinks_reservoir_spillage['excess_costs'] = 1\n",
    "sinks_reservoir_spillage.set_index('label', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reservoir_eu_agg = reservoir_eu.groupby(by='bidding_zone', as_index=False).agg({'capacity': 'sum'})\n",
    "reservoir_eu_agg.drop(index=reservoir_eu_agg[reservoir_eu_agg['bidding_zone'].isin(['BE', 'PL', 'CZ'])].index, \n",
    "                      inplace=True)\n",
    "reservoir_eu_agg = reservoir_eu_agg.set_index(reservoir_eu_agg['bidding_zone'] + '_storage_el_reservoir')\n",
    "\n",
    "reservoir_eu_agg['bus_inflow'] = reservoir_eu_agg['bidding_zone'] + '_bus_hydro'\n",
    "reservoir_eu_agg['bus_outflow'] = reservoir_eu_agg['bidding_zone'] + '_bus_el'\n",
    "reservoir_eu_agg['capacity_pump'] = 100000\n",
    "reservoir_eu_agg.rename(columns={'capacity': 'capacity_turbine', 'bidding_zone': 'country'}, inplace=True)\n",
    "\n",
    "reservoir_eu_agg['type'] = 'reservoir'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pumped hydro storages\n",
    "\n",
    "The basic pumped hydro storages information is obtained from the power plants lists (see above).\n",
    "\n",
    "Steps applied:\n",
    "- Group the European PHES by bidding zones\n",
    "- Include TYNDP 2018 capacity projections and interpolate in between the years.\n",
    "- Assign values for 2025 to 2030 if TYNDP states no capacities. This is assumed to be a data error.\n",
    "- Combine into a common data set for the target year\n",
    "- Aggregate the German PHES units as well\n",
    "- Concatenate both data sets and set storage type to 'phes'\n",
    "- Add information on inflow and outflow bus as well as capacity information\n",
    "- Nominal storable energy is based on an assumption: It is assumed that the nominal storable energy equals 4times the plant capacity.\n",
    "\n",
    "> _Note: TYNDP 2018 values are chosen over latest TYNDP 2022 values. Reason: In TYNDP 2022, hydro capacities are aggregatedm whereas data basis for TYNDP distincts between hydro pump (assumed to equal PHES capacity), hydro turbine (can be larger, due to reservoirs / natural inflow) and hydro run of river._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pumpedstorage_eu_agg = pumpedstorage_eu.groupby('bidding_zone', as_index=False).agg({'capacity': 'sum'})\n",
    "pumpedstorage_eu_agg = pumpedstorage_eu_agg.set_index(pumpedstorage_eu_agg['bidding_zone'] + '_storage_el_PHS')\n",
    "pumpedstorage_eu_agg.rename(columns={'bidding_zone': 'country'}, inplace=True)\n",
    "\n",
    "# There is a large deviation between pumped capacity in AT which is fixed here\n",
    "pumpedstorage_eu_agg.loc['AT_storage_el_PHS', 'capacity'] =(\n",
    "    AT['Hydro Pumped Storage  - Actual Aggregated [MW]'].max())\n",
    "\n",
    "phes_de = phes_de.set_index(phes_de['country'] + '_storage_el_PHS_' + phes_de.index)\n",
    "phes_de.rename(columns={'capacity_net_bnetza': 'capacity'}, inplace=True)\n",
    "phes_de.drop(index=phes_de[phes_de['capacity'].isna()].index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine phes plants still operational in taget year\n",
    "* It is assumed that all plants operational in the status quo will be retroffited instead of shutdown.\n",
    "* New-built plant(s) is / are added for Germany.\n",
    "* The European data set is taken as it is given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pumpedstorage_eu_2025_BEST = pumpedstorage_eu_2025_BEST.pivot(index='country', columns='fuel', values='capacity')\n",
    "pumpedstorage_eu_2025_BEST = pumpedstorage_eu_2025_BEST.T.fillna(pumpedstorage_eu_2025_BEST.mean(axis=1)).T\n",
    "\n",
    "# Group Italian bidding zones\n",
    "pumpedstorage_eu_2025_BEST.rename(index=italian_market_areas, inplace=True)\n",
    "pumpedstorage_eu_2025_BEST = pumpedstorage_eu_2025_BEST.groupby(pumpedstorage_eu_2025_BEST.index).sum()\n",
    "\n",
    "pumpedstorage_eu_2025_BEST['label'] = pumpedstorage_eu_2025_BEST.index.values + '_storage_el_PHS'\n",
    "pumpedstorage_eu_2025_BEST.reset_index(inplace=True)\n",
    "pumpedstorage_eu_2025_BEST.set_index('label', inplace=True)\n",
    "\n",
    "pumpedstorage_eu_2025_BEST.rename(columns={'PHES_capacity_pump': 'capacity_pump',\n",
    "                                           'PHES_capacity_turbine': 'capacity_turbine'}, inplace=True)\n",
    "\n",
    "pumpedstorage_eu_2030_DG = pumpedstorage_eu_2030_DG.pivot(index='country', columns='fuel', values='capacity')\n",
    "pumpedstorage_eu_2030_DG = pumpedstorage_eu_2030_DG.T.fillna(pumpedstorage_eu_2030_DG.mean(axis=1)).T\n",
    "\n",
    "# Group Italian bidding zones\n",
    "pumpedstorage_eu_2030_DG.rename(index=italian_market_areas, inplace=True)\n",
    "pumpedstorage_eu_2030_DG = pumpedstorage_eu_2030_DG.groupby(pumpedstorage_eu_2030_DG.index).sum()\n",
    "\n",
    "pumpedstorage_eu_2030_DG['label'] = pumpedstorage_eu_2030_DG.index.values + '_storage_el_PHS'\n",
    "pumpedstorage_eu_2030_DG.reset_index(inplace=True)\n",
    "pumpedstorage_eu_2030_DG.set_index('label', inplace=True)\n",
    "\n",
    "pumpedstorage_eu_2030_DG.rename(columns={'PHES_capacity_pump': 'capacity_pump',\n",
    "                                         'PHES_capacity_turbine': 'capacity_turbine'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pump capacity since ENTSO-E data set seems to have a lot of reservoir included\n",
    "pumpedstorage_eu_2025_BEST[\"capacity\"] = pumpedstorage_eu_2025_BEST[\"capacity_pump\"]\n",
    "pumpedstorage_eu_2030_DG[\"capacity\"] = pumpedstorage_eu_2030_DG[\"capacity_pump\"]\n",
    "\n",
    "pumpedstorage_eu_capacities = pd.DataFrame(\n",
    "    index=pumpedstorage_eu_agg.index.union(pumpedstorage_eu_2025_BEST.index).union(pumpedstorage_eu_2030_DG.index),\n",
    "    columns=list(range(2019, 2031)), dtype=\"float64\")\n",
    "\n",
    "pumpedstorage_eu_capacities[2019] = pumpedstorage_eu_agg[\"capacity\"]\n",
    "pumpedstorage_eu_capacities[2019] = pumpedstorage_eu_capacities[2019].fillna(0)\n",
    "\n",
    "pumpedstorage_eu_capacities[2025] = np.where(\n",
    "    pumpedstorage_eu_2025_BEST[\"capacity\"] > pumpedstorage_eu_capacities[2019],\n",
    "    pumpedstorage_eu_2025_BEST[\"capacity\"],\n",
    "    pumpedstorage_eu_capacities[2019]\n",
    ")\n",
    "\n",
    "# Correct erraneous TYNDP data: if there is no capacity given for 2030, take 2025 best estimate values\n",
    "pumpedstorage_eu_capacities[2030] = np.where(\n",
    "    pumpedstorage_eu_2030_DG[\"capacity\"] > pumpedstorage_eu_capacities[2025], \n",
    "    pumpedstorage_eu_2030_DG[\"capacity\"], \n",
    "    pumpedstorage_eu_capacities[2025]\n",
    ")\n",
    "\n",
    "pumpedstorage_eu_capacities.loc[:, [2019, 2025, 2030]] = (\n",
    "    pumpedstorage_eu_capacities.loc[:, [2019, 2025, 2030]].fillna(0))\n",
    "pumpedstorage_eu_capacities = pumpedstorage_eu_capacities.interpolate(axis=\"columns\").round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_concat = [col for col in pumpedstorage_eu_agg.columns\n",
    "                  if col not in pumpedstorage_eu_capacities.columns]\n",
    "\n",
    "pumpedstorage_eu_agg = pd.merge(pumpedstorage_eu_capacities, pumpedstorage_eu_agg[cols_to_concat], \n",
    "                            left_index=True, right_index=True, how='left')\n",
    "\n",
    "pumpedstorage_eu_investment = pumpedstorage_eu_agg.copy()\n",
    "\n",
    "if year > 2019:\n",
    "    pumpedstorage_eu_agg[\"capacity\"] = pumpedstorage_eu_capacities[year]\n",
    "    pumpedstorage_eu_agg[\"country\"] = [pumpedstorage_eu_agg.index.str.split(\"_\")[i][0] \n",
    "                                       for i in range(len(pumpedstorage_eu_agg))]\n",
    "else:\n",
    "    pumpedstorage_eu_agg = pumpedstorage_eu_agg.loc[pumpedstorage_eu_agg[\"capacity\"].notna()]\n",
    "\n",
    "pumpedstorage_eu_agg.drop(pumpedstorage_eu_capacities.columns, axis=\"columns\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data for investment model\n",
    "* Evaluate differences in capacity for snapshot years\n",
    "* Introduce \"new-built\" clusters for the snapshot years considered\n",
    "* Derive max values for plants going online at later snapshot years &rarr; Force maximum to zero until unit is operational"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pumpedstorage_eu_investment[\"new_built_2025\"] = pumpedstorage_eu_investment[2025] - pumpedstorage_eu_investment[2019]\n",
    "pumpedstorage_eu_investment[\"new_built_2030\"] = pumpedstorage_eu_investment[2030] - pumpedstorage_eu_investment[2025]\n",
    "\n",
    "new_units = {\n",
    "    key: [\n",
    "        idx + f\"_{key}\" \n",
    "        for idx in pumpedstorage_eu_investment.loc[pumpedstorage_eu_investment[f\"new_built_{key}\"] > 0].index\n",
    "    ]\n",
    "    for key in [\"2025\", \"2030\"]\n",
    "}\n",
    "\n",
    "# Use capacity column to store the actual (pump) capacities\n",
    "pumpedstorage_eu_investment[\"capacity\"] = pumpedstorage_eu_investment[2019]\n",
    "\n",
    "for key, units in new_units.items():\n",
    "    for unit in units:\n",
    "        pumpedstorage_eu_investment.at[unit, \"capacity\"] = (\n",
    "            pumpedstorage_eu_investment.at[unit.rsplit(\"_\", 1)[0], f\"new_built_{key}\"]\n",
    "        )\n",
    "        pumpedstorage_eu_investment.at[unit, \"country\"] = (\n",
    "            pumpedstorage_eu_investment.at[unit.rsplit(\"_\", 1)[0], \"country\"]\n",
    "        )\n",
    "\n",
    "pumpedstorage_eu_investment = pumpedstorage_eu_investment.loc[\n",
    "    pumpedstorage_eu_investment[\"capacity\"] > 0, [\"country\", \"capacity\"]\n",
    "]\n",
    "# Fill missing country values\n",
    "pumpedstorage_eu_investment.loc[pumpedstorage_eu_investment[\"country\"].isna(), \"country\"] = (\n",
    "    pumpedstorage_eu_investment.loc[\n",
    "        pumpedstorage_eu_investment[\"country\"].isna()\n",
    "    ].index.str.split(\"_\", expand=True).get_level_values(0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pumpedstorages_eu_investment_max_ts = pd.DataFrame(\n",
    "    index=[f\"{iter_year}-01-01\" for iter_year in range(2020, 2051)],\n",
    "    columns=pumpedstorage_eu_investment.index,\n",
    "    data=1\n",
    ")\n",
    "pumpedstorages_eu_investment_max_ts[\"DE_storage_el_PHS\"] = 1\n",
    "\n",
    "for snapshot_year in [2025, 2030]:\n",
    "    pumpedstorages_eu_investment_max_ts.loc[\n",
    "        [f\"{iter_year}-01-01\" for iter_year in range(2020, snapshot_year + 1)], \n",
    "        pumpedstorages_eu_investment_max_ts.columns.str.contains(str(snapshot_year))\n",
    "    ] = 0\n",
    "\n",
    "pumpedstorages_eu_investment_max_ts.to_csv(\n",
    "    main_path[\"outputs\"] + output_file[\"storages_el_exogenous_max_ts\"] + \".csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add missing values\n",
    "* Perform aggregation for German pumped storages\n",
    "* Add assumption on nominal storage level for non-German plants\n",
    "* Correct German values by those obtained from ENTSO-E for a more precise modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phes_de.rename(columns={'name_bnetza': 'name',\n",
    "                        'eic_code_plant': 'eic_code'}, inplace=True)\n",
    "phes_de = phes_de.append(phes_de_new)\n",
    "\n",
    "# Aggregation for German pumped storages\n",
    "phes_de_agg = phes_de.groupby('country', as_index=False).agg({'capacity': 'sum'}).rename(\n",
    "    index={0: 'DE_storage_el_PHS'})\n",
    "\n",
    "phes = pd.concat([phes_de_agg, pumpedstorage_eu_agg])\n",
    "phes['type'] = 'phes'\n",
    "\n",
    "phes['bus_inflow'] = phes['country'] + '_bus_el'\n",
    "phes['bus_outflow'] = phes['country'] + '_bus_el'\n",
    "phes['capacity_pump'] = phes['capacity']\n",
    "phes['capacity_turbine'] = phes['capacity']\n",
    "phes['nominal_storable_energy'] = 4 * phes['capacity_turbine']\n",
    "\n",
    "# Overwrite using values obtained from ENTSO-E\n",
    "phes.loc[phes['country'] == 'DE', ['capacity_pump', 'capacity_turbine']] = [8680.75, 4475.25]\n",
    "phes.loc[phes['country'] == 'DE', 'nominal_storable_energy'] = 25116\n",
    "\n",
    "phes.drop(columns='capacity', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pumpedstorage_eu_investment['type'] = 'phes'\n",
    "pumpedstorage_eu_investment['bus_inflow'] = pumpedstorage_eu_investment['country'] + '_bus_el'\n",
    "pumpedstorage_eu_investment['bus_outflow'] = pumpedstorage_eu_investment['country'] + '_bus_el'\n",
    "pumpedstorage_eu_investment['capacity_pump'] = pumpedstorage_eu_investment['capacity']\n",
    "pumpedstorage_eu_investment['capacity_turbine'] = pumpedstorage_eu_investment['capacity']\n",
    "pumpedstorage_eu_investment['nominal_storable_energy'] = 4 * pumpedstorage_eu_investment['capacity_turbine']\n",
    "\n",
    "phes_investment = pd.concat([phes.loc[phes[\"country\"] == \"DE\"], pumpedstorage_eu_investment])\n",
    "\n",
    "phes_investment.drop(columns='capacity', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hydrogen storage\n",
    "An artificial generic hydrogen storage unit is added in order to decouple hydrogen supply and power generation from hydrogen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hydrogen_storage = {\n",
    "    \"country\": \"DE\",\n",
    "    \"type\": \"hydrogen\",\n",
    "    \"bus_inflow\": \"DE_bus_hydrogen\",\n",
    "    \"bus_outflow\": \"DE_bus_hydrogen\",\n",
    "    # practically, the hydrogen storage unit is unbounded\n",
    "    \"capacity_pump\": 1e6,\n",
    "    \"capacity_turbine\": 1e6,\n",
    "    \"nominal_storable_energy\": 1e10,\n",
    "    \"max\": 1,\n",
    "    \"loss_rate\": 0,\n",
    "    \"efficiency_pump\": 0.975,  # 95% roundtrip efficiency according to ISE (2020)\n",
    "    \"efficiency_turbine\": 0.975,\n",
    "    \"initial_storage_level\": 0.5,\n",
    "    \"min_storage_level\": 0,\n",
    "    \"max_storage_level\": 1,\n",
    "    \"max_invest_pump\": np.nan,\n",
    "    \"min_invest_pump\": 0,\n",
    "    \"max_invest_turbine\": np.nan,\n",
    "    \"min_invest_turbine\": 0,\n",
    "    \"max_invest\": np.nan,\n",
    "    \"min_invest\": 0,\n",
    "    \"unit_lifetime_pump\": 100,\n",
    "    \"unit_lifetime_turbine\": 100,\n",
    "    \"unit_lifetime\": 100,\n",
    "    \"invest_relation_input_output\": 1,\n",
    "    \"invest_relation_input_capacity\": np.nan,\n",
    "    \"invest_relation_output_capacity\": np.nan,\n",
    "    \"max_load_factor\": 1,\n",
    "    \"min_load_factor\": 0\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add assumptions\n",
    "\n",
    "Steps applied:\n",
    "- Put together the data for pumped hydro and reservoir storages\n",
    "- Add missing parameterization for pumped hydro energy storage based on assumptions (i.e. efficiencies, loss rate, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storages_el = pd.concat([phes, reservoir_eu_agg], axis=0, sort=False)\n",
    "\n",
    "assumptions_phes = pd.read_csv(\n",
    "    main_path[\"inputs\"] + sub_path[\"assumptions\"] + input_file[\"phes_assumptions\"],\n",
    "    index_col=0, sep=\"\\t\", decimal=\",\"\n",
    ")\n",
    "\n",
    "# Set missing parameters for phes based on assumptions\n",
    "for column in assumptions_phes.index:\n",
    "    storages_el[column] = assumptions_phes.at[column, 'value']\n",
    "\n",
    "# Set back values for reservoir storages which have been previously overwritten (again)\n",
    "storages_el.loc[hydro_reservoir_data.index, hydro_reservoir_data.columns] = hydro_reservoir_data.values\n",
    "\n",
    "# Restrict maximum and minimum load factors according to historical generation data\n",
    "reservoir_load_factors.index = [_[0]+'_storage_el_reservoir' for _ in reservoir_load_factors.index.str.split('_')]\n",
    "storages_el = storages_el.join(reservoir_load_factors, how='left')\n",
    "storages_el['max_load_factor'] /= storages_el['capacity_turbine']\n",
    "storages_el['min_load_factor'] /= storages_el['capacity_turbine']\n",
    "storages_el['max_load_factor'] = storages_el['max_load_factor'].fillna(1)\n",
    "storages_el['min_load_factor'] = storages_el['min_load_factor'].fillna(0)\n",
    "storages_el[['max_load_factor', 'min_load_factor']] = storages_el[['max_load_factor', 'min_load_factor']].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storages_el_investment = pd.concat([phes_investment, reservoir_eu_agg], axis=0, sort=False)\n",
    "\n",
    "assumptions_phes = pd.read_csv(\n",
    "    main_path[\"inputs\"] + sub_path[\"assumptions\"] + input_file[\"phes_assumptions\"],\n",
    "    index_col=0, sep=\"\\t\", decimal=\",\"\n",
    ")\n",
    "\n",
    "# Set missing parameters for phes based on assumptions\n",
    "for column in assumptions_phes.index:\n",
    "    storages_el_investment[column] = assumptions_phes.at[column, 'value']\n",
    "\n",
    "# Set back values for reservoir storages which have been previously overwritten (again)\n",
    "storages_el_investment.loc[hydro_reservoir_data.index, hydro_reservoir_data.columns] = hydro_reservoir_data.values\n",
    "\n",
    "# Restrict maximum and minimum load factors according to historical generation data\n",
    "reservoir_load_factors.index = [\n",
    "    _[0]+'_storage_el_reservoir' \n",
    "    for _ in reservoir_load_factors.index.str.split('_')\n",
    "]\n",
    "storages_el_investment = storages_el_investment.join(reservoir_load_factors, how='left')\n",
    "storages_el_investment['max_load_factor'] /= storages_el_investment['capacity_turbine']\n",
    "storages_el_investment['min_load_factor'] /= storages_el_investment['capacity_turbine']\n",
    "storages_el_investment['max_load_factor'] = storages_el_investment['max_load_factor'].fillna(1)\n",
    "storages_el_investment['min_load_factor'] = storages_el_investment['min_load_factor'].fillna(0)\n",
    "storages_el_investment[['max_load_factor', 'min_load_factor']] = (\n",
    "    storages_el_investment[['max_load_factor', 'min_load_factor']].round(2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save storages data to file\n",
    "Save the storages data in a csv and an Excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storages_el.to_csv(\n",
    "    main_path[\"outputs\"] + output_file[\"storages_el\"]  + \"_\" + str(year) + \".csv\"\n",
    ")\n",
    "storages_el.to_excel(writer, sheet_name='storages_el' + \"_\" + str(year))\n",
    "\n",
    "# Add generic hydrogen storage unit to data set (investment model only)\n",
    "storages_el.loc[\"DE_storage_hydrogen_generic\"] = hydrogen_storage\n",
    "\n",
    "storages_el_investment.to_csv(\n",
    "   main_path[\"outputs\"] + output_file[\"storages_el_exogenous\"] + \".csv\" \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storages investment options (investment model only)\n",
    "The following storage technologies can be invested in:\n",
    "* Pumped Hydro Energy Storage\n",
    "* Battery Energy Storage (lithium ion)\n",
    "\n",
    "Technological data for lithium ion batteries is taken from:\n",
    "* Sterner, Michael; Stadler, Ingo (2014): Energiespeicher. Bedarf, Technologien, Integration, Springer Vieweg, Berlin Heidelberg, ISBN 978-3-642-37380-0, p. 600\n",
    "* HTW (2022): Stromspeicher-Inspektion 2022.\n",
    "\n",
    "Steps applied:\n",
    "* Read in the raw data set\n",
    "* Write to prepared data folder again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storages_investment_options = pd.read_csv(\n",
    "    main_path[\"inputs\"] + sub_path[\"storages\"] + input_file[\"storages_el_investment_options\"], \n",
    "    index_col=0, sep=\";\", decimal=\",\"\n",
    ")\n",
    "\n",
    "storages_investment_options.to_csv(\n",
    "    main_path[\"outputs\"] + output_file[\"storages_el_investment_options\"] + \".csv\", \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do some garbage collection\n",
    "try:\n",
    "    del (AT, assumptions_phes, bidding_zone, calculated_energy, cols_NO, cols_SE, \n",
    "         cols_nonSE, cols_to_concat, column, correction_factor, efficiency_turbine, end_values,\n",
    "         file, files, files_generation, files_reservoir, generation_df,\n",
    "         generation_reservoir, hydro_countries, hydro_reservoir_data, hydro_storage_loss_rate,\n",
    "         inflow_df, inflow_df_averaged, max_storageable_energy, min_storageable_energy,\n",
    "         path, phes, phes_de_agg, phes_de_new, \n",
    "         pumpedstorage_eu, pumpedstorage_eu_2025_BEST, pumpedstorage_eu_2030_DG,\n",
    "         pumpedstorage_eu_agg, pumpedstorage_eu_capacities,\n",
    "         reservoir_df, reservoir_eu, reservoir_eu_agg, \n",
    "         reservoir_init_level, reservoir_load_factors, usable_storage)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Electric vehicles\n",
    "Unlike other components, electric vehicles are represented by a number of individual components, namely\n",
    "* transformers (for inflow and outflow management to buses),\n",
    "* a dedicated bus (serving as a connection between inflows/outflows, battery and withdrawal of driving energy),\n",
    "* a sink (for the withdrawal of driving energy) as well as\n",
    "* a generic storage (for depicting time-dependent fleet battery limitations).\n",
    "\n",
    "It has been decided, not to split these up among the different groups, but to keep them bundled here to facilitate development.\n",
    "\n",
    "The raw input data is compiled from\n",
    "* an assumption on the number of electric vehicles as well as the split between uncontrolled (inflexible) and controlled (flexible) charging.\n",
    "* VenCoPy profiles determining the electric vehicles energy consumption as well as flexibility limitations.\n",
    "\n",
    "Sources:\n",
    "* Share for controlled charging: Bailey, Joseph & Axsen, John (2015): Anticipating PEV buyers’ acceptance of utility controlled charging, in: Transportation Research Part A 82 (2015) 29–46.\n",
    "* Number of EVs historical: KBA (2023): Bestand an Kraftfahrzeugen und Kraftfahrzeuganhängern nach Zulassungsbezirken, 1. Januar 2023 (FZ 1), https://www.kba.de/DE/Statistik/Fahrzeuge/Bestand/Motorisierung/motorisierung_node.html, accessed 07.07.2023.\n",
    "* Number of EVs prospective: Lübbers, Sebastian; Wünsch, Marco; Lovis, Miriam; Wagner, Johannes; Sensfuß, Frank; Luderer, Gunnar & Bartels, Federike (2022): Vergleich der „Big 5“ Klimaneutralitätsszenarien, https://ariadneprojekt.de/media/2022/03/2022-03-16-Big5-Szenarienvergleich_final.pdf, accessed 07.07.2023, p. 32\n",
    "* Profiles: Wulff, Niklas & Miorelli, Fabia: VenCoPy. Vehicle Energy Consumption in Python (VencoPy), https://gitlab.com/dlr-ve/esy/vencopy/vencopy/-/tree/master, accessed 07.07.2023."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process assumptions\n",
    "Steps applied:\n",
    "* Read in table with assumptions on flexible share and number of electric vehicles for a certain year.\n",
    "* Interpolate to create data in yearly resolution.\n",
    "* Read in efficiency assumptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev_fleet_assumptions = pd.read_csv(\n",
    "    f\"{main_path['inputs']}{sub_path['electric_vehicles']}/{input_file['ev_fleet_assumptions']}\",\n",
    "    header=0, sep=\";\", index_col=0\n",
    ")\n",
    "ev_efficiency_assumptions = pd.read_csv(\n",
    "    f\"{main_path['inputs']}{sub_path['electric_vehicles']}/{input_file['ev_efficiency_assumptions']}\",\n",
    "    header=0, sep=\";\"\n",
    ")\n",
    "\n",
    "specific_annual_energy_consumption_per_ev_in_mwh = 2.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev_fleet_assumptions_annual = pd.DataFrame(index=range(2020, 2051), columns=ev_fleet_assumptions.columns)\n",
    "for idx in ev_fleet_assumptions.index:\n",
    "    ev_fleet_assumptions_annual.loc[idx] = ev_fleet_assumptions.loc[idx]\n",
    "\n",
    "ev_fleet_assumptions_annual[\"number of Evs in mio.\"] = (\n",
    "    ev_fleet_assumptions_annual[\"number of Evs in mio.\"].astype(\"float\").interpolate(how=\"linear\")\n",
    ")\n",
    "ev_fleet_assumptions_annual = ev_fleet_assumptions_annual.fillna(method=\"bfill\").fillna(method=\"ffill\")\n",
    "ev_fleet_assumptions_annual[\"energy consumption in MWh\"] = (\n",
    "    specific_annual_energy_consumption_per_ev_in_mwh * ev_fleet_assumptions_annual[\"number of Evs in mio.\"] * 1e6\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess profiles from VencoPy\n",
    "* Read in profiles:\n",
    "    * controlled charging availability (fleet connection rates)\n",
    "    * lower limit for state of charge\n",
    "    * upper limit for state of charge\n",
    "    * uncontrolled charging profile\n",
    "    * driving energy withdrawal\n",
    "* Normalize controlled charging availability (rescale to ensure maximum value = 1)\n",
    "* Normalize battery SoC limit profiles (rescale to ensure maximum of upper SoC limit = 1)\n",
    "* Normalize uncontrolled charging availability (rescale to ensure maximum value = 1)\n",
    "* Normalize driving profiles\n",
    "* Extract maxima (nominal values) for all profiles\n",
    "* Write the profiles to a joint csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in EV profiles from VencoPy\n",
    "ev_profile_names = [\"cc_avail\", \"soc_lower\", \"soc_upper\", \"uc_avail\", \"driving\"]\n",
    "\n",
    "ev_profiles = {\n",
    "    profile: pd.read_csv(\n",
    "        f\"{main_path['inputs']}{sub_path['electric_vehicles']}/{input_file[profile]}\"\n",
    "    )\n",
    "    for profile in ev_profile_names \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_share = ev_fleet_assumptions_annual[\"flexible share high\"].unique().item()\n",
    "ev_fleet_assumptions_annual[\"number of EVs with CC\"] = (\n",
    "    ev_fleet_assumptions_annual[\"number of Evs in mio.\"] * cc_share\n",
    ")\n",
    "ev_fleet_assumptions_annual[\"number of EVs with UC\"] = (\n",
    "    ev_fleet_assumptions_annual[\"number of Evs in mio.\"] * (1 - cc_share)\n",
    ")\n",
    "\n",
    "for profile in ev_profile_names:\n",
    "    ev_profiles[profile] = tools.preprocess_ev_profile(ev_profiles[profile])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_avail_max, cc_avail_profile_long = tools.prepare_ev_profile(\n",
    "    profile=ev_profiles[\"cc_avail\"], \n",
    "    assumptions=ev_fleet_assumptions_annual, \n",
    "    column=\"number of EVs with CC\"\n",
    ")\n",
    "\n",
    "soc_max, soc_max_profile_long = tools.prepare_ev_profile(\n",
    "    profile=ev_profiles[\"soc_upper\"], \n",
    "    assumptions=ev_fleet_assumptions_annual, \n",
    "    column=\"number of EVs with CC\"\n",
    ")\n",
    "\n",
    "soc_max, soc_min_profile_long = tools.prepare_ev_profile(\n",
    "    profile=ev_profiles[\"soc_lower\"], \n",
    "    assumptions=ev_fleet_assumptions_annual, \n",
    "    column=\"number of EVs with CC\",\n",
    "    other_profile_for_normalization=ev_profiles[\"soc_upper\"],\n",
    ")\n",
    "\n",
    "driving_max, driving_profile_long = tools.prepare_ev_profile(\n",
    "    profile=ev_profiles[\"driving\"],\n",
    "    assumptions=ev_fleet_assumptions_annual,\n",
    "    column=\"number of Evs in mio.\",\n",
    ")\n",
    "\n",
    "uc_avail_max, uc_avail_profile_long = tools.prepare_ev_profile(\n",
    "    profile=ev_profiles[\"uc_avail\"],\n",
    "    assumptions=ev_fleet_assumptions_annual,\n",
    "    column=\"number of EVs with UC\",\n",
    ")\n",
    "\n",
    "ev_long_profiles = pd.DataFrame(\n",
    "    data = {\n",
    "        \"cc_avail\": cc_avail_profile_long.values.reshape(cc_avail_profile_long.shape[0], ),\n",
    "        \"soc_upper\": soc_max_profile_long.values.reshape(soc_max_profile_long.shape[0], ),\n",
    "        \"soc_lower\": soc_min_profile_long.values.reshape(soc_min_profile_long.shape[0], ),\n",
    "        \"driving\": driving_profile_long.values.reshape(driving_profile_long.shape[0], ),\n",
    "        \"uc_avail\": uc_avail_profile_long.values.reshape(uc_avail_profile_long.shape[0], )\n",
    "    },\n",
    "    index=cc_avail_profile_long.index\n",
    ").round(4)\n",
    "\n",
    "# Correct driving profile by car efficiency\n",
    "ev_long_profiles[\"driving\"] = (\n",
    "    driving_profile_long.div(\n",
    "        ev_efficiency_assumptions.loc[\n",
    "            (ev_efficiency_assumptions[\"from\"] == \"bus_ev\")\n",
    "            & (ev_efficiency_assumptions[\"to\"] == \"transformer_ev_driving\"), \n",
    "            \"efficiency assumption\"\n",
    "        ].item()\n",
    "    )\n",
    ").round(4)\n",
    "\n",
    "ev_long_profiles.to_csv(\n",
    "    f\"{main_path['outputs']}{output_file['electric_vehicles_ts']}.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create elements to be used in oemof.solph\n",
    "Create components that are used for representing electric vehicle (flexibility) in oemof.solph. These comprise\n",
    "* a transformer for controlled charging,\n",
    "* a transformer for uncontrolled charging,\n",
    "* a bus collecting EV energy,\n",
    "* a storage depicting the aggregated fleet battery as well as\n",
    "* a sink to represent driving energy consumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components_ev = pd.DataFrame(columns=[\"type\", \"time_series\", \"from\", \"to\", \"efficiency_el\", \"nominal_value\"])\n",
    "\n",
    "components_ev.loc[\"transformer_ev_cc\"] = {\n",
    "    \"type\": \"transformer\",\n",
    "    \"time_series\": \"cc_avail\",\n",
    "    \"from\": \"DE_bus_el\",\n",
    "    \"to\": \"DE_bus_ev\",\n",
    "    \"efficiency_el\": ev_efficiency_assumptions.loc[\n",
    "            (ev_efficiency_assumptions[\"from\"] == \"transformer_ev_cc\")\n",
    "            & (ev_efficiency_assumptions[\"to\"] == \"bus_ev\"), \n",
    "            \"efficiency assumption\"\n",
    "        ].item(),\n",
    "    \"nominal_value\": cc_avail_max\n",
    "}\n",
    "\n",
    "components_ev.loc[\"transformer_ev_uc\"] = {\n",
    "    \"type\": \"transformer\",\n",
    "    \"time_series\": \"uc_avail\",\n",
    "    \"from\": \"DE_bus_el\",\n",
    "    \"to\": \"DE_bus_ev\",\n",
    "    \"efficiency_el\": ev_efficiency_assumptions.loc[\n",
    "            (ev_efficiency_assumptions[\"from\"] == \"transformer_ev_uc\")\n",
    "            & (ev_efficiency_assumptions[\"to\"] == \"bus_ev\"), \n",
    "            \"efficiency assumption\"\n",
    "        ].item(),\n",
    "    \"nominal_value\": uc_avail_max\n",
    "}\n",
    "\n",
    "components_ev.loc[\"bus_ev_uc\"] = np.nan\n",
    "components_ev.at[\"bus_ev_uc\", \"type\"] = \"bus\"\n",
    "\n",
    "components_ev.loc[\"storage_ev\"] = {\n",
    "    \"type\": \"storage\",\n",
    "    \"time_series\": [\"soc_lower\", \"soc_upper\"],\n",
    "    \"from\": \"DE_bus_ev\",\n",
    "    \"to\": \"DE_bus_ev\",\n",
    "    \"efficiency_el\": 1,\n",
    "    \"nominal_value\": ev_fleet_assumptions_annual[\"energy consumption in MWh\"].max().item()\n",
    "}\n",
    "\n",
    "components_ev.loc[\"sink_ev_driving\"] = {\n",
    "    \"type\": \"sink\",\n",
    "    \"time_series\": \"driving\",\n",
    "    \"from\": \"bus_ev\",\n",
    "    \"to\": np.nan,\n",
    "    \"efficiency_el\": np.nan,\n",
    "    \"nominal_value\": driving_max\n",
    "}\n",
    "\n",
    "components_ev.to_csv(\n",
    "    f\"{main_path['outputs']}{output_file['components_electric_vehicles']}.csv\"\n",
    ")\n",
    "\n",
    "ev_baseline_consumption = ev_long_profiles[\"driving\"].mul(driving_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iter_year in range(2020, 2051):\n",
    "    print(f\"{iter_year}: {(ev_baseline_consumption.loc[ev_baseline_consumption.index.year == iter_year].sum() / 1e6).item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cc_avail_max, soc_max, driving_max, uc_avail_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driving_profile_long.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 1, figsize=(15, 5 * 3))\n",
    "_ = cc_avail_profile_long.mul(cc_avail_max).plot(ax=axs[0], color=\"k\")\n",
    "_ = soc_max_profile_long.mul(soc_max).plot(ax=axs[0], color=\"lightgrey\")\n",
    "_ = soc_min_profile_long.mul(soc_max).plot(ax=axs[0], color=\"darkgrey\")\n",
    "\n",
    "_ = driving_profile_long.mul(driving_max).plot(ax=axs[1], color=\"blue\")\n",
    "\n",
    "_ = uc_avail_profile_long.mul(uc_avail_max).plot(ax=axs[2], color=\"red\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del (\n",
    "        cc_avail_profile_long, soc_max_profile_long, soc_min_profile_long, ev_long_profiles,\n",
    "        driving_profile_long, uc_avail_profile_long\n",
    "    )\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources\n",
    "\n",
    "In this section, **sources** data is put together.<br>\n",
    "In [oemof.solph](https://github.com/oemof/oemof-solph) (generic) \"sources\" components can be used to represent any kind of energy source. In _POMMES_ sources for the commodities (fuels), for energy shortage and for RES are used. Shortage sources are there to account for situations when overall generation is not sufficient to meet demand. In reality this would result in imports from other European countries and/or activation of reserves as well as forced load sheddings (\"brownouts\") to prevent a blackout. Shortage sources carry high penalty costs and are used for preserving model feasibility.\n",
    "\n",
    "For **commodity sources**, see [this section below](#Commodity-sources) which includes commodity sources according to the power plant status of the target year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emission limits\n",
    "Historical emissions as well as political target emission levels can be used to impose optional overall emissions limit.\n",
    "\n",
    "There are different target emission limits possible:\n",
    "* BAU prognosis: Simple linear regression based on historical emissions\n",
    "* 80% reduction (KSP, EK): 80% linear reduction taking into account the sector goal from the Klimaschutzplan 2050 (2016) as well as the 2050 goal from the Energiekonzept (2010)\n",
    "* 95% reduction (KSP, EK): 95% linear reduction taking into account the sector goal from the Klimaschutzplan 2050 (2016) as well as the 2050 goal from the Energiekonzept (2010)\n",
    "* 100% reduction (KSG): 100% linear reduction taking into account the sector goals from the Klimaschutzgesetz (KSG) which was agreed upon at the end of 2019 and updated in mid 2021.\n",
    "* 100% reduction (KNS 2035): 100% linear reduction aiming at the goal of a nearly carbon-neutral power sector by 2035.\n",
    "\n",
    "Goals:\n",
    "* KSP 2050: 61-62 % reduction in the energy industry using 466 Mt emissions in 1990 as a starting value which leads to 175-183 Mt GHG emissions in 2030. The mean value (179 Mt) is used. (Table 2 on page 33)\n",
    "* KSG: Anlage 2 zu § 4: 280 Mt in 2020, 257 Mt in 2022, 108 Mt in 2030; carbon neutrality (which practically means 0 emissions in the energy sector) in 2045 (§ 1 KSG). The goal of 88% emissions reduction until 2040 is not introduced since it is assumed that the contribution of the energy economy sector needs to be higher.\n",
    "\n",
    "Steps applied:\n",
    "* Perform a linear regression using statsmodels based on historical emissions (1990-2018) and years.\n",
    "* Apply historical values to all other possible emission pathways.\n",
    "* Add the target values as data points for the other pathways and interpolate linearly in between.\n",
    "\n",
    "Data sources:\n",
    "* Overall historical emissions: BMWi (2020). Energiedaten Gesamtausgabe. As of 23.10.2020, table 10, line 31 (Energiewirtschaft). https://www.bmwi.de/Redaktion/DE/Binaer/Energiedaten/energiedaten-gesamt-xls.xlsx?__blob=publicationFile&v=131, accessed 23.12.2020.\n",
    "* Bundesregierung (2010): Energiekonzept für eine umweltschonende, zuverlässige und bezahlbare Energieversorgung. 29. September 2010. https://www.bmwi.de/Redaktion/DE/Downloads/E/energiekonzept-2010.pdf?__blob=publicationFile&v=3, accessed 23.12.2020.\n",
    "* Bundesregierung (2016): Klimaschutzplan 2050. Klimaschutzpolitische Grundsätze und Ziele der Bundesregierung. https://www.bmu.de/fileadmin/Daten_BMU/Download_PDF/Klimaschutz/klimaschutzplan_2050_bf.pdf, accessed 23.12.2020.\n",
    "* KSG. Bundes-Klimaschutzgesetz vom 12. Dezember 2019 (BGBl. I S. 2513).\n",
    "* KNS 2035: Agora Energiewende, Prognos, Consentec (2022): Klimaneutrales Stromsystem 2035. Wie der deutsche Stromsektor bis zum Jahr 2035 klimaneutral werden kann.\n",
    "\n",
    "> _NOTE: Setting an emissions limit may conflict resp. interrelates with setting carbon prices. Thus, it is recommended to use carbon prices in the dispatch model (if available) and annual emissions limits in the investment model were minimum loads (of power plant fleets) are considered in a simplified manner._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GHG_emissions = pd.read_csv(\n",
    "    main_path[\"inputs\"] + sub_path[\"assumptions\"] + input_file[\"GHG_emissions\"],\n",
    "    index_col=0, sep=\";\", decimal=\",\" \n",
    ").T\n",
    "GHG_emissions.drop(index=\"Basiswert+\", inplace=True)\n",
    "GHG_emissions.index = GHG_emissions.index.str.strip(\"*\").astype(int)\n",
    "GHG_emissions.columns = GHG_emissions.columns.str.strip()\n",
    "\n",
    "# Do a linear regression for the BAU prognosis\n",
    "X = GHG_emissions.index.values\n",
    "Y = GHG_emissions.Energiewirtschaft.values\n",
    "\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "GHG_regression = sm.OLS(Y, X).fit()\n",
    "\n",
    "prediction_horizon = range(GHG_emissions.index.values[-1] + 1, 2051)\n",
    "X_pred = sm.add_constant(prediction_horizon)\n",
    "GHG_prediction = GHG_regression.predict(X_pred)\n",
    "GHG_pred = pd.DataFrame(index=prediction_horizon, data={\"Energiewirtschaft\": GHG_prediction})\n",
    "\n",
    "GHG_df = GHG_emissions.append(GHG_pred)\n",
    "GHG_df.columns = ['BAU']\n",
    "GHG_df['80_percent_linear'], GHG_df['95_percent_linear'], GHG_df['KNS_2035'], GHG_df['100_percent_linear'] = (\n",
    "    [GHG_df['BAU'].values] * 4\n",
    ")\n",
    "GHG_df.loc[2019:2050, ['80_percent_linear', '95_percent_linear', '100_percent_linear', 'KNS_2035']] = np.nan\n",
    "\n",
    "# Assign values for other paths\n",
    "start_val = GHG_df.at[1990, 'BAU']\n",
    "\n",
    "emission_targets_KSP_80 = pd.DataFrame({2030: np.mean([175, 183]), \n",
    "                                        2050: start_val * (1 - 0.8)},\n",
    "                                      index=[\"values\"]).T\n",
    "emission_targets_KSP_95 = pd.DataFrame({2030: np.mean([175, 183]), \n",
    "                                        2050: start_val * (1 - 0.95)},\n",
    "                                      index=[\"values\"]).T\n",
    "emission_targets_KSG_100 = pd.DataFrame({2020: 280,\n",
    "                                         2022: 257,\n",
    "                                         2030: 108,\n",
    "                                         2045: 0,\n",
    "                                         2050: 0},\n",
    "                                       index=[\"values\"]).T\n",
    "emission_targets_KNS_2035_100 = pd.DataFrame({2020: 280,\n",
    "                                         2021: 247,\n",
    "                                         2025: 142,\n",
    "                                         2030: 79,\n",
    "                                         2035: 22,\n",
    "                                         2040: 0,\n",
    "                                         2050: 0},\n",
    "                                       index=[\"values\"]).T\n",
    "\n",
    "GHG_df.loc[GHG_df['80_percent_linear'].isna(), '80_percent_linear'] = emission_targets_KSP_80['values']\n",
    "GHG_df.loc[GHG_df['95_percent_linear'].isna(), '95_percent_linear'] = emission_targets_KSP_95['values']\n",
    "GHG_df.loc[GHG_df['100_percent_linear'].isna(), '100_percent_linear'] = emission_targets_KSG_100['values']\n",
    "GHG_df.loc[GHG_df['KNS_2035'].isna(), 'KNS_2035'] = emission_targets_KNS_2035_100['values']\n",
    "GHG_df.interpolate(method='linear', inplace=True)\n",
    "# convert million tons of CO2 to tons of CO2\n",
    "GHG_df = GHG_df.mul(1e6)\n",
    "\n",
    "GHG_df.to_csv(\n",
    "    main_path[\"outputs\"] + output_file[\"emission_limits\"] + \".csv\"\n",
    ")\n",
    "GHG_df.to_excel(writer, sheet_name='emission_limits')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derive development factors for minimum loads used to scale down minimum load requirements for generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emissions_development_factors = GHG_df.copy().div(GHG_df.loc[2020])\n",
    "emissions_development_factors.index = [f\"{idx}-01-01\" for idx in emissions_development_factors.index]\n",
    "emissions_development_factors.to_csv(\n",
    "    main_path[\"outputs\"] + output_file[\"emission_development_factors\"] + \".csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do some garbage collection\n",
    "try:\n",
    "    del (GHG_df, GHG_emissions, GHG_pred, GHG_prediction, GHG_regression,\n",
    "         X, X_pred, Y, emission_targets_KSG_100, emission_targets_KSP_80, emission_targets_KSP_95,\n",
    "         prediction_horizon, start_val)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shortage sources\n",
    "Steps applied:\n",
    "- Introduce one electricity shortage source per country modelled and one hydro shortage source for each country with hydro generation\n",
    "- Concat the two data sets and assign high penalty costs\n",
    "- Split the data sets for investment resp. dispatch modeling\n",
    "- Add one shortage source for hydrogen for the investment model only\n",
    "- Write to csv and excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources_shortage_el = pd.DataFrame(data={'country': countries_modeled})\n",
    "sources_shortage_el = tools.nodes_to_oemof(sources_shortage_el, '_bus_el', '_source_el_shortage')\n",
    "\n",
    "sources_shortage_hydro = sources_hydro.copy()\n",
    "sources_shortage_hydro.index = [_+'_shortage' for _ in sources_hydro.index]\n",
    "sources_shortage_hydro.drop(columns='capacity', inplace=True)\n",
    "\n",
    "sources_shortage = pd.concat([sources_shortage_el, sources_shortage_hydro])\n",
    "sources_shortage['shortage_costs'] = 3000\n",
    "sources_shortage_dispatch = sources_shortage.copy(deep=True)\n",
    "sources_shortage_investment = sources_shortage.copy(deep=True)\n",
    "\n",
    "# dispatch model\n",
    "sources_shortage_dispatch.to_csv(\n",
    "    main_path[\"outputs\"] + output_file[\"sources_shortage\"] + \"_\" + str(year) + \".csv\"\n",
    ")\n",
    "sources_shortage_dispatch.to_excel(writer, sheet_name='sources_shortage' + \"_\" + str(year))\n",
    "\n",
    "# investment model\n",
    "sources_shortage_investment.loc[\"DE_source_hydrogen_shortage\"] = {\n",
    "    \"country\": \"DE\", \"to\": \"DE_bus_hydrogen\", \"shortage_costs\": shortage_costs_investment\n",
    "}\n",
    "sources_shortage_investment[\"shortage_costs\"] = shortage_costs_investment\n",
    "sources_shortage_investment.to_csv(\n",
    "    main_path[\"outputs\"] + output_file[\"sources_shortage\"] + \".csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Artificial shortage sources for market clearing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add artificial shortage capacity to meet the demand:\n",
    "* The sum of unmet demand is around 8 GW for Germany for simulation year 2017 (8,059 MW).\n",
    "* The unmet demand is due a rather rigorous limiting of maximum capacity outputs to the maximum observable ones in combination with limiting the scope to Germany and its neighours.\n",
    "* To adress this shortcoming, \n",
    "    * Artifical peaking units are introduced that are only used to capture price spikes. They are modelled as additional shortage sources.\n",
    "    * Their bidding price differs dependent on the historically observed resp. prognosed price spikes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_prices = {\n",
    "    2017: 163,\n",
    "    2018: 128,\n",
    "    2019: 121,\n",
    "    2020: 200,\n",
    "    2021: 300,\n",
    "    2022: 700\n",
    "}\n",
    "peak_prices=pd.Series(index=range(2017, 2031), data=peak_prices)\n",
    "peak_prices[2030] = 1000  # set 1,000 €/MWh as an implicit price limit\n",
    "peak_prices = peak_prices.interpolate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources_shortage_el_add = pd.DataFrame(columns=[\"country\", \"to\", \"nominal_value\"])\n",
    "\n",
    "for no in range(10):\n",
    "    sources_shortage_el_add.loc[f\"DE_source_el_shortage_add_{no}\"] = [\"DE\", \"DE_bus_el\", 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources_shortage_el_add.at[\"DE_source_el_shortage_add_0\", \"shortage_costs\"] = 120\n",
    "sources_shortage_el_add.at[\"DE_source_el_shortage_add_9\", \"shortage_costs\"] = peak_prices.loc[year]\n",
    "sources_shortage_el_add[\"shortage_costs\"] = sources_shortage_el_add[\"shortage_costs\"].interpolate().round(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources_shortage_el_add.to_csv(\n",
    "    main_path[\"outputs\"] + output_file[\"sources_shortage_el_add\"] + \"_\" + str(year) + \".csv\"\n",
    ")\n",
    "sources_shortage_el_add.to_excel(writer, sheet_name='sources_shortage_el_add' + \"_\" + str(year))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial shortage sources for investment modelling\n",
    "One problem in `pommesinvest` was that in foreign countries (i.e. countries other than Germany which is at the focus of the analyses) shortage events occured quite frequently. This is for multiple reasons:\n",
    "1. Capacities in countries other than Germany are exogenously determined by some external studies using a different modelling scope (Europe as a whole).\n",
    "2. Demand in turn was also taken from external studies, but profiles where simply scaled up which does neglect structural changes in demand patterns which are - at least in parts considered in the external studies.\n",
    "3. NTCs are quite limited and sufficient to cover this shortages. Also, there is no option to import from outside the model scope for these countries.\n",
    "While this is no problem in general and the model still will be able to find a cost-optimal solution, it becomes a problem when these shortage event begin to distort prices for Germany.\n",
    "\n",
    "To account for this model shortcoming, artificial storage units are introduced for `pommesinvest` in the following way:\n",
    "* First, a regular `pommesinvest` run is performed, not accounting for any \"artificial\" shortages.\n",
    "* After that, the dispatch results are analyzed and shortage patterns are extracted.\n",
    "* The shortage patterns are read here and combined with the \"artificial\" shortage unit cost assumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dr_scenario in [\"none\", \"5\", \"50\", \"95\"]:\n",
    "    # max capacities and costs\n",
    "    shortage_sources_artficial = pd.read_csv(\n",
    "        f\"{main_path['inputs']}{sub_path['assumptions']}\"\n",
    "        f\"{input_file['sources_el_artificial']}_{dr_scenario}.csv\",\n",
    "        index_col=0\n",
    "    )\n",
    "    shortage_sources_artficial[\"shortage_costs\"] = artificial_shortage_costs_investment\n",
    "    shortage_sources_artficial.to_csv(\n",
    "        f\"{main_path['outputs']}\"\n",
    "        f\"{output_file['sources_el_artificial']}_{dr_scenario}.csv\"\n",
    "    )\n",
    "    # normalized patterns\n",
    "    shortage_sources_artficial_ts = pd.read_csv(\n",
    "        f\"{main_path['inputs']}{sub_path['assumptions']}\"\n",
    "        f\"{input_file['sources_el_artificial']}_ts_{dr_scenario}.csv\",\n",
    "        index_col=0\n",
    "    )\n",
    "    shortage_sources_artficial_ts.round(rounding_precision).to_csv(\n",
    "        f\"{main_path['outputs']}\"\n",
    "        f\"{output_file['sources_el_artificial']}_ts_{dr_scenario}.csv\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do some garbage collection\n",
    "try:\n",
    "    del (no, peak_prices, sources_shortage_el_add, sources_shortage_artificial, sources_shortage_artificial_ts)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Renewable sources\n",
    "In this subsection, **renewable energy sources** (RES) data is put together.<br>\n",
    "For Germany, a detailled RES modelling approach is used. A distinction between fluctuating and non-fluctuating RES is made.\n",
    "\n",
    "> _NOTE: Germany and other European bidding zones are treated differently:_\n",
    "> * _For **Germany**, the sources data set contains the **installed capacities** while the timeseries data set contains maximum values smaller than 1 considering simultaneity of RES infeed._\n",
    "> * _For other **European** bidding zones, the sources data set contains the **maximum infeed capacity** while the timeseries data set contains maximum values equal to 1._\n",
    "\n",
    "> _NOTE: The POMMES investment model depicts RES in a much less granular fashion since its only aim is to determine optimal capacity portfolios (as well as overall power system costs). Therefore, the approach laid down in the following is not applied in the investment model._\n",
    "\n",
    "### Fluctuating RES\n",
    "Fluctuating RES depicted in _POMMES_ are wind onshore, wind offshore and PV. Run of river (ROR) is modelled seperately (see [extensions for Germany](#Extensions-for-Germany) for data preparation).\n",
    "\n",
    "#### Extract timeseries data\n",
    "Steps applied:\n",
    "- Read in timeseries data from OPSD for Europe\n",
    "- Slice values for the year 2017 and do some column renaming for obtaining the oemof.solph terminology\n",
    "- Rename columns for Skandinavian countries for proper bidding zone terminology (e.g. 'NO1' instead of 'NO_1')\n",
    "\n",
    "> _Note: Original file has been amended by data for Italy on 19.11.2022._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_eu = pd.read_csv(\n",
    "    main_path[\"inputs\"] + sub_path[\"timeseries\"] + input_file[\"opsd_timeseries\"],\n",
    "    index_col = 'utc_timestamp'\n",
    ")\n",
    "\n",
    "ts_eu = ts_eu['2017-01-01T00:00:00Z':'2017-12-31T23:00:00Z']\n",
    "ts_eu.index = pd.to_datetime(ts_eu.index)\n",
    "\n",
    "dict_rename_ts = {'_wind_onshore_generation_actual': '_source_windonshore',\n",
    "                  '_wind_offshore_generation_actual': '_source_windoffshore',\n",
    "                  '_solar_generation_actual': '_source_solarPV',\n",
    "                  '_load_actual_entsoe_transparency': '_sink_el_load'}\n",
    "\n",
    "for el in dict_rename_ts:\n",
    "    ts_eu.columns = ts_eu.columns.str.replace(el, dict_rename_ts[el])\n",
    "\n",
    "dict_names = dict(zip(ts_eu.filter(regex = 'DK|NO|SE').columns.values,\n",
    "                      ts_eu.filter(regex = 'DK|NO|SE').columns.str.replace(r\"[_]\", \"\", n = 1).values))\n",
    "ts_eu.rename(dict_names, axis = 1, inplace = True)\n",
    "\n",
    "ts_eu = ts_eu.tz_localize(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do manual data corrections\n",
    "Steps applied:\n",
    "- PV infeed in France is below zero for some (night) hours which is obviously a sign of a data error / inconsistency in data formatting &rarr; Hence, PV infeed is forced to 0 for the respective hours.\n",
    "- For some reason there is no windonshore feedin for the Netherlands. For a rough approximation, the TenneT profile (from DE) is used and scaled with the installed capacity obtained from IRENA (2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_eu.loc[ts_eu['FR_source_solarPV'] < 0, 'FR_source_solarPV'] = 0\n",
    "\n",
    "ts_eu['NL_source_windonshore'] = (\n",
    "    ts_eu['DE_tennet_wind_generation_actual'] / ts_eu['DE_tennet_wind_generation_actual'].max() * 3245)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare RES data for usage in oemof.solph\n",
    "Steps applied:\n",
    "- Filter the columns containing RES timeseries data\n",
    "- Concat the data with ROR generation (has been extracted in hydro (reservoir) data preparation above)\n",
    "- Extract maximum value for RES generation and set this as \"capacity\" for European RES\n",
    "> _Note: What is set as RES \"capacity\" here is actually the maximum feed in value, taking into account simultaneity of RES generation. Hence, a normalized generation time series with a maximum value of 1 can be derived from that. Actually installed RES generation capacity is higher, though. For Germany, a different approach is chosen and actually installed RES generation capacity is used for time series \"normalization\". Hence, for Germany, the maximum value of the \"normalized\" is smaller than 1._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [c+ts for c in countries_modeled for ts in dict_rename_ts.values()]\n",
    "ts_eu = ts_eu[ts_eu.columns[ts_eu.columns.isin(cols)]]\n",
    "\n",
    "ts_eu = pd.concat([ts_eu, generation_ror], axis=1, sort=False)\n",
    "# NO1 and NO5 have zero windonshore\n",
    "ts_eu.drop(columns=['NO1_source_windonshore', 'NO5_source_windonshore'], inplace=True)\n",
    "\n",
    "# Cut outliers in data set; assume outliers if 99.9 percentile is exceeded by at least 20%\n",
    "ts_eu = tools.cut_outliers(\n",
    "    ts_eu, cols=[\n",
    "        col for col in ts_eu.columns if ts_eu[col].values.dtype not in [\"str\", \"object\"]\n",
    "    ], \n",
    "    quantile=0.999, \n",
    "    multiplier=1.2\n",
    ")\n",
    "\n",
    "ts_res = ts_eu.filter(regex = 'wind|PV|ROR')\n",
    "\n",
    "sources_res = ts_res.max()\n",
    "sources_res = sources_res.to_frame().rename(columns={0: 'capacity'})\n",
    "\n",
    "sources_ts_res = ts_res / sources_res['capacity']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extensions for Germany\n",
    "Since Germany is in the focus of the analysis, some extensions are made for the modelling of renewables:\n",
    "- Non-fluctuating renewables are displayed in more detail\n",
    "- For flucutating renewables, the bidding strategy is approximated by simulating negative costs (see [this section](#EEG-transformers-with-negative-costs-(clustering)))\n",
    "\n",
    "**Approach for modelling marketing of RES**\n",
    "> In Germany, (fluctuating) RES generation is marketed within the so called \"market premium model\". Plant operators receive a market premium ($MP$) to compensate for the difference of their individual value applied (more or less equal to their full costs, i.e. LCOE) and the monthly average market revenue at the Day-ahead spot market. So their compensation is\n",
    "$$Earnings = Spot + MP$$\n",
    "Hence, it is economically rational to market the generation as long as profit is larger or equal to 0. For flucutating RES, it can be assumed that marginal costs are nearly 0 so that profits are nearly equal to earnings. This leads to:\n",
    "$$0 \\leq Spot + MP$$\n",
    "and\n",
    "$$-MP \\leq Spot$$\n",
    "The last inequation means that (rationally acting) plant operators are willing to market their generation as long as spot prices are at least as high as the negative market premium. This rational is modelled in _POMMES_.\n",
    "Since modelling each RES unit individually would blow up model complexity, power plant clusters for similar market premiums resp. values applicable within the same energy source are created.\n",
    "\n",
    "Steps applied:\n",
    "- Read in RES data and group by energy source. Primary data source is TSO data.\n",
    "- Do some renaming in order to be consistent with the remainder.\n",
    "\n",
    "Main data sources:\n",
    "* ÜNB (2018): EEG-Anlagenstammdaten zur Jahresabrechnung 2017, https://www.netztransparenz.de/EEG/Anlagenstammdaten.\n",
    "* ÜNB (2018): EEG-Bewegungsdaten zur Jahresabrechnung 2017, https://www.netztransparenz.de/EEG/Jahresabrechnungen.\n",
    "\n",
    "The detailled approach for RES modeling is described in the master's thesis of Yannick Werner (YW):\n",
    "\n",
    "Werner, Yannick (2020): Modellierung der Auswirkungen einer CO<sub>2</sub>-Bepreisung auf die EEG-Umlage, master's thesis at the chair of energy and resources economics at TU Berlin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usecols = ['eeg_id', 'capacity', 'energy_source', 'commissioning_date', 'decommissioning_date',\n",
    "           'support_scheme', 'value_applied']\n",
    "eeg_pps_de = pd.read_csv(\n",
    "    main_path[\"inputs\"] + sub_path[\"renewables\"] + input_file[\"eeg_powerplants\"],\n",
    "    usecols=usecols,\n",
    "    parse_dates=['commissioning_date', 'decommissioning_date']\n",
    ")\n",
    "\n",
    "eeg_pps_de_agg = eeg_pps_de.groupby('energy_source').agg({'capacity': 'sum'}) / 1000 # kW to MW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Powerplants, that are decommissioned before \"year\" are dropped\n",
    "eeg_pps_de.drop(index=eeg_pps_de[eeg_pps_de['decommissioning_date'].dt.year < year].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_eeg_pps_names = {'biomass': 'biomassEEG',\n",
    "                      'landfill_gas': 'landfillgas',\n",
    "                      'geothermal': 'geothermal',\n",
    "                      'mine_gas': 'minegas',\n",
    "                      'sevage_gas': 'larga',\n",
    "                      'solar': 'solarPV',\n",
    "                      'hydro': 'ROR',\n",
    "                      'wind_offshore': 'windoffshore',\n",
    "                      'wind_onshore': 'windonshore'}\n",
    "\n",
    "eeg_pps_de_agg.rename(index=dict_eeg_pps_names, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run-Of-River\n",
    "Steps applied and data sources used:\n",
    "- For Germany, ROR data is extracted from OPSD data\n",
    "- ROR under the EEG is obtained from the Anlagenstammdaten and added\n",
    "- The capacity of individual plants is summed up in order to create one representative source\n",
    "- It is assumed, that (EEG and non-EEG) run-of-river plants are renewed 1:1 instead of being commissioned.\n",
    "- New-built plant(s) is / are added to the data set.\n",
    "- Read in ROR generation timeseries data, drop NA values and resample from quarter-hourly to hourly frequency\n",
    "- Normalize the ROR generation profile by dividing through overall installed capacity\n",
    "- Data source for European ROR plants is (conventional) power plant matching data (see [this section](#Prepare-European-conventional-PP-Raw-Data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ror_de.rename(columns={'capacity_net_bnetza': 'capacity',\n",
    "                       'eic_code_plant': 'eic_code',\n",
    "                       'name_bnetza': 'name'}, inplace=True)\n",
    "\n",
    "ror_de = pd.concat([ror_de, ror_de_new])\n",
    "\n",
    "ror_de_not_eeg = ror_de.loc[(ror_de['status'] == 'operating') \n",
    "                            & (ror_de['eeg'] == 'no'), ['name', 'country', 'capacity']]\n",
    "\n",
    "\n",
    "ror_de_agg = ror_de_not_eeg.groupby(by='country').agg({'capacity': 'sum'})\n",
    "\n",
    "# Add eeg based capacity\n",
    "ror_de_agg.loc['DE', 'capacity'] += eeg_pps_de_agg.loc['ROR'].values\n",
    "ror_de_agg.index = ['DE_source_ROR']\n",
    "\n",
    "ror_ts_de = pd.read_csv(\n",
    "    main_path[\"inputs\"] + sub_path[\"timeseries\"] + input_file[\"entsoe_generation_de\"],\n",
    "    usecols=['Hydro Run-of-river and poundage  - Actual Aggregated [MW]']\n",
    ")\n",
    "ror_ts_de.columns = ['DE_source_ROR']\n",
    "ror_ts_de.drop(index=ror_ts_de[ror_ts_de['DE_source_ROR'].isna()].index, \n",
    "               inplace=True)\n",
    "ror_ts_de.index = pd.date_range(start='2017-01-01 00:00:00', \n",
    "                                end='2017-12-31 23:45:00', \n",
    "                                freq='15min')\n",
    "ror_ts_de = ror_ts_de.resample('H').mean()\n",
    "\n",
    "# Calculate ROR profile for Germany\n",
    "ror_ts_de = ror_ts_de / ror_de_agg.loc['DE_source_ROR', 'capacity']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other non-fluctuating RES (EEG)\n",
    "Steps applied and data sources used:\n",
    "- Read in power plants capacity and monthly capacity factors which are taken from the master thesis of YW\n",
    "- Determine the number of hours per month for 2017\n",
    "- \"Roll out\" the monthly capacity factors (from 2017) over all hours of a month and assign the hourly time series profile\n",
    "- Transfer the data to an oemof.solph compatible format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources_nonfluc_monthly_capacity_factors = pd.read_csv(\n",
    "    main_path[\"inputs\"] + sub_path[\"renewables\"] + input_file[\"renewables_nonfluctuating\"],\n",
    "    index_col=0,\n",
    "    parse_dates=True\n",
    ")\n",
    "\n",
    "sources_nonfluc_ts = sources_nonfluc_monthly_capacity_factors\n",
    "sources_nonfluc_ts.rename(columns={k: v for k, v in dict_eeg_pps_names.items() \n",
    "                                   if k in sources_nonfluc_ts.columns}, inplace=True)\n",
    "\n",
    "energy_sources = ['biomassEEG', 'landfillgas', 'geothermal', 'minegas', 'larga']\n",
    "sources_nonfluc = eeg_pps_de_agg.loc[energy_sources].reset_index().rename(\n",
    "    columns={'energy_source': 'technology'})\n",
    "\n",
    "sources_nonfluc['country'] = 'DE'\n",
    "\n",
    "sources_nonfluc = tools.nodes_to_oemof(\n",
    "    sources_nonfluc, to='_bus_el', component='_source_', component_suffix_var='technology')\n",
    "sources_nonfluc.drop(columns=['technology'], inplace=True)\n",
    "\n",
    "sources_nonfluc_ts.columns = sources_nonfluc.index\n",
    "sources_nonfluc_ts = sources_nonfluc_ts.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other adjustments for RES in Germany\n",
    "- Overwrite installed RES capacities for Germany with the _true_ ones to derive proper time series (overall capacity of market premium _and_ other (feed-in tariff) RES plants)\n",
    "- Overwrite German RES time series by dividing through the installed RES capacities (leads to a maximum value smaller than one for the \"normalized\" infeed time series, see [this section above](#Prepare-RES-data-for-usage-in-oemof.solph)); Result is assigned to RES buses, not sources (since it is provided by multiple units)\n",
    "- Do some column renaming and fill nan values through linear interpolation\n",
    "- Overwrite data for German RES power plants from the overall RES data set with the accurate data for Germany\n",
    "- Write the RES data for Germany to csv\n",
    "- Load market values from netztransparenz.de and do some data pre-preparations (see function `get_market_values` for details) in order to derive market value time series\n",
    "- Write the market values time series to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RES sources, buses and timeseries\n",
    "fluc_res_de = {'DE_source_solarPV': ('oh_solar', 'DE_bus_solarPV'), \n",
    "               'DE_source_windoffshore': ('oh_windoffshore', 'DE_bus_windoffshore'), \n",
    "               'DE_source_windonshore': ('oh_windonshore', 'DE_bus_windonshore')}\n",
    "\n",
    "for key, value in fluc_res_de.items():\n",
    "    ts_res.loc[:, key] = eeg.load_online_extrapolation(key, value[0])\n",
    "\n",
    "sources_res.loc[fluc_res_de, 'capacity'] = (\n",
    "    eeg_pps_de_agg.loc[[key.split(\"_\")[2] \n",
    "                        for key in fluc_res_de.keys()], 'capacity'].values)\n",
    "\n",
    "sources_ts_res[list(fluc_res_de.keys())] = (\n",
    "    ts_res[list(fluc_res_de.keys())] / sources_res.loc[list(fluc_res_de.keys()), 'capacity'])\n",
    "\n",
    "sources_ts_res.rename(columns={key: value[1] for key, value in fluc_res_de.items()}, \n",
    "                      inplace=True)\n",
    "\n",
    "# Interpolate NaNs\n",
    "print('Timeseries that contain NaNs: ', sources_ts_res.columns[sources_ts_res.isna().any()])\n",
    "sources_ts_res = sources_ts_res.interpolate(method='linear')\n",
    "\n",
    "sources_res.drop(index=list(fluc_res_de.keys()), inplace=True)\n",
    "\n",
    "sources_res_fluc_de = pd.DataFrame(index=list(fluc_res_de.keys()), \n",
    "                                   data=[value[1] for value in fluc_res_de.values()],      \n",
    "                                   columns=['to'])\n",
    "sources_res_fluc_de['country'] = 'DE'\n",
    "sources_res_fluc_de.index.name = 'label'\n",
    "sources_res_fluc_de.to_csv(\n",
    "    main_path[\"outputs\"] + output_file[\"sources_fluc_res\"] + \"_\" + str(year) + \".csv\"\n",
    ")\n",
    "\n",
    "# Market values\n",
    "market_values_ts = eeg.get_market_values(\n",
    "    netztransparenz=True,\n",
    "    filename=main_path[\"inputs\"] + sub_path[\"renewables\"] + input_file[\"market_values\"],\n",
    "    year=2017\n",
    ")\n",
    "\n",
    "market_values_ts = tools.reindex_time_series(market_values_ts, year)\n",
    "market_values_ts.to_csv(\n",
    "    f\"{main_path['outputs']}{output_file['costs_market_values']}_\"\n",
    "    f\"{eeg_clusters_per_technology}_clusters_{year}.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare RES data for usage in oemof.solph\n",
    "\n",
    "Steps applied:\n",
    "- Put together fluctuating RES data\n",
    "- Add country information and electricity bus as well as label\n",
    "- Combine data sets for fluctuating and non-fluctuating RES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources_res = pd.concat([sources_res, ror_de_agg])\n",
    "sources_res['country'] = [c[0] for c in sources_res.index.str.split('_')]\n",
    "sources_res['to'] = sources_res['country'] + '_bus_el'\n",
    "sources_res.index.name = 'label'\n",
    "\n",
    "sources_RES = pd.concat([sources_res, sources_nonfluc, sources_hydro], sort=False)\n",
    "sources_RES_original = sources_RES.copy()\n",
    "\n",
    "sources_ts_res = pd.concat([sources_ts_res, ror_ts_de, sources_nonfluc_ts, rel_avg_hourly_inflow], \n",
    "                           axis=1, \n",
    "                           sort=False)\n",
    "sources_ts_res = np.round(sources_ts_res, rounding_precision).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do some garbage collection\n",
    "try:\n",
    "    del (cols, dict_names, dict_rename_ts, el, energy_sources, fluc_res_de, generation_ror,\n",
    "         key, market_values_ts, rel_avg_hourly_inflow, ror_de, ror_de_new, ror_de_not_eeg,\n",
    "         ror_ts_de, sources_nonfluc_monthly_capacity_factors, sources_nonfluc_ts,\n",
    "         sources_res_fluc_de, sources_shortage, sources_shortage_el, sources_shortage_hydro)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renewable capacities and infeed for the target year\n",
    "\n",
    "Basic steps:\n",
    "* For German and European RES, capacity expansion factors per RES source are calculated and installed resp. maximum infeed capacities are scaled accordingly.\n",
    "* European capacities are determined based on IRENA data. For Germany, a study by Prognos et al. is used for capacity projection.\n",
    "* For Germany, an in-depth analysis of new installations is made in order to derive future values applied. Thereby, the results from the German tender procedures are evaluated which will determine prices for the next few years. In addition to that, cost projections are put together based on literature projections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determine European capacity development\n",
    "\n",
    "Steps applied:\n",
    "* Read in capacity data for the status quo from IRENA\n",
    "* Determine capacity development from TYNDP for Europe.\n",
    "* Keep capacity for ROR, hydro and other non-fluctuating resources constant; The data is not included in TYNDP projections, but it seems unlikely that there is a capacity decline.\n",
    "* For 2017 and 2018, use 2019 data.\n",
    "* Determine a scaling factor for the capacity values of RES.\n",
    "    * Use quotient between TYNDP data and capacities from IRENA where applicable.\n",
    "    * Assign values of 1 for reservoir and ROR plants. (no other information available &rarr; assumed to remain constant)\n",
    "    * Use quotient between TYNDP and ENTSOE data from above (peak infeed capacities) where no other information is available in IRENA data set.\n",
    "\n",
    "Data sources:\n",
    "- Data on capacity of fluctuating RES (except denmark), IRENA (2020): https://www.irena.org/Statistics/Download-Data, data downloaded 09.11.2020; all sources, data for 2017 used; extended by Italy on 24.11.2022.\n",
    "- Data on fluctuating RES for denmark: ENTSO-E (2019): Generation - Installed Capacity per Production Type, BZN DK1 / DK2, https://transparency.entsoe.eu/dashboard/show, accessed 20.11.2019. Data for recent years seems to underestimate capacities slightly compared to IRENA data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources_RES_cap = pd.read_csv(\n",
    "    main_path[\"inputs\"] + sub_path[\"renewables\"] + input_file[\"REcap_eu\"],\n",
    "    sep=\";\", decimal=\",\"\n",
    ")\n",
    "\n",
    "DK_cap = pd.read_csv(\n",
    "    main_path[\"inputs\"] + sub_path[\"renewables\"] + input_file[\"REcap_DK\"],\n",
    "    sep=\";\", decimal=\",\"\n",
    ")\n",
    "\n",
    "sources_RES_cap.loc[:,'Technology'] = sources_RES_cap.loc[:,'Technology'].fillna(method='ffill')\n",
    "\n",
    "sources_RES_cap.rename(columns = {'Country/area': 'country', 'Technology': 'technology'}, \n",
    "                       inplace = True)\n",
    "sources_RES_cap.drop(columns = 'Indicator', inplace = True)\n",
    "sources_RES_cap['country'].replace(\n",
    "    {'Europe': 'EU', 'Austria': 'AT', 'Belgium': 'BE', 'Czechia': 'CZ', 'Denmark': 'DK', 'Finland': 'FI',\n",
    "     'France': 'FR', 'Germany': 'DE', 'Hungary': 'HU', 'Luxembourg': 'LU', 'Netherlands': 'NL', 'Norway': 'NO', \n",
    "     'Poland': 'PL', 'Portugal': 'PO', 'Slovakia': 'SK', 'Spain': 'ES', 'Sweden': 'SE', 'Switzerland': 'CH', \n",
    "     'UK': 'GB', 'Italy': 'IT'},\n",
    "    inplace = True)\n",
    "\n",
    "sources_RES_cap['technology'].replace({'Onshore wind energy': 'windonshore', \n",
    "                                       'Offshore wind energy': 'windoffshore',\n",
    "                                       'Solar photovoltaic': 'solarPV'}, \n",
    "                                      inplace = True)\n",
    "sources_RES_cap = sources_RES_cap[(sources_RES_cap['technology'].isin(['windonshore', 'windoffshore', 'solarPV']))\n",
    "                                  & (sources_RES_cap['country'].isin(countries_modeled))]\n",
    "sources_RES_cap.columns = sources_RES_cap.columns.astype(str)\n",
    "\n",
    "DK_cap['technology'].replace({'Wind Onshore': 'windonshore', \n",
    "                              'Wind Offshore': 'windoffshore', \n",
    "                              'Solar': 'solarPV'}, \n",
    "                             inplace = True)\n",
    "DK_cap = DK_cap[DK_cap['technology'].isin(['windonshore', 'windoffshore', 'solarPV'])]\n",
    "DK_cap[['2016', '2017', '2018']] = DK_cap[['2016', '2017', '2018']].astype('int32')\n",
    "\n",
    "sources_RES_cap = pd.concat([sources_RES_cap, DK_cap], sort = True)\n",
    "\n",
    "sources_RES_cap.drop(columns=['2016','2017'], inplace=True)\n",
    "sources_RES_cap.drop(index=sources_RES_cap[sources_RES_cap['2018'].isna()].index, inplace=True)\n",
    "sources_RES_cap.rename(columns={'2018': 'capacity', \n",
    "                                'technology': 'fuel'}, inplace=True)\n",
    "\n",
    "sources_RES_cap['label'] = sources_RES_cap['country'] + '_source_' + sources_RES_cap['fuel']\n",
    "sources_RES_cap.set_index('label', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "renewables_eu_tyndp2022[\"country\"] = renewables_eu_tyndp2022.index.get_level_values(0) \n",
    "renewables_eu_tyndp2022[\"label\"] = (\n",
    "    renewables_eu_tyndp2022.index.get_level_values(0) \n",
    "    + \"_source_\" + renewables_eu_tyndp2022.index.get_level_values(1)\n",
    ")\n",
    "renewables_eu_tyndp2022.set_index(\"label\", inplace=True)\n",
    "renewables_eu_tyndp2022.loc[:,'to'] = renewables_eu_tyndp2022.loc[:,'country'] + '_bus_el'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_tyndp_2018:\n",
    "    # TYNDP 2018 data on RES\n",
    "    renewables_eu_2025_BEST['label'] = (renewables_eu_2025_BEST['country'].values\n",
    "                                        + '_source_' + renewables_eu_2025_BEST['fuel'].values)\n",
    "    renewables_eu_2025_BEST.set_index('label', inplace=True)\n",
    "    renewables_eu_2025_BEST.loc[:,'to'] = renewables_eu_2025_BEST.loc[:,'country'] + '_bus_el'\n",
    "    renewables_eu_2025_BEST.drop(columns='fuel', inplace=True)\n",
    "    renewables_eu_2025_BEST = renewables_eu_2025_BEST.sort_values(by='country')\n",
    "\n",
    "    renewables_eu_2030_DG['label'] = (renewables_eu_2030_DG['country'].values\n",
    "                                      + '_source_' + renewables_eu_2030_DG['fuel'].values)\n",
    "    renewables_eu_2030_DG.set_index('label', inplace=True)\n",
    "    renewables_eu_2030_DG.loc[:,'to'] = renewables_eu_2025_BEST.loc[:,'country'] + '_bus_el'\n",
    "    renewables_eu_2030_DG.drop(columns='fuel', inplace=True)\n",
    "    renewables_eu_2030_DG = renewables_eu_2030_DG.sort_values(by='country')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sources_res = pd.concat([sources_res, sources_nonfluc, sources_hydro, sources_RES])\n",
    "all_sources_res = all_sources_res.loc[~(all_sources_res.index.duplicated(keep=\"first\"))]\n",
    "\n",
    "if use_tyndp_2018:\n",
    "    renewables_eu_capacities = pd.DataFrame(\n",
    "        index=all_sources_res.index.union(renewables_eu_2025_BEST.index).union(renewables_eu_2030_DG.index),\n",
    "        columns=list(range(2018, 2031)), dtype=\"float64\")\n",
    "\n",
    "    renewables_eu_capacities[2018] = all_sources_res[\"capacity\"].astype(\"float64\")\n",
    "    renewables_eu_capacities[2025] = renewables_eu_2025_BEST[\"capacity\"]\n",
    "    renewables_eu_capacities[2030] = renewables_eu_2030_DG[\"capacity\"]\n",
    "\n",
    "    renewables_eu_capacities.loc[:, [2018, 2025, 2030]] = (\n",
    "        renewables_eu_capacities.loc[:, [2018, 2025, 2030]].fillna(0))\n",
    "    renewables_eu_capacities = renewables_eu_capacities.interpolate(axis=\"columns\").round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not use_tyndp_2018:\n",
    "    renewables_eu_capacities = pd.DataFrame(\n",
    "    index=all_sources_res.index.union(renewables_eu_tyndp2022.index),\n",
    "    columns=list(range(2018, 2051)), dtype=\"float64\")\n",
    "\n",
    "    renewables_eu_capacities[2018] = all_sources_res[\"capacity\"].astype(\"float64\")\n",
    "    for iter_year in [2030, 2040, 2050]:\n",
    "        renewables_eu_capacities[iter_year] = renewables_eu_tyndp2022[f\"{iter_year}-01-01\"]\n",
    "\n",
    "    renewables_eu_capacities.loc[:, [2018, 2030, 2040, 2050]] = (\n",
    "        renewables_eu_capacities.loc[:, [2018, 2030, 2040, 2050]].fillna(0))\n",
    "    renewables_eu_capacities = renewables_eu_capacities.interpolate(axis=\"columns\").round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_constant_ix = [ix for ix in renewables_eu_capacities.index if '_ROR' in ix \n",
    "                    or ix in sources_nonfluc.index or ix in sources_hydro.index]\n",
    "renewables_eu_capacities.loc[keep_constant_ix,:] = (\n",
    "    renewables_eu_capacities.loc[keep_constant_ix, 2018].values.repeat(len(\n",
    "        renewables_eu_capacities.columns)).reshape(len(keep_constant_ix), len(renewables_eu_capacities.columns)))\n",
    "\n",
    "# Create subset for which capacities are extracted directly; no status quo capacity values given\n",
    "diff_ix = renewables_eu_capacities.index.difference(sources_RES.index)\n",
    "renewables_eu_abs_capacities = renewables_eu_capacities.loc[diff_ix]\n",
    "renewables_eu_abs_capacities[2017] = 0.0\n",
    "renewables_eu_capacities = renewables_eu_capacities.loc[[ix for ix in renewables_eu_capacities.index \n",
    "                                                         if ix not in diff_ix]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine capacity expansion factors for RES:\n",
    "* As quotient between capacity for 2030 from TYNDP and status quo of installed capacities (IRENA)\n",
    "* Values are corrected:\n",
    "    * For the status quo year (2019), values are set to 1. The same is made for 2017 and 2018.\n",
    "    * For wind onshore in DK1, the status quo values given were comparatively high whereas the TYNDP projection was moderate. Values were adjusted to match the moderate TYNDP projection.\n",
    "* Capacities are assumed to remain constant for the remaining RES incl. hydro and ROR (=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in renewables_eu_capacities.columns:\n",
    "    renewables_eu_capacities[col] = renewables_eu_capacities[col].div(sources_RES_cap['capacity'].astype(\"float64\"))\n",
    "    # data fix: lower threshold 1, i.e. installed capacity as of 2018\n",
    "    renewables_eu_capacities[col] = np.where(renewables_eu_capacities[col] < 1, 1.0, \n",
    "                                             renewables_eu_capacities[col])\n",
    "\n",
    "renewables_eu_capacities[2018] = np.where(renewables_eu_capacities[2018] > 1, 1.0, \n",
    "                                          renewables_eu_capacities[2018]) \n",
    "\n",
    "# Correct values for wind onshore in DK1\n",
    "DK_val_2030 = renewables_eu_capacities.at[\"DK1_source_windonshore\", 2030]\n",
    "renewables_eu_capacities.loc[\"DK1_source_windonshore\"] = np.nan\n",
    "renewables_eu_capacities.at[\"DK1_source_windonshore\", 2030] = DK_val_2030\n",
    "renewables_eu_capacities.at[\"DK1_source_windonshore\", 2018] = 1.0\n",
    "renewables_eu_capacities.interpolate(axis=1)\n",
    "\n",
    "# Fill na values and assume capacities for 2017 to be the same than 2018 values\n",
    "renewables_eu_capacities = renewables_eu_capacities.fillna(1)\n",
    "renewables_eu_capacities[2017] = 1.0\n",
    "renewables_eu_capacities = renewables_eu_capacities[range(2017, 2051)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add RES projection for Germany\n",
    "\n",
    "Add capacity assumption for RES capacity development for Germany:\n",
    "* Capacity values for 2030 can be either based on\n",
    "    * a study by Prognos, Öko-Institut and Wuppertal-Institut on behalf of Agora Energiewende, Agora Verkahreswende and Stiftung Klimaneutralität or\n",
    "    * EEG 2021 capacity expansion targets.\n",
    "    * EEG 2023 draft capacity expansion targets.\n",
    "* Values for water comprise ROR, reservoir as well as phes with natural inflow (same as in BMWi statistics from AGEB). &rarr; ROR part needed here is probably assumed to reamin constant and so is proceeded here.\n",
    "\n",
    "Steps applied:\n",
    "* Read in and select data source to be used (Prognos or EEG_2021)\n",
    "* Filter capacity values for 2030\n",
    "* Determine capacity expansion factors by dividing the capacities for 2030 through those of the status quo \n",
    "\n",
    "Full citations:\n",
    "* Prognos, Öko-Institut and Wuppertal-Institut (2020): Klimaneutrales Deutschland. In drei Schritten zu null Treibhausgasen bis 2050 über ein Zwischenziel von -65 % im Jahr 2030 als Teil des EU-Green-Deals. Study on behalf of Agora Energiewende, Agora Verkehrswende and Stiftung Klimaneutralität (full version), https://www.agora-energiewende.de/veroeffentlichungen/klimaneutrales-deutschland/, p. 53, accessed 09.11.2020.\n",
    "* EEG 2021: Gesetz für den Ausbau erneuerbarer Energien (Erneuerbare-Energien-Gesetz - EEG 2021). Erneuerbare-Energien-Gesetz vom 21. Juli 2014 (BGBl. I S. 1066), das zuletzt durch Artikel 1 des Gesetzes vom21. Dezember 2020 (BGBl. I S. 3138) geändert worden ist.\n",
    "* Kabinettsbeschluss EEG 2023 as of 06.04.2022: Entwurf eines Gesetzes zu Sofortmaßnahmen für einen beschleunigten Ausbau der erneuerbaren Energien und weiteren Maßnahmen im Stromsektor. https://www.bmwk.de/Redaktion/DE/Downloads/Energie/04_EEG_2023.pdf?__blob=publicationFile&v=8, accessed 14.05.2022\n",
    "* Kabinettsbeschluss WindSeeG as of 06.04.2022: Entwurf eines Zweiten Gesetzes zur Änderung des Windenergie-auf-See-Gesetzes und anderer Vorschriften. https://www.bmwk.de/Redaktion/DE/Downloads/Energie/04_novelle_windSeeG_kabinettfassung.pdf?__blob=publicationFile&v=8, accessed 14.05.2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_DE_EEG_2021 = pd.read_csv(\n",
    "    main_path[\"inputs\"] + sub_path[\"renewables\"] + input_file[\"RES_DE_EEG_2021\"],\n",
    "    sep=\";\", decimal=\",\"\n",
    ")\n",
    "\n",
    "res_DE_EEG_2023 = pd.read_csv(\n",
    "    main_path[\"inputs\"] + sub_path[\"renewables\"] + input_file[\"RES_DE_EEG_2023\"],\n",
    "    sep=\";\", decimal=\",\"\n",
    ")\n",
    "\n",
    "res_DE_Prognos = pd.read_csv(\n",
    "    main_path[\"inputs\"] + sub_path[\"renewables\"] + input_file[\"RES_DE_Prognos\"],\n",
    "    sep=\";\", decimal=\",\"\n",
    ")\n",
    "\n",
    "# Drop 2018 capacities\n",
    "res_DE_Prognos.drop(index=res_DE_Prognos[res_DE_Prognos[\"year\"] == 2018].index, inplace=True)\n",
    "\n",
    "res_capacity_projections = {\n",
    "    \"Prognos\": res_DE_Prognos,\n",
    "    \"EEG_2021\": res_DE_EEG_2021,\n",
    "    \"EEG_2023\": res_DE_EEG_2023\n",
    "}\n",
    "\n",
    "if res_capacity_projection not in res_capacity_projections.keys():\n",
    "    raise ValueError(\"`res_capacity_projection` has to be either 'Prognos', 'EEG_2021' or 'EEG_2023'.\")\n",
    "\n",
    "res_de_projection = res_capacity_projections[res_capacity_projection]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_de_projection.set_index(\"fuel\", inplace=True)\n",
    "\n",
    "res_de_capacity_development = pd.DataFrame(\n",
    "    index=res_de_projection.index.unique(),\n",
    "    columns=range(2020, 2031), dtype=\"float64\"\n",
    ")\n",
    "\n",
    "if res_capacity_projection == \"EEG_2023\":\n",
    "    res_de_capacity_development = pd.DataFrame(\n",
    "        index=res_de_projection.index.unique(),\n",
    "        columns=range(2020, 2051), dtype=\"float64\"\n",
    "    )\n",
    "\n",
    "years_for_iteration = []\n",
    "if res_capacity_projection == \"Prognos\":\n",
    "    years_for_iteration = [2025, 2030]\n",
    "elif res_capacity_projection == \"EEG2_021\":\n",
    "    years_for_iteration = [2022, 2024, 2026, 2028, 2030]\n",
    "elif res_capacity_projection == \"EEG_2023\":\n",
    "    years_for_iteration = [2024, 2026, 2028, 2030, 2035, 2040]\n",
    "\n",
    "for iter_year in years_for_iteration:\n",
    "    try:\n",
    "        res_de_capacity_development[iter_year] = (\n",
    "            res_de_projection.loc[res_de_projection[\"year\"] == iter_year, \"capacity [GW]\"].astype(\"float64\"))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "eeg_pps_de_agg[\"label\"] = \"DE_source_\" + eeg_pps_de_agg.index\n",
    "res_de_capacity_development[2020] = eeg_pps_de_agg.loc[res_de_capacity_development.index, 'capacity']/1000\n",
    "    \n",
    "res_de_capacity_development = res_de_capacity_development.interpolate(axis=\"columns\").round(1)\n",
    "res_de_capacity_development = res_de_capacity_development.mul(1000).div(\n",
    "    eeg_pps_de_agg[\"capacity\"], axis=0).fillna(1)\n",
    "res_de_capacity_development.loc[\"biomassEEG\"] = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separate RES capacities for Germany and create time series for Germany for investment model\n",
    "* Extract capacities for 2020\n",
    "* Scale up time series by multipyling with capacity development factors; reindex to respective year and concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_DE_capacity_development_transposed = res_de_capacity_development.T\n",
    "res_DE_capacity_development_transposed.columns = [\n",
    "    \"DE_source_\" + col for col in res_DE_capacity_development_transposed.columns\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources_res_germany_ts = sources_ts_res[[col for col in sources_ts_res.columns if \"DE_\" in col]]\n",
    "sources_res_germany_ts.rename(columns={\n",
    "    \"DE_bus_solarPV\": \"DE_source_solarPV\",\n",
    "    \"DE_bus_windonshore\": \"DE_source_windonshore\",\n",
    "    \"DE_bus_windoffshore\": \"DE_source_windoffshore\"\n",
    "}, inplace=True)\n",
    "to_concat = []\n",
    "for iter_year in range(2020, 2051):\n",
    "    ts_of_iter_year = sources_res_germany_ts.mul(res_DE_capacity_development_transposed.loc[iter_year])\n",
    "    ts_of_iter_year = tools.reindex_time_series(ts_of_iter_year, iter_year)\n",
    "    to_concat.append(ts_of_iter_year)\n",
    "\n",
    "res_investment_ts_de = pd.concat(to_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res_de_investment = eeg_pps_de_agg.copy()\n",
    "res_de_investment[\"country\"] = \"DE\"\n",
    "res_de_investment[\"to\"] = res_de_investment[\"country\"] + \"_bus_el\"\n",
    "res_de_investment.set_index(\"label\", drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scale (up) capacities for renewable infeed\n",
    "\n",
    "Renewable infeed for the target year is determined by a simple scaling the renewable capacities by the capacity expansion factors (capacity in 2030 / capacity in 2017 resp. 2019) determined above:\n",
    "* The capacities of the RES sources are multiplied by the capacity expansion factors.<br>\n",
    "_NOTE: While they represent installed capacities for Germany, for other European countries they represent the peak infeed capacity._\n",
    "* The infeed timeseries themselves are not modified, but taken as they are.\n",
    "* For RES not contained in the status quo, \n",
    "    * the absolute capacity projections from the TYNDP are used and assigned and\n",
    "    * timeseries of neighbouring countries are assigned as a proxy.\n",
    "    * _Note: A scaling is not made due to the inconsistencies of the IRENA / ENTSOE data.<br>Actually, this would be useful since one is installed capacities while the other is maximum outputs._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_de_capacity_development[\"label\"] = \"DE_source_\" + res_de_capacity_development.index\n",
    "res_de_capacity_development.set_index(\"label\", inplace=True)\n",
    "\n",
    "res_capacities = pd.concat([renewables_eu_capacities, res_de_capacity_development])\n",
    "res_capacities = res_capacities.loc[~res_capacities.index.duplicated()]\n",
    "res_capacities[2020] = 1.0\n",
    "\n",
    "# Extract capacity for 2020 for investment model\n",
    "sources_RES_investment = sources_RES.copy()\n",
    "\n",
    "# Determine capacity in target year\n",
    "sources_RES[\"capacity\"] = sources_RES[\"capacity\"].astype(\"float64\").mul(res_capacities[year], axis=0).round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "renewables_eu_abs_capacities_dispatch = renewables_eu_abs_capacities[[year]]\n",
    "renewables_eu_abs_capacities_dispatch[\"index_label\"] = renewables_eu_abs_capacities_dispatch.index\n",
    "renewables_eu_abs_capacities_dispatch[\"country\"] = (\n",
    "    renewables_eu_abs_capacities_dispatch[\"index_label\"].str.split(\"_\", n=1, expand=True)[0])\n",
    "renewables_eu_abs_capacities_dispatch[\"to\"] = renewables_eu_abs_capacities_dispatch[\"country\"] + \"_bus_el\"\n",
    "renewables_eu_abs_capacities_dispatch = renewables_eu_abs_capacities_dispatch.drop(\n",
    "    columns=\"index_label\").rename(columns={year: \"capacity\"})\n",
    "\n",
    "# Split investment data set and use capacities as of 2020\n",
    "renewables_eu_abs_capacities_investment = renewables_eu_abs_capacities_dispatch.copy()\n",
    "renewables_eu_abs_capacities_investment[\"capacity\"] = renewables_eu_abs_capacities[[2020]]\n",
    "\n",
    "sources_RES = pd.concat([sources_RES, renewables_eu_abs_capacities_dispatch])\n",
    "sources_RES_investment = pd.concat([sources_RES_investment, renewables_eu_abs_capacities_investment])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some bidding zones for which no capacity information for certain RES for the status quo is available.\n",
    "Thus, no timeseries information exists.\n",
    "\n",
    "As a proxy, these market zones will simply be assigned timeseries from neighbouring countries:\n",
    "* FR offshore, IT offshore &rarr; DE offshore\n",
    "* NO2, NO5 offshore &rarr; DK1_source_windoffshore\n",
    "* NO2-5 & SE1-4 solarPV &rarr; DK solarPV\n",
    "* PL solarPV and offshore &rarr; DE solarPV and offshore\n",
    "* SE4 offshore &rarr; DK offshore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_res_ts_dict = {\n",
    "    'FR_source_windoffshore': 'DE_bus_windoffshore',\n",
    "    'IT_source_windoffshore': 'DE_bus_windoffshore',\n",
    "    'NO2_source_windoffshore': 'DK1_source_windoffshore',\n",
    "    'NO5_source_windoffshore': 'DK1_source_windoffshore',\n",
    "    'NO2_source_solarPV': 'DK1_source_solarPV',\n",
    "    'NO3_source_solarPV': 'DK1_source_solarPV',\n",
    "    'NO4_source_solarPV': 'DK1_source_solarPV',\n",
    "    'NO5_source_solarPV': 'DK1_source_solarPV',\n",
    "    'NO5_source_windonshore': 'DK1_source_windonshore',\n",
    "    'PL_source_solarPV': 'DE_bus_solarPV',\n",
    "    'PL_source_windoffshore': 'DE_bus_windoffshore',\n",
    "    'SE1_source_solarPV': 'DK2_source_solarPV',\n",
    "    'SE2_source_solarPV': 'DK2_source_solarPV',\n",
    "    'SE3_source_solarPV': 'DK2_source_solarPV',\n",
    "    'SE4_source_solarPV': 'DK2_source_solarPV',\n",
    "    'SE1_source_windoffshore': 'DK2_source_windoffshore',\n",
    "    'SE2_source_windoffshore': 'DK2_source_windoffshore',\n",
    "    'SE3_source_windoffshore': 'DK2_source_windoffshore',\n",
    "    'SE4_source_windoffshore': 'DK2_source_windoffshore'\n",
    "}\n",
    "\n",
    "# Assign missing timeseries\n",
    "for k, v in missing_res_ts_dict.items():\n",
    "    sources_ts_res[k] = sources_ts_res.loc[:,v]\n",
    "    # Normalize if German (non-normalized) values have been assigned\n",
    "    if \"DE_bus_\" in v:\n",
    "        sources_ts_res[k] = sources_ts_res[k].div(sources_ts_res[k].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources_RES.to_csv(\n",
    "    main_path[\"outputs\"] + output_file[\"sources_renewables\"] + \"_\" + str(year) + \".csv\"\n",
    ")\n",
    "sources_ts_res_2017 = sources_ts_res.copy()\n",
    "\n",
    "sources_ts_res = tools.reindex_time_series(sources_ts_res_2017, year)\n",
    "\n",
    "sources_ts_res.to_csv(\n",
    "    main_path[\"outputs\"] + output_file[\"sources_renewables_ts\"] + \"_\" + str(year) + \".csv\"\n",
    ")\n",
    "sources_RES.to_excel(writer, sheet_name='sources_renewables' + \"_\" + str(year))\n",
    "sources_ts_res.tz_localize(None).to_excel(writer, sheet_name='sources_renewables_ts' + \"_\" + str(year))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and write investment-related data for Europe\n",
    "* Use capacities for 2020 and scale infeed time series accordingly\n",
    "* Combine with data for Germany and write to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capacities_2020 = renewables_eu_capacities[2020].copy()\n",
    "for col in renewables_eu_capacities.columns:\n",
    "    renewables_eu_capacities[col] = renewables_eu_capacities[col].div(capacities_2020)\n",
    "renewables_eu_capacities = renewables_eu_capacities[range(2020, 2051)]\n",
    "renewables_eu_capacities_transposed = renewables_eu_capacities.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capacities_abs_2020 = renewables_eu_abs_capacities[2020].copy()\n",
    "for col in renewables_eu_abs_capacities.columns:\n",
    "    renewables_eu_abs_capacities[col] = renewables_eu_abs_capacities[col].div(capacities_abs_2020)\n",
    "renewables_eu_abs_capacities = renewables_eu_abs_capacities[range(2020, 2051)]\n",
    "renewables_eu_abs_capacities_transposed = renewables_eu_abs_capacities.T\n",
    "\n",
    "renewables_eu_capacities_transposed = pd.concat([\n",
    "    renewables_eu_capacities_transposed, renewables_eu_abs_capacities_transposed\n",
    "], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources_ts_res_eu = sources_ts_res_2017[[col for col in sources_ts_res_2017.columns if \"DE_\" not in col]]\n",
    "\n",
    "to_concat = []\n",
    "for iter_year in range(2020, 2051):\n",
    "    ts_of_iter_year = sources_ts_res_eu.mul(renewables_eu_capacities_transposed.loc[iter_year])\n",
    "    ts_of_iter_year = tools.reindex_time_series(ts_of_iter_year, iter_year)\n",
    "    to_concat.append(ts_of_iter_year)\n",
    "\n",
    "# Combine data sets for EU and Germany\n",
    "res_investment_ts_eu = pd.concat(to_concat)\n",
    "res_investment_ts = pd.concat([\n",
    "    res_investment_ts_de, \n",
    "    res_investment_ts_eu[[col for col in res_investment_ts_eu.columns if col not in res_investment_ts_de.columns]]\n",
    "], axis=1)\n",
    "\n",
    "res_investment = pd.concat([res_de_investment, sources_RES_investment])\n",
    "res_investment = res_investment.drop_duplicates()\n",
    "res_investment = res_investment.loc[~res_investment.index.duplicated()]\n",
    "\n",
    "# Write investment data\n",
    "res_investment_ts.round(rounding_precision).to_csv(\n",
    "    main_path[\"outputs\"] + output_file[\"sources_renewables_ts\"] + \"_hourly.csv\"\n",
    ")\n",
    "res_investment.to_csv(\n",
    "    main_path[\"outputs\"] + output_file[\"sources_renewables\"] + \"_investment_model.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate sensitivities for PV generation\n",
    "One of the considered sensitivities for the demand reponse analyses with *pommesinvest* is the split between PV on the one-hand and wind onshore on the other hand. This is for the following three reasons:\n",
    "1. It has been shown that PV better fits daily profiles of demand reponse. Thus, a (slightly) higher demand response activation is assumed in case of more PV in the system, compared to wind onshore.\n",
    "2. For wind onshore, there are some problems with local acceptance and also there is the problem of lengthy planning procedures. Thus, solar PV might close this gap.\n",
    "3. Solar PV has experienced a very drastic reduction in costs and LCOE. In the technology-neutral tenders, solar PV plants have outperformed wind.\n",
    "Thus, especially higher shares of solar PV seem like an interesting scenario. Nonetheless, for academic purposes, also lower shares of PV compared to the exogenous pathway shall be considered.\n",
    "\n",
    "The aim is to keep the overall renewable power production stable for Germany on a yearly basis. Thus, it is proceeded as follows:\n",
    "* First, the annual generations of PV and wind onshore are calculated.\n",
    "* Second, the production of PV is altered by the sensitivity factors defined above (as a default: -50%, -25%, +25%, +50%).\n",
    "* Third, wind onshore production is scaled accordingly so that the prerequisite of an overall constant annual production is met."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solarPV_and_windonshore_germany_ts = res_investment_ts[[\"DE_source_solarPV\", \"DE_source_windonshore\"]].copy()\n",
    "\n",
    "annual_generation = pd.DataFrame(index=range(2020, 2051), columns=[\"DE_source_solarPV\", \"DE_source_windonshore\"])\n",
    "for iter_year in range(2020, 2051):\n",
    "    annual_generation.loc[iter_year] = solarPV_and_windonshore_germany_ts.loc[\n",
    "        solarPV_and_windonshore_germany_ts.index.year == iter_year\n",
    "    ].sum() * res_investment.loc[[\"DE_source_solarPV\", \"DE_source_windonshore\"], \"capacity\"].T.values\n",
    "\n",
    "annual_generation[\"overall\"] = annual_generation.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_investment_ts_sensitivity = res_investment_ts.copy()\n",
    "annual_generation_sensitivity = annual_generation[[\"DE_source_solarPV\", \"DE_source_windonshore\"]].copy()\n",
    "\n",
    "for sensitivity, multiplier in sensitivities.items():\n",
    "    # Calculate annual generation and derive multiplier for wind onshore\n",
    "    annual_generation_sensitivity[\"DE_source_solarPV\"] = annual_generation[\"DE_source_solarPV\"] * multiplier\n",
    "    annual_generation_sensitivity[\"DE_source_windonshore\"] = (\n",
    "        annual_generation[\"overall\"] - annual_generation_sensitivity[\"DE_source_solarPV\"]\n",
    "    )\n",
    "    \n",
    "    multiplier_windonshore = (\n",
    "        annual_generation_sensitivity[\"DE_source_windonshore\"]\n",
    "        / annual_generation[\"DE_source_windonshore\"] \n",
    "    )\n",
    "    multiplier_windonshore.index = multiplier_windonshore.index.astype(str) + \"-01-01\"\n",
    "    multiplier_windonshore.loc[\"2051-01-01\"] = multiplier_windonshore.iloc[-1]\n",
    "    multiplier_windonshore = helpers.resample_timeseries(\n",
    "        multiplier_windonshore, freq=\"H\", interpolation_rule=\"ffill\"\n",
    "    )[:-1]\n",
    "    \n",
    "    # Assign adjusted time series and write\n",
    "    res_investment_ts_sensitivity[\"DE_source_solarPV\"] = (\n",
    "        res_investment_ts[\"DE_source_solarPV\"] * multiplier\n",
    "    )\n",
    "    res_investment_ts_sensitivity[\"DE_source_windonshore\"] = (\n",
    "        res_investment_ts[\"DE_source_windonshore\"] * multiplier_windonshore\n",
    "    )\n",
    "    res_investment_ts_sensitivity.round(rounding_precision).to_csv(\n",
    "        main_path[\"outputs\"] + output_file[\"sources_renewables_ts\"] + f\"_hourly_sensitivity_{sensitivity}.csv\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extension for Germany: Determine new-built RES capacities and values applied\n",
    "General approach:\n",
    "* Add plants commissioned until 2030 and introduce an assumption for the values applied by year.\n",
    "* For commissionings until 2022 (2025 for offshore) the current results from the EEG tender procedures (capacities and prices) are used.\n",
    "* For commissionings from 2023 on (2026 for offshore), an assumption for the capacities to be commissioned is made. Therefore, the target capacity to be build is split equally onto the years from 2023 to 2030.\n",
    "* Thereby, decommissionings based on unit age have to be taken into account. It is assumed that plants are decommissioned after 20 years of operation.\n",
    "\n",
    "#### Determine decommissioning years and calculate (gross) capacity expansion needed\n",
    "\n",
    "Determine amount of RES capacity to be installed for every year:\n",
    "* Decommissioning of EEG plants will happen from 2021 on after 20 years of operation.\n",
    "    * For the sake of simplicity, use commissioning year only and only stick to the rules from prior EEG versions (until EEG 2014) which granted payments for 20 years + remainder of the commissioning year. &rarr; Assume 21 years lifetime.\n",
    "    * Determine the remaining capacities by energy source and store them in a dictionary indexed by years.\n",
    "* Determine the amount of capacity commissioned until 2022 by already finished RES tenders.\n",
    "* Determine the amount of (gross) capacity to be added to fill up the gap up to the installed capacities for 2030 given in the study from Prognos et al. (2020) or the EEG 2021."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eeg_pps_de['commissioning_year'] = eeg_pps_de['commissioning_date'].astype(str).str.slice(start=0, stop=4).astype(int)\n",
    "eeg_pps_de['decommissioning_year'] = np.where(eeg_pps_de['commissioning_year'] + 21 > 2020, \n",
    "                                              eeg_pps_de['commissioning_year'] + 21, 2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the amount of installed capacity remaining for every year\n",
    "eeg_pps_de_agg_by_year = pd.DataFrame(index=eeg_pps_de.energy_source.unique(), \n",
    "                                      columns=range(2020, 2031), dtype=\"float64\")\n",
    "\n",
    "for iter_year in range(2020, 2031):\n",
    "    to_group = eeg_pps_de[eeg_pps_de['decommissioning_year'] > iter_year]\n",
    "    eeg_pps_de_agg_by_year[iter_year] = (to_group.groupby('energy_source').agg({'capacity': 'sum'}) \n",
    "                                         / 1000).round(1)  # convert kW to MW\n",
    "    del to_group\n",
    "    \n",
    "eeg_pps_de_agg_by_year.rename(index=dict_eeg_pps_names, inplace=True)\n",
    "\n",
    "cap_needed_until_2030 = res_de_projection.loc[res_de_projection[\"year\"] == 2030, 'capacity [GW]'].mul(1000).sub(\n",
    "    eeg_pps_de_agg_by_year[2030]).fillna(0).round(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Include data from German RES tenders\n",
    "\n",
    "* Read in the data for the different energy sources.\n",
    "* Decrease the amount of capacity needed by\n",
    "    * capacity to be commissioned from the EEG tender procedures and\n",
    "    * capacity installed elsewhise (only for solarPV).\n",
    "* Obtain values applied from the tender data.\n",
    "\n",
    "_NOTE: For the sake of simplicity, the deviating deadlines for Bürgerenergiegesellschaften are neglected for wind onshore._\n",
    "\n",
    "RES tenders for wind onshore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onshore_tenders = pd.read_excel(\n",
    "    main_path[\"inputs\"] + sub_path[\"renewables\"] + input_file[\"onshore_tenders\"],\n",
    "    sheet_name='Alle Runden', skiprows=6, nrows=24, index_col=0, \n",
    "    usecols=[1, 13, 18, 19, 20, 26]\n",
    ")\n",
    "\n",
    "dict_tender_cols = {'Zuschlagsmenge (kW)': 'capacity_awarded',\n",
    "                    'Gebotswerte mit Zuschlag (ct/kWh)': 'min',\n",
    "                    'Unnamed: 19': 'max',\n",
    "                    'Zuschlagswert (ct/kWh)': 'weighted_ave',\n",
    "                    'Frist Inbetriebnahme ohne Pönale2': 'due_date'}\n",
    "onshore_tenders.rename(columns=dict_tender_cols, inplace=True)\n",
    "onshore_tenders = onshore_tenders[~(onshore_tenders.index.isna())]\n",
    "\n",
    "# Due to the EEG, deadline for commissioning is 24 months for the tenders in 2019\n",
    "onshore_tenders.loc[onshore_tenders.due_date.isna(), \n",
    "                    'due_date'] = (onshore_tenders.loc[onshore_tenders.due_date.isna()].index\n",
    "                                   + pd.DateOffset(months=24) + pd.tseries.offsets.MonthEnd(1))\n",
    "\n",
    "# Determine weigthed averages per commissioning year\n",
    "onshore_tenders['tender_year'] = onshore_tenders.index.year\n",
    "onshore_tenders['commissioning_year'] = pd.DatetimeIndex(onshore_tenders.due_date).year\n",
    "\n",
    "wm = lambda x: np.average(x, weights=onshore_tenders.loc[x.index, 'capacity_awarded'])\n",
    "\n",
    "onshore_tenders.loc[:,['min', 'max', 'weighted_ave']] = (\n",
    "    onshore_tenders.loc[:,['min', 'max', 'weighted_ave']].astype(float))\n",
    "\n",
    "windonshore_commissionings = onshore_tenders.groupby('commissioning_year').aggregate({\n",
    "    'capacity_awarded':'sum',\n",
    "    'min': wm, 'max': wm, 'weighted_ave': wm})\n",
    "\n",
    "windonshore_commissionings.drop([2019,2020], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RES tenders for PV (1st and 2nd segment) and common RES tenders (wind + PV):\n",
    "* So far, only PV plants have been successful within the common tender procedure.\n",
    "* Hence, the capacities are added onto the PV bids.\n",
    "\n",
    "_Note: Innovation tenders are note included since there are some difficulties with the remuneration instrument of a fixed market premium which would require for a power price estimate in order to come to an LCOE estimate. Therefore, this segment (which is mostly dominated by PV + storage) is neglegted._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solarPV_tenders = pd.read_excel(\n",
    "    main_path[\"inputs\"] + sub_path[\"renewables\"] + input_file[\"solar_tenders\"],\n",
    "    sheet_name='Alle Runden', skiprows=7, nrows=28, index_col=0, \n",
    "    usecols=[1, 13, 18, 19, 20, 26])\n",
    "\n",
    "solarPV_tenders2 = pd.read_excel(\n",
    "    main_path[\"inputs\"] + sub_path[\"renewables\"] + input_file[\"solar_tenders2\"],\n",
    "    sheet_name='Alle Runden', skiprows=5, nrows=2, index_col=0, \n",
    "    usecols=[1, 13, 18, 19, 20, 24])\n",
    "\n",
    "common_tenders = pd.read_excel(\n",
    "    main_path[\"inputs\"] + sub_path[\"renewables\"] + input_file[\"common_tenders\"],\n",
    "    sheet_name='Alle Runden', skiprows=5, nrows=7, index_col=0, \n",
    "    usecols=[0, 8, 10, 11, 12, 18])\n",
    "\n",
    "dict_tender_cols['Frist Zahlungsberechtigung ohne Fördersatzreduktion2'] = 'due_date'\n",
    "solarPV_tenders.rename(columns=dict_tender_cols, inplace=True)\n",
    "solarPV_tenders2.rename(columns=dict_tender_cols, inplace=True)\n",
    "\n",
    "dict_tender_cols[\"Zuschlagsmenge (MW)\"] = 'capacity_awarded'\n",
    "dict_tender_cols[\"Zuschlagswerte (ct/kWh)\"] = 'min'\n",
    "dict_tender_cols[\"Unnamed: 11\"] = 'max'\n",
    "dict_tender_cols[\"Unnamed: 12\"] = 'weighted_ave'\n",
    "dict_tender_cols['Frist zum Antrag auf Zahlungsberechtigung2 ohne Fördersatzreduktion\\n - bei Solar -'] = 'due_date'\n",
    "common_tenders.rename(columns=dict_tender_cols, inplace=True)\n",
    "\n",
    "solarPV_tenders = solarPV_tenders[~(solarPV_tenders.index.isna())]\n",
    "solarPV_tenders2 = solarPV_tenders2[~(solarPV_tenders2.index.isna())]\n",
    "common_tenders = common_tenders[~(common_tenders.index.isna())]\n",
    "\n",
    "solarPV_tenders = solarPV_tenders.append(common_tenders).append(solarPV_tenders2)\n",
    "\n",
    "solarPV_tenders.loc[solarPV_tenders.due_date.isna(), \n",
    "                    'due_date'] = (solarPV_tenders.loc[solarPV_tenders.due_date.isna()].index\n",
    "                                   + pd.DateOffset(months=24) + pd.tseries.offsets.MonthEnd(1))\n",
    "\n",
    "# Determine weigthed averages per commissioning year\n",
    "solarPV_tenders['tender_year'] = solarPV_tenders.index.year\n",
    "solarPV_tenders['commissioning_year'] = pd.DatetimeIndex(solarPV_tenders.due_date).year\n",
    "\n",
    "solarPV_tenders.reset_index(drop=True, inplace=True)\n",
    "\n",
    "wm2 = lambda x: np.average(x, weights=solarPV_tenders.loc[x.index, 'capacity_awarded'])\n",
    "solarPV_tenders.loc[:,['min', 'max', 'weighted_ave']] = (\n",
    "    solarPV_tenders.loc[:,['min', 'max', 'weighted_ave']].astype(float))\n",
    "\n",
    "solarPV_commissionings = solarPV_tenders.groupby('commissioning_year').aggregate({\n",
    "    'capacity_awarded':'sum',\n",
    "    'min': wm2, 'max': wm2, 'weighted_ave': wm2})\n",
    "solarPV_commissionings.drop([2016, 2017, 2018, 2019, 2020], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RES tenders for wind offshore (transitional model):\n",
    "\n",
    "Data sources:\n",
    "* BNetzA (2017): BK6-17-001, Ergebnisse der 1. Ausschreibung vom 01.04.2017, https://www.bundesnetzagentur.de/DE/Service-Funktionen/Beschlusskammern/1_GZ/BK6-GZ/2017/BK6-17-001/Ergebnisse_erste_Ausschreibung.pdf?__blob=publicationFile&v=3, Abruf am 17.11.2020.\n",
    "* BNetzA (2018): BK6-18-001, Ergebnisse der 2. Ausschreibung vom 01.04.2018, https://www.bundesnetzagentur.de/DE/Service-Funktionen/Beschlusskammern/1_GZ/BK6-GZ/2018/BK6-18-001/Ergebnisse_zweite_ausschreibung.pdf?__blob=publicationFile&v=3, Abruf am 17.11.2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windoffshore_commissionings = pd.DataFrame(\n",
    "    index=['2024', '2025'],\n",
    "    data={'capacity_awarded': [1490000, 1610000], \n",
    "          'min': [0, 0],\n",
    "          'max': [6, 9.83],\n",
    "          'weighted_ave': [0.44, 4.66]})\n",
    "\n",
    "windoffshore_commissionings.index.name='commissioning_year'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decrease capacity needed until 2030 by the amount of capacity already (to be) installed from the tender procedures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap_needed_until_2030.loc['windonshore'] -= windonshore_commissionings.loc[2021:2022, \n",
    "                                                                           'capacity_awarded'].div(1000).sum()\n",
    "cap_needed_until_2030.loc['windoffshore'] -= windoffshore_commissionings['capacity_awarded'].div(1000).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add capacity from PV build outside the tendering procedures:\n",
    "* rooftop PV plants, tenant electricity plants (\"Mieterstrom\") and smaller open space plants &rarr; The value applied is determined by the gross capacity expansion of those plants for which in turn the corrected expansion values are used.\n",
    "* The capacity values and remuneration values are read in, whereby the values are adjusted every quarter based on the rolling half year before.\n",
    "* The capacity values are aggregated per year and subtracted from the capacity exapansion need for solarPV until 2030\n",
    "\n",
    "Data sources:\n",
    "* BNetzA (2020): EEG-Registerdaten und Fördersätze, https://www.bundesnetzagentur.de/DE/Sachgebiete/ElektrizitaetundGas/Unternehmen_Institutionen/ErneuerbareEnergien/ZahlenDatenInformationen/EEG_Registerdaten/EEG_Registerdaten_node.html;jsessionid=C9A499BB48D4F32668B73B6769FBBD84, Abruf am 19.11.2020.\n",
    "* BNetzA (2020): Archiv EEG-Vergütungssätze und Datenmeldungen, https://www.bundesnetzagentur.de/DE/Sachgebiete/ElektrizitaetundGas/Unternehmen_Institutionen/ErneuerbareEnergien/ZahlenDatenInformationen/EEG_Registerdaten/ArchivDatenMeldgn/ArchivDatenMeldgn_node.html, Abruf am 19.11.2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_cap_names_dict = {\n",
    "    '2021_01': (input_file[\"PV_2021_01\"], 6),\n",
    "    '2021_02': (input_file[\"PV_2021_02\"], 3),\n",
    "    '2021_03': (input_file[\"PV_2021_03\"], 3)\n",
    "}\n",
    "\n",
    "pv_cap_dict = {}\n",
    "\n",
    "pv_capacities = pd.DataFrame(columns=['capacity'])\n",
    "pv_capacities.index.name = 'year'\n",
    "\n",
    "pv_col_names = {'Leistung (kWp)': 'capacity',\n",
    "                'Leistung (kWp) 1': 'capacity',\n",
    "                'Leistung (kW) 1': 'capacity'}\n",
    "\n",
    "for k, v in pv_cap_names_dict.items():\n",
    "    # Extract capacity installations info\n",
    "    pv_cap_dict[k] = pd.read_excel(\n",
    "        main_path[\"inputs\"] + sub_path[\"renewables\"] + v[0],\n",
    "        skiprows=6, usecols=\"B:C\", nrows=v[1], index_col=0\n",
    "    )\n",
    "\n",
    "    if not isinstance(pv_cap_dict[k].index, pd.DatetimeIndex):\n",
    "        pv_cap_dict[k].index = pv_cap_dict[k].index.str.strip('*')\n",
    "        pv_cap_dict[k]['year'] = pv_cap_dict[k].index.str[-4:]\n",
    "    else:\n",
    "        pv_cap_dict[k]['year'] = pv_cap_dict[k].index.year\n",
    "    \n",
    "    pv_cap_dict[k].rename(columns=pv_col_names, inplace=True)\n",
    "    pv_capacities = pv_capacities.append(pv_cap_dict[k].groupby('year').agg('sum'))\n",
    "    \n",
    "pv_capacities.reset_index(inplace=True)\n",
    "pv_capacities['year'] = pv_capacities['year'].astype(str)\n",
    "pv_capacities_agg = pv_capacities.groupby('year').agg('sum').div(1000)\n",
    "\n",
    "# Drop values for 2020 (already given)\n",
    "pv_capacities_agg.drop(index='2020', inplace=True)\n",
    "pv_capacities_agg.at['2021', 'capacity'] = pv_capacities_agg.at['2021', 'capacity']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribute the capacity needed (gross) installations:\n",
    "* Gross capacity installations have to take place between 2023 and 2030 for wind and solarPV\n",
    "* For windoffshore, gross installations equal the net installations and take place between 2026 and 2030\n",
    "* Installations from tender procedures are given for PV &rarr; I.e., remaining rooftop installations needed for PV for 2021 and 2022 are given by the annual capacity expansion need minus the installations from the tender procedures (and already installed rooftop capacity for 2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annual_cap_expansion = pd.DataFrame(dtype=float, index=['windonshore', 'solarPV', 'windoffshore'],\n",
    "                                    columns=['capacity', 'start_year'])\n",
    "\n",
    "annual_cap_expansion.loc['windonshore', \n",
    "                         'solarPV', \n",
    "                         'windoffshore'] = [(cap_needed_until_2030.loc['windonshore'] / (2030 - 2022), 2023), \n",
    "                                            (cap_needed_until_2030.loc['solarPV'] / (2030 - 2020), 2021),\n",
    "                                            (cap_needed_until_2030.loc['windoffshore'] / (2030 - 2025), 2026)]\n",
    "\n",
    "pv_capacities_agg.loc['2021'] = (annual_cap_expansion.at['solarPV', 'capacity'] \n",
    "                                 - solarPV_commissionings.at[2021, 'capacity_awarded'] / 1000)\n",
    "pv_capacities_agg.loc['2022'] = (annual_cap_expansion.at['solarPV', 'capacity'] \n",
    "                                 - solarPV_commissionings.at[2022, 'capacity_awarded'] / 1000)\n",
    "pv_capacities_agg.index = pv_capacities_agg.index.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add capacities and values applied from the RES tender scheme\n",
    "Determine remaining capacities in target year:\n",
    "* Filter out plants with a decommissioning year prior to the target year.\n",
    "* Include DataFrames holding artificial new-built units. Artificial new-built units hereby means a collection of units to be newly installed which are grouped by their values applied due to a given capacity distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out plants to be decommissioned\n",
    "eeg_pps_de = eeg_pps_de[eeg_pps_de['decommissioning_year'] > year]\n",
    "\n",
    "eeg_pps_de_solarPV = pd.DataFrame(columns=eeg_pps_de.columns)\n",
    "eeg_pps_de_windonshore = pd.DataFrame(columns=eeg_pps_de.columns)\n",
    "eeg_pps_de_windoffshore = pd.DataFrame(columns=eeg_pps_de.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply a function to determine capacity distribution as well as another function to create new-built units:\n",
    "* The data sets each contain a lower, a middle and an upper value for the value applied.\n",
    "* A beta distribution function for the capacity shares is introduced and the corresponding equally sliced values applied are attributed to the corresponding capacity shares.\n",
    "* Add power plants based on capacity distribution to the new-built DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create capacity distributions and corresponding values applied\n",
    "PV_cap_distribution = eeg.create_cap_distribution_from_beta(\n",
    "    solarPV_commissionings,\n",
    "    cols=['min', 'weighted_ave', 'max'])\n",
    "                                              \n",
    "onshore_cap_distribution = eeg.create_cap_distribution_from_beta(\n",
    "    windonshore_commissionings,\n",
    "    cols=['min', 'weighted_ave', 'max'])\n",
    "                                              \n",
    "offshore_cap_distribution = eeg.create_cap_distribution_from_beta(\n",
    "    windoffshore_commissionings,\n",
    "    cols=['min', 'weighted_ave', 'max'])\n",
    "\n",
    "# Create new-built units with the corresponding market values\n",
    "new_eeg_pps_de_solarPV = eeg.add_new_built_res(PV_cap_distribution, eeg_pps_de_solarPV, \n",
    "                                               solarPV_commissionings, 'solarPV', \n",
    "                                               capacity_col='capacity_awarded', indexed_by_years=True)\n",
    "\n",
    "new_eeg_pps_de_windonshore = eeg.add_new_built_res(onshore_cap_distribution, eeg_pps_de_windonshore, \n",
    "                                                   windonshore_commissionings, 'windonshore', \n",
    "                                                   capacity_col='capacity_awarded', indexed_by_years=True)\n",
    "\n",
    "new_eeg_pps_de_windoffshore = eeg.add_new_built_res(offshore_cap_distribution, eeg_pps_de_windoffshore, \n",
    "                                                    windoffshore_commissionings, 'windoffshore', \n",
    "                                                    capacity_col='capacity_awarded', indexed_by_years=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add exogeneous assumptions for development of RES value applied\n",
    "\n",
    "The estimates are based on the latest study from Fraunhofer ISE:\n",
    "\n",
    "Fraunhofer ISE (2021): Stromgestehungskosten Erneuerbare Energien , Freiburg, June 2021.\n",
    "\n",
    "The study in turn uses a learning rate approach based on the following sources for wind capacity projections:\n",
    "* GWEC (2016): Global Wind Energy Outlook 2016. Global Wind Energy Council.\n",
    "* IRENA (2021b): World Energy Transitions Outlook: 1.5°C Pathway. International Atomic Energy Agency. Abu Dhabi.\n",
    "\n",
    "The following learning curve formula is used to derive capex for future years:\n",
    "$$ C(x_t)=C(x_0)(\\frac{x_t}{x_0})^{-b} $$\n",
    "with\n",
    "$$ LR = 1-2^{-b} $$\n",
    "\n",
    "The corresponding capacities have been extracted from the plot in the ISE study on p. 22 using the [WebPlotDigitizer](https://automeris.io/WebPlotDigitizer/) open source tool.\n",
    "\n",
    "Steps applied:\n",
    "* Read in worldwide wind and PV capacity installation projections\n",
    "* Read in the cost information for RES LCOE calculation\n",
    "* Calculate exponents for the learning curve cost development projection\n",
    "* Determine factors for costs by applying the learning curve formulat\n",
    "* Calculate capex and opex using the learning curve quotient\n",
    "* Derive LCOEs by combining time-dependent capex with opex and other parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_capacity_development = pd.read_csv(\n",
    "    main_path[\"inputs\"] + sub_path[\"renewables\"] + input_file[\"RES_market_development\"],\n",
    "    index_col=0, header=[0, 1], encoding=\"ISO 8859-1\", sep=\";\", decimal=\".\").drop(\n",
    "    columns=\"source\").T\n",
    "\n",
    "res_capacity_development.columns = res_capacity_development.columns.astype(str)\n",
    "res_capacity_development.index.names = [\"energy_carrier\", \"parameter\"]\n",
    "\n",
    "res_costs = pd.read_csv(\n",
    "    main_path[\"inputs\"] + sub_path[\"renewables\"] + input_file[\"RES_costs_ISE21\"],\n",
    "    index_col=None, header=0, encoding=\"ISO 8859-1\", sep=\";\").drop(\n",
    "    columns={\"unit\", \"source\"})\n",
    "\n",
    "res_costs = res_costs.fillna(method=\"ffill\")\n",
    "res_costs = res_costs.set_index([\"energy_carrier\", \"parameter\"])\n",
    "res_costs.rename(index={\"solarPV_open_area\": \"solarPV\"}, inplace=True)\n",
    "res_costs = res_costs.loc[res_costs.index.get_level_values(0).isin(RES_to_sep)]\n",
    "\n",
    "for ec in RES_to_sep:\n",
    "    res_costs.at[(ec, \"exponent\"), \"2021\"] = -np.log(1 - res_costs.at[(ec, \"LR\"), \"2021\"] / 100) / np.log(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_cost_data = res_capacity_development.append(res_costs).T\n",
    "res_cost_data = res_cost_data.fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
    "\n",
    "for ec in RES_to_sep:\n",
    "    res_cost_data[(ec, \"cost_factor_low\")] = (\n",
    "        (res_cost_data[(ec, \"high\")].div(res_cost_data.at[\"2021\", (ec, \"high\")])) \n",
    "         ** -res_cost_data[(ec, \"exponent\")])\n",
    "    res_cost_data[(ec, \"cost_factor_middle\")] = (\n",
    "        (res_cost_data[(ec, \"middle\")].div(res_cost_data.at[\"2021\", (ec, \"middle\")])) \n",
    "         ** -res_cost_data[(ec, \"exponent\")])\n",
    "    res_cost_data[(ec, \"cost_factor_high\")] = (\n",
    "        (res_cost_data[(ec, \"low\")].div(res_cost_data.at[\"2021\", (ec, \"low\")])) \n",
    "         ** -res_cost_data[(ec, \"exponent\")])\n",
    "    res_cost_data[(ec, \"capex_low\")] = (\n",
    "        res_cost_data[(ec, \"capex_low\")].mul(res_cost_data[(ec, \"cost_factor_low\")]))\n",
    "    res_cost_data[(ec, \"capex_middle\")] = (\n",
    "        res_cost_data.at[\"2021\", (ec, \"capex_low\")] * res_cost_data[(ec, \"cost_factor_middle\")])\n",
    "    res_cost_data[(ec, \"capex_high\")] = (\n",
    "        res_cost_data[(ec, \"capex_high\")].mul(res_cost_data[(ec, \"cost_factor_high\")]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LCOE_ISE = eeg.estimate_lcoe_ise21(res_cost_data, RES_to_sep)\n",
    "\n",
    "LCOE_windonshore_ISE = LCOE_ISE.loc[(LCOE_ISE.index >= annual_cap_expansion.at['windonshore', 'start_year']), \n",
    "                                    LCOE_ISE.columns.get_level_values(0) == \"windonshore\"].droplevel(0, axis=1)\n",
    "LCOE_windoffshore_ISE = LCOE_ISE.loc[(LCOE_ISE.index >= annual_cap_expansion.at['windoffshore', 'start_year']), \n",
    "                                    LCOE_ISE.columns.get_level_values(0) == \"windoffshore\"].droplevel(0, axis=1)\n",
    "LCOE_solarPV_ISE = LCOE_ISE.loc[(LCOE_ISE.index >= annual_cap_expansion.at['solarPV', 'start_year']), \n",
    "                                 LCOE_ISE.columns.get_level_values(0) == \"solarPV\"].droplevel(0, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine the split of the annual capacity to be installed for PV, i.e. the amount installed in tenders under the market premium scheme resp. the remainder installed under the FIT regime:\n",
    "* For 2021 and 2022, the capacities from the tender procedure are given.\n",
    "* For the remaining years, the volumes are taken from the [EEG 2021](http://www.gesetze-im-internet.de/eeg_2014/index.html#BJNR106610014BJNE020201123) for the years 2023 and 2024 and the [EEG 2023](https://dserver.bundestag.de/btd/20/016/2001630.pdf).\n",
    "    * In §28a EEG 2021 resp. in § 28a EEG 2023 and § 28b EEG 2023, the tender volumes for solarPV are given.\n",
    "    * These are separated into two segments. A second segments of larger rooftop and buildings PV will be eligible for market premium payments as well.\n",
    "    * For solarPV tenders, it is assumed that plants will be operational two years from the tender year. Hence, there is a shift by two years in the dict defining the tender capacities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annual_cap_expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capacity_split_solarPV = pd.DataFrame(index=pv_capacities_agg.index.union(LCOE_solarPV_ISE.index), \n",
    "                                      columns=['MP', 'FIT'])\n",
    "capacity_split_solarPV.loc[:,'FIT'] = annual_cap_expansion.at['solarPV', 'capacity'] * 1000\n",
    "\n",
    "EEG_2021_2023_tenders_dict = {\n",
    "    2023: 1850+300,\n",
    "    2024: 3600+2300,\n",
    "    2025: 5850+650,\n",
    "    2026: 8100+900,\n",
    "    2027: 9900+1100,\n",
    "    2028: 9900+1100,\n",
    "    2029: 9900+1100,\n",
    "    2030: 9900+1100\n",
    "}\n",
    "\n",
    "capacity_split_solarPV.loc[2023:2030, 'MP'] = pd.Series(EEG_2021_2023_tenders_dict).values * 1000\n",
    "capacity_split_solarPV.loc[2023:2030, 'FIT'] = capacity_split_solarPV.loc[2023:2030, 'FIT'].sub(\n",
    "    capacity_split_solarPV.loc[2023:2030, 'MP'])\n",
    "\n",
    "for iter_year in pv_capacities_agg.index.values:\n",
    "    capacity_split_solarPV.at[iter_year, 'FIT'] = pv_capacities_agg.at[iter_year, 'capacity'] * 1000\n",
    "    capacity_split_solarPV.at[iter_year, 'MP'] = solarPV_commissionings.at[iter_year, 'capacity_awarded']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the new-built plants to the data set:\n",
    "* Create a capacity distribution determining the (capacity) distribution for the LCOE estimates.\n",
    "* Add the plants groups to the DataFarme containing new-built RES units and their corresponding values applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onshore_cap_distribution = eeg.create_cap_distribution_from_beta(\n",
    "    LCOE_windonshore_ISE, cols=['LCOE_low', 'LCOE_middle', 'LCOE_high'])\n",
    "offshore_cap_distribution = eeg.create_cap_distribution_from_beta(\n",
    "    LCOE_windoffshore_ISE, cols=['LCOE_low', 'LCOE_middle', 'LCOE_high'])\n",
    "solarPV_cap_distribution = eeg.create_cap_distribution_from_beta(\n",
    "    LCOE_solarPV_ISE, cols=['LCOE_low', 'LCOE_middle', 'LCOE_high'])\n",
    "\n",
    "annual_cap_expansion[\"capacity\"] = annual_cap_expansion[\"capacity\"].mul(1000)  # Convert MW to kW\n",
    "\n",
    "new_eeg_pps_de_windonshore = eeg.add_new_built_res(onshore_cap_distribution, new_eeg_pps_de_windonshore, \n",
    "                                                    annual_cap_expansion, 'windonshore', indexed_by_years=False)\n",
    "new_eeg_pps_de_windoffshore = eeg.add_new_built_res(offshore_cap_distribution, new_eeg_pps_de_windoffshore, \n",
    "                                                    annual_cap_expansion, 'windoffshore', indexed_by_years=False)\n",
    "new_eeg_pps_de_solarPV = eeg.add_new_built_res(solarPV_cap_distribution, new_eeg_pps_de_solarPV, \n",
    "                                               capacity_split_solarPV, 'solarPV', capacity_col='MP',\n",
    "                                               indexed_by_years=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add solarPV units which are to be installed under the FIT scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iter_year in capacity_split_solarPV.index.values:\n",
    "    new_eeg_pps_de_solarPV.loc['solar_PV_FIT_'+str(iter_year),\n",
    "                                ['capacity', 'energy_source', \n",
    "                                'support_scheme', 'commissioning_year']] = (\n",
    "            [capacity_split_solarPV.at[iter_year, 'FIT'], 'solarPV', 'FIT', iter_year])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EEG transformers with negative costs (clustering)\n",
    "German EEG power plants within the market premium model are modeled as transformers to display negative power prices. (see [section above](#Extensions-for-Germany) for a description of the underlying logic)\n",
    "\n",
    "Steps applied here:\n",
    "- Create a common data set by appending the artificial new built units to the RES data set.\n",
    "- Drop new-plants that are to be commissioned after the target year.\n",
    "- Cluster RES power plants by market values and prepare them for usage in _POMMES_ (see function `create_ee_transformers` for details)\n",
    "- Add RES buses needed for the modelling approach\n",
    "- Set RES operation costs as negative value of average market premium of cluster\n",
    "- Write RES data to a csv and to and Excel file\n",
    "- Write the costs to a separate file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_source_dict = dict(wind_onshore='windonshore',\n",
    "                          wind_offshore='windoffshore',\n",
    "                          solar='solarPV')\n",
    "eeg_pps_de[\"energy_source\"].replace(energy_source_dict, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all data sets\n",
    "eeg_pps_de = pd.concat([eeg_pps_de, new_eeg_pps_de_solarPV,\n",
    "                        new_eeg_pps_de_windonshore, new_eeg_pps_de_windoffshore])\n",
    "\n",
    "# Drop plants that are commissioned prior to the target year\n",
    "eeg_pps_de[\"commissioning_year\"] = eeg_pps_de[\"commissioning_year\"].astype(\"int32\")\n",
    "eeg_pps_de = eeg_pps_de.loc[eeg_pps_de[\"commissioning_year\"] <= year]\n",
    "\n",
    "ee_agg = eeg.create_ee_transformers(\n",
    "    eeg_pps_de, \n",
    "    cluster_no=eeg_clusters_per_technology,\n",
    "    value_exogenous=value_exogenous,\n",
    ")\n",
    "ee_agg['country'] = 'DE'\n",
    "ee_agg['to_el'] = 'DE_bus_el'\n",
    "ee_agg['efficiency_el'] = 1\n",
    "buses.extend(ee_agg['from'].unique())\n",
    "\n",
    "# seperate costs and nodes\n",
    "costs_operation_RES = -ee_agg['value_applied'].to_frame().rename(columns={'value_applied': 'costs'})\n",
    "\n",
    "ee_agg.to_csv(\n",
    "    f\"{main_path['outputs']}{output_file['transformers_renewables']}_\"\n",
    "    f\"{eeg_clusters_per_technology}_clusters_{year}.csv\"\n",
    ")\n",
    "ee_agg.to_excel(\n",
    "    writer, \n",
    "    sheet_name=f\"transformers_renewables_{eeg_clusters_per_technology}_clusters_{year}\"\n",
    ")\n",
    "costs_operation_RES.to_csv(\n",
    "    f\"{main_path['outputs']}{output_file['costs_operation_renewables']}_\"\n",
    "    f\"{eeg_clusters_per_technology}_clusters_{year}.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "installed_vres_capacities = (\n",
    "    eeg_pps_de.loc[eeg_pps_de[\"energy_source\"].isin(\n",
    "        [\"solarPV\", \"windonshore\", \"windoffshore\"])].groupby(\"energy_source\").agg({\"capacity\": \n",
    "                                                                                   \"sum\"}).div(1000).round(1)\n",
    ")\n",
    "\n",
    "installed_vres_capacities.rename(dict_eeg_pps_names, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do some garbage collection\n",
    "try:\n",
    "    del (DK_cap, DK_val_2030, EEG_2021_2023_tenders_dict, LCOE_ISE, LCOE_solarPV_ISE, LCOE_windoffshore_ISE,\n",
    "         LCOE_windonshore_ISE, RES_to_sep, PV_cap_distribution,\n",
    "         all_sources_res, annual_cap_expansion,\n",
    "         capacity_split_solarPV, cap_needed_until_2030, col, common_tenders,\n",
    "         costs_operation_RES, dict_eeg_pps_names, dict_tender_cols,\n",
    "         diff_ix, ec, ee_agg, eeg_clusters_per_technology, eeg_de, eeg_pps_de,\n",
    "         eeg_pps_de_agg, eeg_pps_de_agg_by_year, eeg_pps_de_solarPV,\n",
    "         eeg_pps_de_windoffshore, eeg_pps_de_windonshore, energy_source_dict,\n",
    "         iter_year, k, keep_constant_ix, missing_res_ts_dict, new_eeg_pps_de_solarPV,\n",
    "         new_eeg_pps_de_windoffshore, new_eeg_pps_de_windonshore, \n",
    "         onshore_cap_distribution, offshore_cap_distribution,\n",
    "         onshore_tenders, pv_cap_dict, pv_cap_names_dict, pv_capacities,\n",
    "         pv_capacities_agg, pv_col_names, renewables_eu_2025_BEST, renewables_eu_2030_DG,\n",
    "         renewables_eu_abs_capacities, renewables_eu_capacities, res_DE_EEG_2021,\n",
    "         res_DE_Prognos, res_de_projection, res_capacities, res_capacity_development,\n",
    "         res_capacity_projection, res_capacity_projections, res_costs, res_de_capacity_development,\n",
    "         res_cost_data, solarPV_cap_distribution, solarPV_commissionings, \n",
    "         solarPV_tenders, solarPV_tenders2, sources_RES_cap, sources_RES_original, sources_hydro,\n",
    "         sources_nonfluc, sources_res, sources_ts_res, ts_res, usecols, v, value,\n",
    "         windoffshore_commissionings, windonshore_commissionings, years_for_iteration)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers and commodity sources for the target year\n",
    "\n",
    "## Add new-built power plants by assumption\n",
    "**Background and approach**:\n",
    "\n",
    "* A capacity estimation for 2030 showed that large amounts of capacities are about to be decommissioned whereas there is a limited number of plans for new-built power plants and the estimate of the TSOs from the NEP 2018 is rather conservative (i. e. not considering a coal phase-out and assuming fewer plants to go offline).\n",
    "* In order not to provoke situations with a loss of load (LOL), the idea is to introduce backup power plants for levelling out the system balance for future target years.\n",
    "* The scheme of the capacity balance (sheet) is used here. The latest TSO estimations covering 2018-2022 are used as a rough guideline. The scenario with a coal phase out is used. It shows that for 2021 with around 72 GW of installed capacity not including network reserve and security backup (of coal power plants), the remaining margin of 2 GW is small but sufficient. Hence, it is assumed that installed capacity has to add up to 72 GW in order to meet the peak demand.\n",
    "* Accordignly, the difference between the overall capacity given here for 2030 (roughly 57 GW + 80% of phes and 28% of ROR capacities, 60% of biomass capacities and 1% of wind capacities) and the needed 72 GW is filled up with gas power plants.\n",
    "* The lacking capacity is evenly distributed between gas turbines and CCGT power plants.\n",
    "\n",
    "ÜNB (2020): Bericht zur Leistungsbilanz, https://www.netztransparenz.de/portals/1/Bericht_zur_Leistungsbilanz_2019.pdf, accessed 14.11.2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap_requirement = 72000\n",
    "\n",
    "# Add all capacity credits to calculate secured capcity\n",
    "secured_cap_target_year = remaining_capacity_conv + conv_de_new.capacity.sum()\n",
    "secured_cap_target_year += 0.28 * ror_de_agg.at['DE_source_ROR', 'capacity'] + 0.8 * phes_de.capacity.sum()\n",
    "secured_cap_target_year += 0.6 * sources_RES.at['DE_source_biomassEEG', 'capacity']\n",
    "secured_cap_target_year += 0.01 * installed_vres_capacities.loc[['windonshore', 'windoffshore'], 'capacity'].sum()\n",
    "\n",
    "missing_cap_target_year = np.max([cap_requirement - secured_cap_target_year, 0])\n",
    "print(f'Capacity to be newly provided by additional gas power plants in {year}: {missing_cap_target_year:,.2f} MW')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers_new_built = pd.read_csv(\n",
    "    main_path[\"inputs\"] + sub_path[\"pp_er\"] + input_file[\"new_built\"], \n",
    "    sep=\";\", decimal=\",\", index_col=0\n",
    ")\n",
    "\n",
    "transformers_new_built[\"capacity\"] = missing_cap_target_year/2\n",
    "transformers_new_built[\"tech_fuel\"] = (\n",
    "    transformers_new_built[\"technology\"] + \"_\" + transformers_new_built[\"fuel\"]\n",
    ")\n",
    "\n",
    "if transformers_new_built.capacity.sum() == 0:\n",
    "    pass\n",
    "else:\n",
    "    conv = conv.append(transformers_new_built)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster transformers (optionally)\n",
    "> _NOTE: This section has been added to create a reduced transformers data for Germany set for quicker, but less precise simulation runs._\n",
    "\n",
    "Basic steps (only if cluster control variable is True):\n",
    "* Slice subset for German power plants\n",
    "* Cluster by electrical efficiency\n",
    "* Group data based on defined rules\n",
    "* Assign a new index to clustered data\n",
    "* Remove detailled data from transformer input sheet and append clustered one instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cluster_transformers_DE:\n",
    "    conv_de = conv.loc[conv['country'] == \"DE\"]\n",
    "    conv_de_clustered = tf_agg.cluster_transformers(\n",
    "        conv_de, \n",
    "        by='efficiency_el',\n",
    "        grouping=\"fuel\",\n",
    "        share_clusters=0.1\n",
    "    )\n",
    "    \n",
    "    grouping_cols = ['cluster']\n",
    "    mean_cols = ['efficiency_el', 'max_load_factor', 'min_load_factor', 'part_load_losses',\n",
    "                 'min_uptime', 'min_downtime', 'cold_startup_time', 'warm_startup_time',\n",
    "                 'hot_startup_time', 'cold_startup_costs', 'warm_startup_costs', \n",
    "                 'hot_startup_costs', 'grad_pos', 'grad_neg']\n",
    "    sum_cols = ['capacity', 'outages']\n",
    "    cols_considered = grouping_cols + mean_cols + sum_cols\n",
    "\n",
    "    other_cols = [\n",
    "        col for col in conv.columns if col not in cols_considered\n",
    "    ]\n",
    "\n",
    "    conv_de_clustered = tf_agg.group_power_plants(\n",
    "        conv_de_clustered, \n",
    "        grouping_cols, \n",
    "        mean_cols, \n",
    "        sum_cols, \n",
    "        other_cols)\n",
    "\n",
    "    conv_de_clustered['label'] = (\n",
    "        'transformer_' \n",
    "        + conv_de_clustered['fuel']\n",
    "        + '_' + conv_de_clustered['type']\n",
    "        + '_cluster_'\n",
    "        + conv_de_clustered['cluster'].apply(str)\n",
    "    )\n",
    "    conv_de_clustered.set_index('label', inplace=True)\n",
    "    \n",
    "    conv_clustered = conv.drop(conv_de.index)\n",
    "    conv_clustered = conv_clustered.append(conv_de_clustered[conv.columns])\n",
    "    conv_clustered['identifier'] = np.nan\n",
    "    \n",
    "    conv_clustered = conv_clustered.round(rounding_precision)\n",
    "    conv_clustered.to_csv(\n",
    "        main_path[\"outputs\"] + 'transformers_clustered' + \"_\" + str(year) + \".csv\"\n",
    "    )\n",
    "    conv_clustered.to_excel(writer, sheet_name='conv_clustered' + \"_\" + str(year))\n",
    "    \n",
    "    #del (cols_considered, conv_clustered, conv_de_clustered,\n",
    "    #     grouping_cols, mean_cols, other_cols, sum_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write power plants data to csv and Excel\n",
    "\n",
    "Steps applied:\n",
    "* Write data to csv\n",
    "* Write data to Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = conv.round(rounding_precision)\n",
    "conv.to_csv(\n",
    "    main_path[\"outputs\"] + output_file[\"transformers\"] + \"_\" + str(year) + \".csv\"\n",
    ")\n",
    "conv.to_excel(writer, sheet_name='conv' + \"_\" + str(year))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commodity sources\n",
    "\n",
    "Steps applied:\n",
    "- Read in emission factors data\n",
    "- Extract necessary information from conventional power plants data set, i.e. country, fuel and information on commodity bus (commodity source feeds into that bus; transformer receives energy from that bus)\n",
    "- Add emission factor information and rename\n",
    "- Store the results to csv and to Excel\n",
    "- Add commodity bus information on the buses data set\n",
    "- Set costs for carbon (hard-coded) and write that cost data\n",
    "\n",
    "Data sources used:\n",
    "- Emission factors: UBA (2019). Entwicklung der spezifischen Kohlendioxid-Emissionen des deutschen Strommix in den Jahren 1990 - 2018, Dessau-Roßlau, pp. 16, 28\n",
    "- Natural gas: BAFA (2019). Aufkommen und Export von Erdgas sowie die Entwicklung der Grenzübergangspreise ab 1991"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emission_factors = pd.read_csv(\n",
    "    main_path[\"inputs\"] + sub_path[\"assumptions\"] + input_file[\"emf\"],\n",
    "    sep=\";\", decimal=\",\"\n",
    ")\n",
    "\n",
    "sources_commodity = conv.reset_index()[['country', 'fuel', 'from']].drop_duplicates(subset='from')\n",
    "sources_commodity['label'] = sources_commodity['country'] + '_source_' + sources_commodity['fuel']\n",
    "sources_commodity = sources_commodity.merge(emission_factors, on='fuel').drop(columns='fuel').set_index('label')\n",
    "sources_commodity.rename(columns={'from': 'to'}, inplace=True)\n",
    "\n",
    "sources_commodity.to_csv(\n",
    "    main_path[\"outputs\"] + output_file[\"sources_commodity\"] + \"_\" + str(year) + \".csv\"\n",
    ")\n",
    "sources_commodity.to_excel(writer, sheet_name='sources_commodity' + \"_\" + str(year))\n",
    "\n",
    "# Separate all buses; needed for investment modelling (decommissioning)\n",
    "all_buses = buses.copy()\n",
    "buses.extend(sources_commodity['to'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All commodity sources; needed for investment model\n",
    "all_sources_commodity = pd.concat([\n",
    "    all_conv_eu_sources, \n",
    "    all_conv_de.reset_index()[['country', 'fuel', 'from']].drop_duplicates(subset='from'),\n",
    "    all_conv_de_new.reset_index()[['country', 'fuel', 'from']].drop_duplicates(subset='from')\n",
    "]).drop_duplicates()\n",
    "all_sources_commodity['label'] = all_sources_commodity['country'] + '_source_' + all_sources_commodity['fuel']\n",
    "\n",
    "all_sources_commodity = all_sources_commodity.merge(\n",
    "    emission_factors, on='fuel'\n",
    ").drop(columns='fuel').set_index('label')\n",
    "all_sources_commodity.rename(columns={'from': 'to'}, inplace=True)\n",
    "\n",
    "all_sources_commodity.to_csv(\n",
    "    main_path[\"outputs\"] + output_file[\"sources_commodity\"] + \".csv\"\n",
    ")\n",
    "all_sources_commodity.to_excel(writer, sheet_name='sources_commodity')\n",
    "\n",
    "all_buses.extend(all_sources_commodity['to'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers for usage in the investment model\n",
    "* In the investment model, transformers are stronger aggregated by default. Thus, the capacity development over time needs to be tracked.\n",
    "* Furthermore, the only distinction made is the one between exogenous and endogenous transformers.\n",
    "\n",
    "## Prepare transformers data for usage in the investment model\n",
    "Steps applied:\n",
    "* Combine existing and new-built transformer units\n",
    "* Cluster by efficiency\n",
    "* Calculate the capacity that exists in a particular year and introduce columns to store that capacity.\n",
    "* Normalize through a division by the maximum capacity of a cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine existing and (exogenously) new-built units\n",
    "all_conv_de = pd.concat([all_conv_de, all_conv_de_new])\n",
    "\n",
    "# Assign cluster information\n",
    "all_conv_de_clustered = tf_agg.cluster_transformers(\n",
    "    all_conv_de, \n",
    "    by=\"efficiency_el\",\n",
    "    grouping=\"tech_fuel\",\n",
    "    share_clusters=0.1\n",
    ")\n",
    "\n",
    "# Create capacity values for each year in order to derive cluster capacities\n",
    "capacity_cols = []\n",
    "for iter_year in range(2020, 2051):\n",
    "    capacity_col = f\"{iter_year}-01-01\"\n",
    "    capacity_cols.append(capacity_col)\n",
    "    conditions = [\n",
    "        all_conv_de_clustered[\"commissioned_last\"] > iter_year,\n",
    "        all_conv_de_clustered[\"shutdown\"] < iter_year\n",
    "    ]\n",
    "    choices = [0, 0]\n",
    "    all_conv_de_clustered[capacity_col] = np.select(\n",
    "        conditions, choices, default=all_conv_de_clustered[\"capacity\"]\n",
    "    )\n",
    "    \n",
    "# Write unclustered data\n",
    "all_conv_de_clustered.to_csv(\n",
    "    f\"{main_path['outputs']}{output_file['transformers_exogenous']}_unclustered.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouping_cols = ['cluster']\n",
    "mean_cols = ['efficiency_el', 'max_load_factor', 'min_load_factor', 'part_load_losses',\n",
    "             'min_uptime', 'min_downtime', 'cold_startup_time', 'warm_startup_time',\n",
    "             'hot_startup_time', 'cold_startup_costs', 'warm_startup_costs', \n",
    "             'hot_startup_costs', 'grad_pos', 'grad_neg']\n",
    "sum_cols = ['capacity', 'outages']\n",
    "sum_cols.extend(capacity_cols)\n",
    "\n",
    "cols_considered = grouping_cols + mean_cols + sum_cols\n",
    "\n",
    "other_cols = [\"fuel\", \"type\", \"tech_fuel\", \"from\", \"to_el\"]\n",
    "\n",
    "all_conv_de_clustered = tf_agg.group_power_plants(\n",
    "    all_conv_de_clustered, \n",
    "    grouping_cols, \n",
    "    mean_cols, \n",
    "    sum_cols, \n",
    "    other_cols\n",
    ")\n",
    "\n",
    "all_conv_de_clustered['label'] = (\n",
    "    'DE_transformer_' \n",
    "    + all_conv_de_clustered['fuel'] \n",
    "    + '_' + all_conv_de_clustered['type']\n",
    "    + '_cluster_' \n",
    "    + all_conv_de_clustered['cluster'].apply(str)\n",
    ")\n",
    "all_conv_de_clustered.set_index('label', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define maximum capacity and normalize values (Germany)\n",
    "all_conv_de_clustered[\"capacity_max\"] = all_conv_de_clustered[capacity_cols].max(axis=1)\n",
    "for capacity_col in capacity_cols:\n",
    "    all_conv_de_clustered[capacity_col] = all_conv_de_clustered[capacity_col].div(\n",
    "        all_conv_de_clustered[\"capacity_max\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define maximum capacity and normalize values (Europe)\n",
    "\n",
    "if not use_tyndp_2018:\n",
    "    # Drop empty series before normalizing\n",
    "    conv_eu_investment.drop(\n",
    "        index=conv_eu_investment.loc[conv_eu_investment[capacity_cols].sum(axis=1) == 0].index,\n",
    "        inplace=True\n",
    "    )\n",
    "\n",
    "    conv_eu_investment[\"capacity_max\"] = conv_eu_investment[capacity_cols].max(axis=1)\n",
    "    for capacity_col in capacity_cols:\n",
    "        conv_eu_investment[capacity_col] = conv_eu_investment[capacity_col].div(\n",
    "            conv_eu_investment[\"capacity_max\"]\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create maximum output timeseries to account for units commissioning / decommissioning within a cluster\n",
    "Logic:\n",
    "* If plant is decommissioned, maximum capacity of the respective cluster decreases.\n",
    "* If plant is commissioned, maximum capacity of the respective cluster increaeses.\n",
    "\n",
    "Steps applied:\n",
    "* Extract the capacity columns from clustered data set above and transform them to obtain a time series indexed by years.\n",
    "* Storage that time series as well as the clustered data including the maximum capacity information to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_ts = all_conv_de_clustered[capacity_cols].copy().T\n",
    "\n",
    "# Append European units\n",
    "if not use_tyndp_2018:\n",
    "    max_ts_europe = conv_eu_investment[capacity_cols].copy().T\n",
    "    max_ts = pd.concat([max_ts, max_ts_europe], axis=1)\n",
    "\n",
    "max_ts.to_csv(\n",
    "    main_path[\"outputs\"] + output_file[\"transformers_exogenous_max_ts\"] + \".csv\" \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_conv_de_clustered.drop(columns=capacity_cols, inplace=True)\n",
    "all_conv_de_clustered[\"country\"] = \"DE\"\n",
    "\n",
    "# Append European units\n",
    "if not use_tyndp_2018:\n",
    "    conv_eu_investment.drop(columns=capacity_cols, inplace=True)\n",
    "    transformers_exogenous = pd.concat([all_conv_de_clustered, conv_eu_investment])\n",
    "else:\n",
    "    transformers_exogenous = all_conv_de_clustered.copy()\n",
    "\n",
    "transformers_exogenous.to_csv(\n",
    "    main_path[\"outputs\"] + output_file[\"transformers_exogenous\"] + \".csv\" \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do some garbage collection\n",
    "try:\n",
    "    del (cap_requirement, cluster_transformers_DE,\n",
    "         conv, conv_de, conv_de_new, emission_factors, \n",
    "         installed_vres_capacities, missing_cap_target_year, \n",
    "         phes_de, remaining_capacity_conv, ror_de_agg, secured_cap_target_year,\n",
    "         sources_RES, transformers_new_built)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduce and parameterize investment options\n",
    "Steps applied:\n",
    "* Read investment options from .csv including parameter assumptions compiled by Julien Faist based on various primary data sources.\n",
    "* Write to folder with prepared data.\n",
    "\n",
    "__*TODO: Adjust taking into account further data sets from investment expenses section below.*__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_investment_options = pd.read_csv(\n",
    "    main_path[\"inputs\"] + sub_path[\"pp_er\"] + input_file[\"transformers_investment_options\"], \n",
    "    index_col=0, sep=\";\", decimal=\",\"\n",
    ")\n",
    "\n",
    "transformer_investment_options[\"tech_fuel\"] = (\n",
    "    transformer_investment_options[\"technology\"] + \"_\" + transformer_investment_options[\"fuel\"]\n",
    ")\n",
    "\n",
    "transformer_investment_options.to_csv(\n",
    "    main_path[\"outputs\"] + output_file[\"transformers_investment_options\"] + \".csv\", \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constrain hydrogen investments\n",
    "* Investments in hydrogen generation and electrolysis are phased in starting from 2023.\n",
    "* The maximum installable capacity is assumed to gradually increase until 2030 in the course of that phase-in.\n",
    "* The maxima are applied to all different hydrogen technologies separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hydrogen_investment_maxima_dict = {\n",
    "    2020: 1e-3,\n",
    "    2022: 1e-3,\n",
    "    2023: 500,\n",
    "    2030: 5000\n",
    "}\n",
    "hydrogen_investment_maxima = pd.Series(\n",
    "    hydrogen_investment_maxima_dict, index=range(2020, 2051)\n",
    ").interpolate(how=\"linear\")\n",
    "hydrogen_investment_maxima.to_csv(\n",
    "    main_path[\"outputs\"] + output_file[\"hydrogen_investment_maxima\"] + \".csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sinks\n",
    "\n",
    "In this section, **demand** data is put together.<br>\n",
    "In [oemof.solph](https://github.com/oemof/oemof-solph) demand can be represented through (generic) so called \"Sink\" elements which have one input and no output. In addition to the demand sinks per bidding zone, excess sinks are created to model an overall generation surplus which cannot be consumed or stored within the area considered. In reality this would be exports to other European countries or result in curtailments. The excess sinks are needed for ensuring model feasibility.\n",
    "\n",
    "## Electricity Demand\n",
    "\n",
    "Currently the time series for electricity demand is created in the RES section, since the time series are loaded there.\n",
    "\n",
    "Steps applied:\n",
    "- Add bus information and label for demand\n",
    "- Normalize demand time series by determining peak demand and dividing the demand timeseries through that value\n",
    "- Write demand values (i.e. maximum demands) and demand time series to csv and to Excel files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sinks_demand_el = pd.DataFrame(data={'country': countries_modeled})\n",
    "sinks_demand_el['from'] = sinks_demand_el['country'] + '_bus_el'\n",
    "sinks_demand_el['label'] = sinks_demand_el['country'] + '_sink_el_load'\n",
    "sinks_demand_el.set_index('label', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_eu_el_demand = ts_eu.filter(regex = 'sink_el')\n",
    "peak_demand = ts_eu_el_demand.max()\n",
    "sinks_demand_el.loc[peak_demand.index, 'maximum'] = peak_demand\n",
    "\n",
    "sinks_demand_el_ts = ts_eu_el_demand/peak_demand\n",
    "\n",
    "sinks_demand_el['maximum'] = sinks_demand_el['maximum'].astype(int)\n",
    "sinks_demand_el_ts = sinks_demand_el_ts.round(rounding_precision).astype(np.float32)\n",
    "\n",
    "# interpolate NaN values (Czech and France)\n",
    "sinks_demand_el_ts = sinks_demand_el_ts.interpolate(method='linear')\n",
    "sinks_demand_el_ts_2017 = sinks_demand_el_ts.copy()\n",
    "sinks_demand_el_ts = tools.reindex_time_series(sinks_demand_el_ts, year)\n",
    "\n",
    "sinks_demand_el.to_csv(\n",
    "    main_path[\"outputs\"] + output_file[\"sinks_demand_el\"] + \"_\" + str(year) + \".csv\"\n",
    ")\n",
    "sinks_demand_el.to_csv(\n",
    "    main_path[\"outputs\"] + output_file[\"sinks_demand_el\"] + \".csv\"\n",
    ")\n",
    "\n",
    "# Include sensitivities for inflexible demand\n",
    "for sensitivity, multiplier in sensitivities.items():\n",
    "    sinks_demand_el_sensitivity = sinks_demand_el.copy()\n",
    "    sinks_demand_el_sensitivity.at[\"DE_sink_el_load\", \"maximum\"] *= multiplier\n",
    "    sinks_demand_el_sensitivity.to_csv(\n",
    "         f'{main_path[\"outputs\"]}{output_file[\"sinks_demand_el\"]}'\n",
    "         f'_sensitivity_{sensitivity}.csv'\n",
    "    )\n",
    "\n",
    "sinks_demand_el_ts.to_csv(\n",
    "    main_path[\"outputs\"] + output_file[\"sinks_demand_el_ts\"] + \"_\" + str(year) + \".csv\"\n",
    ")\n",
    "sinks_demand_el.to_excel(writer, sheet_name='sinks_demand_el' + \"_\" + str(year))\n",
    "sinks_demand_el_ts.tz_localize(None).to_excel(writer, sheet_name='sinks_demand_el_ts' + \"_\" + str(year))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consider load of demand response applications\n",
    "In _POMMES_, demand response may be modelled separately  using a decidated demand response representation of oemof.solph (see: [this module](https://github.com/oemof/oemof-solph/blob/dev/src/oemof/solph/components/experimental/_sink_dsm.py)). Hereby, demand response units have a dedicated baseline load (i.e. the load if no load shifting or shedding is happening). This needs to be subtracted from the overall load.\n",
    "\n",
    "> _Note:_\n",
    "> * _There are three cases for the potential estimate, a pessimistic, a neutral and an optimistic one, leading to three distinct time series (they differ in scale, the profile applied is the same)._\n",
    "> * _This leads to three different demand time series when considering demand response._\n",
    "\n",
    "Steps applied:\n",
    "* Calculate overall load profile for Germany based on peak load and actual profile.\n",
    "* Read in demand response load profiles (which have been pre-compiled by other data preparation scripts)\n",
    "* Read demand response parameter data and interpolate for the years in between the five year steps.\n",
    "* Write all demand response information to file again in order to have a consolidated full data set.\n",
    "* Calculate overall baseline load profile for demand response units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate overall load profile for Germany\n",
    "german_demand = sinks_demand_el_ts_2017[\"DE_sink_el_load\"] * sinks_demand_el.loc['DE_sink_el_load', 'maximum']\n",
    "\n",
    "demand_response_clusters = {\n",
    "    \"tcs+hoho_cluster_shift_only\": {\"shifting\": 1, \"shedding\": 0}, \n",
    "    \"hoho_cluster_shift_shed\": {\"shifting\": 1, \"shedding\": 1},\n",
    "    \"hoho_cluster_shift_only\": {\"shifting\": 1, \"shedding\": 0},\n",
    "    \"tcs_cluster_shift_only\": {\"shifting\": 1, \"shedding\": 0},\n",
    "    \"ind_cluster_shed_only\": {\"shifting\": 0, \"shedding\": 1},\n",
    "    \"ind_cluster_shift_shed\": {\"shifting\": 1, \"shedding\": 1},\n",
    "    \"ind_cluster_shift_only\": {\"shifting\": 1, \"shedding\": 0},\n",
    "}\n",
    "demand_response_clusters_eligibility = pd.DataFrame(data=demand_response_clusters).T\n",
    "demand_response_clusters_eligibility.to_csv(\n",
    "    f'{main_path[\"outputs\"]}{output_file[\"demand_response_clusters_eligibility\"]}.csv'\n",
    ")\n",
    "opex_columns = [\"variable_costs\", \"variable_costs_shed\"]\n",
    "capex_columns = [\"fixed_costs\", \"specific_investments\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_profile = {}\n",
    "demand_response_parameters = {}\n",
    "demand_response_max_capacities = {}\n",
    "absolute_baseline_profiles = {}\n",
    "\n",
    "for estimate in [\"5\", \"50\", \"95\"]:\n",
    "    demand_response_parameters[estimate] = {}\n",
    "    \n",
    "    # Extract baseline profile and write other demand response information to file\n",
    "    baseline_profile[estimate] = pd.read_csv(\n",
    "        f'{main_path[\"inputs\"]}{sub_path[\"demand\"]}{input_file[\"demand_response_baseline_profile\"]}{estimate}.csv', \n",
    "        index_col=0\n",
    "    )\n",
    "    baseline_profile[estimate].index = pd.to_datetime(baseline_profile[estimate].index)\n",
    "    \n",
    "    # Repeat demand response baseline profile for every year of modeling horizon\n",
    "    to_concat = []\n",
    "    for iter_year in range(2020, 2051):\n",
    "        baseline_profile_2017 = baseline_profile[estimate].copy()\n",
    "        to_concat.append(tools.reindex_time_series(baseline_profile_2017, iter_year))  \n",
    "    baseline_profile[estimate] = pd.concat(to_concat)\n",
    "    baseline_profile[estimate].to_csv(\n",
    "        f'{main_path[\"outputs\"]}{output_file[\"demand_response_baseline_profile\"]}{estimate}.csv'\n",
    "    )\n",
    "    # Dispatch time series\n",
    "    baseline_profile_dispatch = tools.reindex_time_series(baseline_profile_2017, year)\n",
    "    baseline_profile_dispatch.to_csv(\n",
    "        f'{main_path[\"outputs\"]}{output_file[\"demand_response_baseline_profile\"]}{estimate}_{year}.csv'\n",
    "    )\n",
    "    \n",
    "    # Extract maximum capacity and prepare demand response data for usage in POMMES\n",
    "    to_concat = []\n",
    "    for cluster in demand_response_clusters:\n",
    "        dr_parameter_data = pd.read_csv(\n",
    "            f'{main_path[\"inputs\"]}{sub_path[\"demand\"]}{cluster}'\n",
    "            + f'{input_file[\"demand_response_parameters\"]}nominal_{estimate}%.csv',\n",
    "            index_col=[0, 1]\n",
    "        )\n",
    "\n",
    "        dr_parameter_data = (\n",
    "            dr_parameter_data.reset_index(drop=True).set_index(\"year\")\n",
    "        ).drop(index=\"SQ\")\n",
    "        dr_parameter_data.index = dr_parameter_data.index.astype(int)\n",
    "        \n",
    "        dr_parameter_data_interpolated = pd.DataFrame(\n",
    "            index=range(2020, 2051), \n",
    "            columns=dr_parameter_data.columns,\n",
    "            dtype=\"float64\"\n",
    "        )\n",
    "        for index in dr_parameter_data.index:\n",
    "            dr_parameter_data_interpolated.loc[index] = dr_parameter_data.loc[index]\n",
    "        dr_parameter_data_interpolated = dr_parameter_data_interpolated.interpolate()\n",
    "        dr_parameter_data_interpolated[\"country\"] = dr_parameter_data_interpolated.at[2020, \"country\"]\n",
    "        dr_parameter_data_interpolated[\"from\"] = dr_parameter_data_interpolated.at[2020, \"from\"]\n",
    "        \n",
    "        # Add cluster information before concatenating max capacity information\n",
    "        dr_data_to_concat = dr_parameter_data_interpolated[[\"max_cap\"]].copy()\n",
    "        dr_data_to_concat[\"cluster\"] = cluster\n",
    "        dr_data_to_concat.set_index(\"cluster\", append=True, inplace=True)\n",
    "        to_concat.append(dr_data_to_concat)\n",
    "        \n",
    "        # Split OPEX and CAPEX costs data and write to separate files\n",
    "        dr_opex_costs_data = dr_parameter_data_interpolated[opex_columns]\n",
    "        dr_capex_costs_data = dr_parameter_data_interpolated[capex_columns]\n",
    "        \n",
    "        # Extend fixed costs data to be able to simulate \"the long-run\"\n",
    "        add_fixed_costs = pd.DataFrame(index=range(2051, 2081), columns=dr_capex_costs_data.columns)\n",
    "        for iter_year in range(2051, 2081):\n",
    "            add_fixed_costs.loc[iter_year, \"fixed_costs\"] = (\n",
    "                dr_capex_costs_data.at[2050, \"fixed_costs\"] * inflation_rate ** (iter_year - 2050)\n",
    "            )\n",
    "        dr_capex_costs_data = pd.concat([dr_capex_costs_data, add_fixed_costs])\n",
    "        \n",
    "        dr_opex_costs_data.index = pd.date_range(start=\"2020-01-01\", end=\"2050-01-01\", freq=\"AS\")\n",
    "        dr_capex_costs_data.index = pd.date_range(start=\"2020-01-01\", end=\"2080-01-01\", freq=\"AS\")\n",
    "        \n",
    "        dr_parameter_data_interpolated.drop(columns=opex_columns + capex_columns, inplace=True)\n",
    "        dr_parameter_data_interpolated.to_csv(\n",
    "            f'{main_path[\"outputs\"]}{cluster}'\n",
    "            + f'{output_file[\"demand_response_parameters\"]}{estimate}%.csv'\n",
    "        )\n",
    "        demand_response_parameters[estimate][cluster] = dr_parameter_data_interpolated\n",
    "        \n",
    "        dr_opex_costs_data.to_csv(\n",
    "            f'{main_path[\"outputs\"]}{cluster}'\n",
    "            + f'{output_file[\"demand_response_variable_costs_data\"]}{estimate}%.csv'\n",
    "        )\n",
    "        # Convert to correct unit: €/MW!\n",
    "        dr_capex_costs_data = dr_capex_costs_data.mul(1000)\n",
    "        dr_capex_costs_data.to_csv(\n",
    "            f'{main_path[\"outputs\"]}{cluster}'\n",
    "            + f'{output_file[\"demand_response_fixed_costs_and_investments_data\"]}{estimate}%.csv'\n",
    "        )\n",
    "        \n",
    "    demand_response_max_capacities[estimate] = pd.concat(to_concat)\n",
    "    demand_response_max_capacities[estimate] = demand_response_max_capacities[estimate].reset_index()\n",
    "    demand_response_max_capacities[estimate] = demand_response_max_capacities[estimate].pivot(\n",
    "        index=\"level_0\", columns=\"cluster\", values=\"max_cap\"\n",
    "    )\n",
    "    \n",
    "    # Create absolute demand response baseline profiles\n",
    "    to_concat = []\n",
    "    for iter_year in range(2020, 2051):\n",
    "        to_concat.append(\n",
    "            baseline_profile[estimate].loc[baseline_profile[estimate].index.year == iter_year]\n",
    "            * demand_response_max_capacities[estimate].loc[iter_year]\n",
    "        )\n",
    "    absolute_baseline_profiles[estimate] = pd.concat(to_concat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjust availability profiles: Availability profile needs to be scaled such that\n",
    "* the maximum value for the larger one of positive (downshift) and negative (upshift) potential is one.\n",
    "* the lower one is scaled down accordingly.\n",
    "* The average ratio is used as a proxy. In principle, it may change on an annual basis due to the inhomogenous structure of the clusters. But this brings along the problem of having the need for a dedicated availability time series for each year which would blow up model complexity.\n",
    "* Instead, the average ratio over the different years is considered and used to adjust.\n",
    "    \n",
    "Steps applied:\n",
    "* Read in availability profiles.\n",
    "* Use potential data and evaluate the average relation between positive and negative shifting potential.\n",
    "* In case only shedding is feasible, no scaling is nessecary.\n",
    "* Update either positive or negative availability time series and write the result to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for estimate in [\"5\", \"50\", \"95\"]:\n",
    "    dr_param_dict = demand_response_parameters[estimate]\n",
    "    \n",
    "    # Availability profiles are read and repeated for every year of modeling horizon\n",
    "    ava_pos = pd.read_csv(\n",
    "        f'{main_path[\"inputs\"]}{sub_path[\"demand\"]}{input_file[\"demand_response_ava_pos\"]}{estimate}.csv', \n",
    "        index_col=0\n",
    "    )\n",
    "    ava_pos.index = pd.to_datetime(ava_pos.index)\n",
    "    \n",
    "    ava_neg = pd.read_csv(\n",
    "        f'{main_path[\"inputs\"]}{sub_path[\"demand\"]}{input_file[\"demand_response_ava_neg\"]}{estimate}.csv', \n",
    "        index_col=0\n",
    "    )\n",
    "    ava_neg.index = pd.to_datetime(ava_neg.index)\n",
    "    \n",
    "    to_concat_pos = []\n",
    "    to_concat_neg = []\n",
    "    \n",
    "    ava_pos_scaled = pd.DataFrame(columns=ava_pos.columns)\n",
    "    ava_neg_scaled = pd.DataFrame(columns=ava_neg.columns)\n",
    "\n",
    "    for cluster, data in dr_param_dict.items():\n",
    "        data = data[[\"potential_pos_overall\", \"potential_neg_overall\"]]\n",
    "        relation_neg_to_pos_potential = np.nanmean(data[\"potential_neg_overall\"] / data[\"potential_pos_overall\"])\n",
    "        \n",
    "        # No shifting allowed; keep (empty) availabilities as they are\n",
    "        if relation_neg_to_pos_potential == 0:\n",
    "            ava_pos_scaled[cluster] = ava_pos[cluster].copy()\n",
    "            ava_neg_scaled[cluster] = ava_neg[cluster].copy()\n",
    "        # negative potential greater; scale down positive one\n",
    "        elif relation_neg_to_pos_potential > 1:\n",
    "            ava_pos_scaled[cluster] = ava_pos[cluster].copy() / relation_neg_to_pos_potential\n",
    "            ava_neg_scaled[cluster] = ava_neg[cluster].copy()\n",
    "        # positive potential greater; scale down negative one\n",
    "        else:\n",
    "            ava_pos_scaled[cluster] = ava_pos[cluster].copy() \n",
    "            ava_neg_scaled[cluster] = ava_neg[cluster].copy() * relation_neg_to_pos_potential\n",
    "        \n",
    "    for iter_year in range(2020, 2051):\n",
    "        to_concat_pos.append(tools.reindex_time_series(ava_pos_scaled, iter_year))\n",
    "        to_concat_neg.append(tools.reindex_time_series(ava_neg_scaled, iter_year))\n",
    "    \n",
    "    # Dispatch time series\n",
    "    ava_pos_dispatch = tools.reindex_time_series(ava_pos_scaled, year)\n",
    "    ava_neg_dispatch = tools.reindex_time_series(ava_neg_scaled, year)\n",
    "    \n",
    "    ava_pos_dispatch.to_csv(\n",
    "        f'{main_path[\"outputs\"]}{output_file[\"demand_response_ava_pos\"]}{estimate}_{year}.csv'\n",
    "    )\n",
    "    ava_neg_dispatch.to_csv(\n",
    "        f'{main_path[\"outputs\"]}{output_file[\"demand_response_ava_neg\"]}{estimate}_{year}.csv'\n",
    "    )\n",
    "\n",
    "    ava_pos = pd.concat(to_concat_pos)\n",
    "    ava_neg = pd.concat(to_concat_neg)\n",
    "    ava_pos.round(rounding_precision).to_csv(\n",
    "        f'{main_path[\"outputs\"]}{output_file[\"demand_response_ava_pos\"]}{estimate}.csv'\n",
    "    )\n",
    "    ava_neg.round(rounding_precision).to_csv(\n",
    "        f'{main_path[\"outputs\"]}{output_file[\"demand_response_ava_neg\"]}{estimate}.csv'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Include a demand projection\n",
    "German demand in future years is increased according to the assumed increase in power consumption of a scenario put together by Prognos et al. on behalf of Agora Energiewende, whereas the profile is kept.\n",
    "\n",
    "> _Note:_\n",
    "> * _This neglects the change of the consumption profile due to new consumers and assumes a fixed demand pattern in the first place._\n",
    "> * _Nonetheless, pommesinvest and pommesdispatch allow for flexible demand units to utilize demand peaks (in case they coincide with phases of little RES generation and thus high prices). Therefore, the bias due to not adapting the pattern is assumed not to be that large._\n",
    "\n",
    "For European countries, a demand projection from the European Resource Adequacy Outlook (ERAA) 2022 is used (see below).\n",
    "\n",
    "### Germany\n",
    "Steps applied:\n",
    "* Read in the demand from scenario KNS2035 resp. KND2050. (Value for 2050 is assigned to 2045.)\n",
    "* Include the following categories in scaling:\n",
    "    * the inflexible demand: this is passed through.\n",
    "    * heat pumps, electric vehicles and flexible electrode boilers: assumed to be represent baseline demand (assumed elegible for shifting, but not for shedding)\n",
    "* Not included are the following:\n",
    "    * H2 electrolysers: `pommesinvest` can choose to invest in these.\n",
    "    * Pumped storages and battery energy storages: `pommesinvest` can either invest in these or deploy existing units.\n",
    "* For the reference scenario KNDE2045, the respective shares are used for scaling the demand\n",
    "* Normalize values (2020 = 1).\n",
    "* Scale up demand profile accordingly.\n",
    "* Consider demand response:\n",
    "    * Subtract baseline load profiles from overall demand for Germany to calculate load of non-demand response units.\n",
    "    * Scale time series, so that maximum value for 2020 = 1 again.\n",
    "\n",
    "Sources:\n",
    "* Agora Energiewende, Prognos, Consentec (2022): Klimaneutrales Stromsystem 2035. Wie der deutsche Stromsektor bis zum Jahr 2035 klimaneutral werden kann, https://static.agora-energiewende.de/fileadmin/Projekte/2021/2021_11_DE_KNStrom2035/A-EW_264_KNStrom2035_WEB.pdf, accessed 01.12.2022.\n",
    "* Prognos, Öko-Institut, Wuppertal-Institut (2021): Klimaneutrales Deutschland 2045. Wie Deutschland seine Klimaziele schon vor 2050 erreichen kann. Zusammenfassung im Auftrag von Stiftung Klimaneutralität, Agora Energiewende und Agora Verkehrswende https://static.agora-energiewende.de/fileadmin/Projekte/2021/2021_04_KNDE45/A-EW_209_KNDE2045_Zusammenfassung_DE_WEB.pdf, accessed 01.12.2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kns2035_power_consumption = pd.read_csv(\n",
    "    main_path[\"inputs\"] + sub_path[\"assumptions\"] + input_file[\"KNS2035_power_consumption\"],\n",
    "    sep=\";\", index_col=0\n",
    ")\n",
    "kns2035_power_consumption.index = kns2035_power_consumption.index.str.strip().str.replace(\"\\n\", \"\")\n",
    "kns2035_power_consumption.index = kns2035_power_consumption.index.str.strip().str.replace(\"\\r\", \"\")\n",
    "kns2035_power_consumption.columns = kns2035_power_consumption.columns.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kns2035_power_consumption_for_pommes = kns2035_power_consumption.loc[[\n",
    "    \"flexibleElektrokessel\", \"Wärmepumpen*\", \"Elektromobilität**\",\n",
    "    \"inflexiblerStrombedarf\"\n",
    "]].sum()\n",
    "\n",
    "kns2035_power_consumption_for_pommes.loc[2045] = (\n",
    "    kns2035_power_consumption.at[\"VergleichsszenarioKNDE2045\", 2045] \n",
    "    - kns2035_power_consumption.at[\"KNDE2045 H2-Elektrolyseure\", 2045]\n",
    ")\n",
    "kns2035_power_consumption_for_pommes_all_years = pd.Series(index=range(2020, 2051), dtype=\"float64\")\n",
    "for iter_year in kns2035_power_consumption_for_pommes.index:\n",
    "    kns2035_power_consumption_for_pommes_all_years.loc[iter_year] = (\n",
    "        kns2035_power_consumption_for_pommes.loc[iter_year]\n",
    "    )\n",
    "\n",
    "kns2035_power_consumption_for_pommes_all_years.loc[2020] = kns2035_power_consumption_for_pommes_all_years.loc[2022]\n",
    "kns2035_power_consumption_for_pommes_all_years = kns2035_power_consumption_for_pommes_all_years.interpolate(how=\"linear\")\n",
    "kns2035_power_consumption_for_pommes_all_years = kns2035_power_consumption_for_pommes_all_years.div(\n",
    "    kns2035_power_consumption_for_pommes_all_years.loc[2020]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Europe\n",
    "* Use ERAA 2022 and TYNDP 2022 data for demand projection of other European countries.\n",
    "    * ERAA 2022 data only reaches up to 2030 and provides a rather conservative estimate.\n",
    "    * TYNDP 2022 data provides a massive demand for hydrogen that is unlikely to be produced domestically.\n",
    "    * To cope with this problem, mean values of the two estimates are calculated to provide somewhat of a more moderate assumption.\n",
    "* ERAA 2022 data set:\n",
    "    * Extract only peak demand values\n",
    "    * For Italy, sum up; for NO1, NO2 and NO5 distribute according to the consumption (which showed not to be as extreme as a   distribution according to current demand).\n",
    "    * Use climate year 2009 (average to slightly higher consumption, little wind; at least for Germany) to compare against peak demands of 2017.\n",
    "    * Create time series for peak demand development and interpolate in between.\n",
    "* TYNDP 2022 data set:\n",
    "    * Read in demand data and filter for climate year 2009 and Distributed Generation scenario (same as for generation above).\n",
    "    * Group values by country node and calculate overall annual demand.\n",
    "    * Derive time series for peak demand development from calculated overall demand and interpolate in between\n",
    "\n",
    "Sources:\n",
    "* ENTSO-E (2022a): ERAA 2022. Demand TimeSeries post consultation, https://eepublicdownloads.entsoe.eu/clean-documents/sdc-documents/ERAA/2022/data-for-publication/ERAA_2022_Demand_TimeSeries_post_consultation.7z, Accessed 01.12.2022.\n",
    "* ENTSO-E & ENTSO-G (2022b): TYNDP 2022 Scenario Report – additional Downloads, Electricity Modelling Data, Demand, https://view.officeapps.live.com/op/view.aspx?src=https%3A%2F%2F2022.entsos-tyndp-scenarios.eu%2Fwp-content%2Fuploads%2F2022%2F04%2F220310_Updated_Electricity_Modelling_Results.xlsx&wdOrigin=BROWSELINK, accessed 05.12.2022.\n",
    "\n",
    "#### ERAA Demand projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bidding_zones = {\n",
    "    \"AT00\": \"AT\",\n",
    "    \"BE00\": \"BE\",\n",
    "    \"CH00\": \"CH\",\n",
    "    \"CZ00\": \"CZ\",\n",
    "    \"DKE1\": \"DK1\",\n",
    "    \"DKW1\": \"DK2\",\n",
    "    \"FR00\": \"FR\",\n",
    "    \"FR15\": \"FR\",\n",
    "    \"ITSI\": \"IT\",  # Italian market zones to be combined\n",
    "    \"ITSA\": \"IT\",\n",
    "    \"ITS1\": \"IT\",\n",
    "    \"ITN1\": \"IT\",\n",
    "    \"ITCS\": \"IT\",\n",
    "    \"ITCN\": \"IT\",\n",
    "    \"ITCA\": \"IT\",\n",
    "    \"NL00\": \"NL\",\n",
    "    \"NOM1\": \"NO3\",\n",
    "    \"NON1\": \"NO4\",\n",
    "    \"NOS0\": \"NO2\",  # to be split among NO2, NO1 and NO5\n",
    "    \"PL00\": \"PL\",\n",
    "    \"SE01\": \"SE1\",\n",
    "    \"SE02\": \"SE2\",\n",
    "    \"SE03\": \"SE3\",\n",
    "    \"SE04\": \"SE4\",\n",
    "}\n",
    "peak_demand_eraa = {zone: pd.Series(0.0, range(1982, 2017)) for zone in set(bidding_zones.values())}\n",
    "for entsoe_zone, zone in bidding_zones.items():\n",
    "    file = pd.read_excel(\n",
    "        main_path[\"inputs\"] + sub_path[\"demand\"] + input_file[\"entsoe_national_demand_estimate\"],\n",
    "        sheet_name=entsoe_zone, usecols=\"B:AK\", skiprows=7, nrows=5, index_col=0, header=None\n",
    "    )\n",
    "    file.columns = file.loc[\"Hour\"].values.astype(\"int\")\n",
    "    file = file.loc[\"Yearly peak (MW)\"].astype(\"float64\")\n",
    "    peak_demand_eraa[zone] += file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribute demand for norway according to current consumption distribution\n",
    "NO_zones = [\"NO1\", \"NO2\", \"NO5\"]\n",
    "NO_demand = sinks_demand_el_ts[[f\"{no_zone}_sink_el_load\" for no_zone in NO_zones]].sum().sum()\n",
    "NO_shares = {\n",
    "    no_zone: sinks_demand_el_ts[f\"{no_zone}_sink_el_load\"].sum() / NO_demand for no_zone in NO_zones\n",
    "}\n",
    "\n",
    "for no_zone in NO_zones:\n",
    "    peak_demand_eraa[no_zone] = peak_demand_eraa[\"NO2\"] * NO_shares[no_zone]\n",
    "    \n",
    "bidding_zones = list(set(bidding_zones.values()))\n",
    "bidding_zones.extend([\"NO1\", \"NO5\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sinks_demand_el_eraa_factors = pd.DataFrame(columns=range(2020, 2051), index=sinks_demand_el.index)\n",
    "\n",
    "sinks_demand_el_eraa_factors[2020] = 1\n",
    "for country in bidding_zones:\n",
    "    sinks_demand_el_eraa_factors.at[f\"{country}_sink_el_load\", 2030] = (\n",
    "        peak_demand_eraa[country].loc[2009] / sinks_demand_el.at[f\"{country}_sink_el_load\", \"maximum\"]\n",
    "    )\n",
    "\n",
    "sinks_demand_el_eraa_factors.loc[\"DE_sink_el_load\"] = 1\n",
    "sinks_demand_el_eraa_factors = sinks_demand_el_eraa_factors.astype(\"float64\").interpolate(how=\"linear\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TYNDP demand projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tyndp2022_demand = pd.read_excel(\n",
    "    f\"{main_path['inputs']}{sub_path['pp_public']}{input_file['tyndp_2022']}\",\n",
    "    sheet_name=\"Demand\"\n",
    ")\n",
    "\n",
    "tyndp2022_nodes = {\n",
    "    \"AT00\": \"AT\",\n",
    "    \"BE00\": \"BE\",\n",
    "    \"CH00\": \"CH\",\n",
    "    \"CZ00\": \"CZ\",\n",
    "    \"DKE1\": \"DK2\",\n",
    "    \"DKKF\": \"DK1\",\n",
    "    \"DKW1\": \"DK1\",\n",
    "    \"FR00\": \"FR\",\n",
    "    \"FR15\": \"FR\",  # Corsica attributed to France\n",
    "    \"IT00\": \"IT\",\n",
    "    \"NL00\": \"NL\",\n",
    "    \"NOM1\": \"NO3\",\n",
    "    \"NON1\": \"NO4\",\n",
    "    \"NOS0\": \"NO1\",  # Split to NO1, NO2, NO5 according to current capacity shares\n",
    "    \"PL00\": \"PL\",\n",
    "    \"SE01\": \"SE1\",\n",
    "    \"SE02\": \"SE2\",\n",
    "    \"SE03\": \"SE3\",\n",
    "    \"SE04\": \"SE4\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tyndp2022_demand = tyndp2022_demand.loc[\n",
    "    (tyndp2022_demand[\"Climate Year\"] == \"CY 2009\")\n",
    "    & (tyndp2022_demand[\"Scenario\"] == \"Distributed Energy\")\n",
    "    & (tyndp2022_demand[\"Node\"].isin(tyndp2022_nodes.keys()))\n",
    "    & (tyndp2022_demand[\"Parameter\"] == \"Native Demand (GWh)\")\n",
    "]\n",
    "# Do some renaming and reindexing\n",
    "tyndp2022_demand[\"Country\"] = tyndp2022_demand[\"Node\"].replace(tyndp2022_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tyndp2022_demand_values = pd.DataFrame(columns=[2030, 2040, 2050])\n",
    "for iter_year in [2030, 2040, 2050]:\n",
    "    tyndp2022_demand_values[iter_year] = tyndp2022_demand.loc[\n",
    "        tyndp2022_demand[\"Year\"] == iter_year, [\"Country\", \"Value\"]].groupby(\n",
    "            \"Country\"\n",
    "        ).sum()\n",
    "\n",
    "# Add estimate for Norwegian split\n",
    "for no_zone in NO_zones:\n",
    "    tyndp2022_demand_values.loc[no_zone] = tyndp2022_demand_values.loc[\"NO1\"] * NO_shares[no_zone]\n",
    "    \n",
    "for country in tyndp2022_demand_values.index.unique():\n",
    "    tyndp2022_demand_values.at[country, 2020] = (\n",
    "        sinks_demand_el_ts[f\"{country}_sink_el_load\"].sum() \n",
    "        * sinks_demand_el.at[f\"{country}_sink_el_load\", \"maximum\"] / 1000\n",
    "    )\n",
    "tyndp2022_demand_values = tyndp2022_demand_values[[2020, 2030, 2040, 2050]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sinks_demand_el_tyndp_factors = pd.DataFrame(columns=range(2020, 2051))\n",
    "\n",
    "demand_2020 = tyndp2022_demand_values[2020].copy()\n",
    "for col in tyndp2022_demand_values.columns:\n",
    "    sinks_demand_el_tyndp_factors[col] = tyndp2022_demand_values[col].div(demand_2020)\n",
    "\n",
    "sinks_demand_el_tyndp_factors.loc[\"DE\"] = 1\n",
    "sinks_demand_el_tyndp_factors = sinks_demand_el_tyndp_factors.astype(\"float64\").interpolate(how=\"linear\", axis=1)\n",
    "sinks_demand_el_tyndp_factors.index = sinks_demand_el_tyndp_factors.index + \"_sink_el_load\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sinks_demand_el_factors = sinks_demand_el_eraa_factors.copy()\n",
    "for col in sinks_demand_el_eraa_factors.columns:\n",
    "    sinks_demand_el_factors[col] = (sinks_demand_el_eraa_factors[col] + sinks_demand_el_tyndp_factors[col]) / 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demand not considering demand response\n",
    "to_concat = []\n",
    "for iter_year in range(2020, 2051):\n",
    "    sinks_demand_el_ts_2017_scaled = sinks_demand_el_ts_2017.copy()\n",
    "    sinks_demand_el_ts_2017_scaled[\"DE_sink_el_load\"] *= kns2035_power_consumption_for_pommes_all_years.loc[iter_year]\n",
    "    sinks_demand_el_ts_2017_scaled *= sinks_demand_el_factors[iter_year]\n",
    "    to_concat.append(tools.reindex_time_series(sinks_demand_el_ts_2017_scaled, iter_year))\n",
    "    \n",
    "overall_demand_ts = pd.concat(to_concat)\n",
    "\n",
    "overall_demand_ts.to_csv(\n",
    "    main_path[\"outputs\"] + output_file[\"sinks_demand_el_ts\"] + \"_hourly.csv\"\n",
    ")\n",
    "\n",
    "if year >= 2020:\n",
    "    overall_demand_ts_dispatch = overall_demand_ts.loc[str(year)]\n",
    "else:\n",
    "    overall_demand_ts_dispatch = overall_demand_ts.loc[str(2020)].copy()\n",
    "    # Hack: 2017 is used as a reference for reindexing time series; we have 2020 here, thus year-3\n",
    "    overall_demand_ts_dispatch = tools.reindex_time_series(overall_demand_ts_dispatch, year-3)\n",
    "\n",
    "overall_demand_ts_dispatch.to_csv(\n",
    "    main_path[\"outputs\"] + output_file[\"sinks_demand_el_ts\"] + \"_hourly_\" + str(year) + \".csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demand considering demand response\n",
    "german_load = overall_demand_ts[\"DE_sink_el_load\"] * sinks_demand_el.loc['DE_sink_el_load', 'maximum']\n",
    "load_profile_germany = {}\n",
    "\n",
    "new_german_load = {}\n",
    "\n",
    "for estimate in [\"5\", \"50\", \"95\"]:\n",
    "    to_concat = []\n",
    "    new_german_load[estimate] = german_load - absolute_baseline_profiles[estimate].sum(axis=1)\n",
    "    new_german_load[estimate] -= ev_baseline_consumption\n",
    "    \n",
    "    # Replace value for Germany only and write values to file\n",
    "    sinks_demand_el_excl_demand_response_units = sinks_demand_el.copy()\n",
    "    sinks_demand_el_excl_demand_response_units.at[\"DE_sink_el_load\", \"maximum\"] = (\n",
    "        new_german_load[estimate].loc[new_german_load[estimate].index.year == 2020].max()\n",
    "    )\n",
    "    \n",
    "    overall_demand_ts[\"DE_sink_el_load\"] = (\n",
    "        new_german_load[estimate] \n",
    "        / new_german_load[estimate].loc[new_german_load[estimate].index.year == 2020].max()\n",
    "    )\n",
    "    \n",
    "    if year >= 2020:\n",
    "        overall_demand_ts_dispatch[\"DE_sink_el_load\"] = overall_demand_ts.loc[str(year), \"DE_sink_el_load\"]\n",
    "    else:\n",
    "        overall_demand_ts_dispatch[\"DE_sink_el_load\"] = overall_demand_ts.loc[str(2020), \"DE_sink_el_load\"].values\n",
    "    \n",
    "    sinks_demand_el_excl_demand_response_units.to_csv(\n",
    "        f'{main_path[\"outputs\"]}{output_file[\"sinks_demand_el_excl_demand_response\"]}_{estimate}.csv'\n",
    "    )\n",
    "    overall_demand_ts.round(rounding_precision).to_csv(\n",
    "        f'{main_path[\"outputs\"]}{output_file[\"sinks_demand_el_excl_demand_response_ts\"]}_{estimate}_hourly.csv'\n",
    "    )\n",
    "    overall_demand_ts_dispatch.to_csv(\n",
    "        f'{main_path[\"outputs\"]}{output_file[\"sinks_demand_el_excl_demand_response_ts\"]}_{estimate}_hourly_{year}.csv'\n",
    "    )\n",
    "    \n",
    "    # Include sensitivities for inflexible baseline demand excl demand response\n",
    "    for sensitivity, multiplier in sensitivities.items():\n",
    "        sinks_demand_el_excl_demand_response_units_sensitivity = sinks_demand_el_excl_demand_response_units.copy()\n",
    "        sinks_demand_el_excl_demand_response_units_sensitivity.at[\"DE_sink_el_load\", \"maximum\"] *= multiplier\n",
    "        sinks_demand_el_excl_demand_response_units_sensitivity.round(rounding_precision).to_csv(\n",
    "             f'{main_path[\"outputs\"]}{output_file[\"sinks_demand_el_excl_demand_response\"]}'\n",
    "             f'_{estimate}_sensitivity_{sensitivity}.csv'\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excess Sinks\n",
    "Steps applied:\n",
    "- Define and parameterize electricity excess sink per bidding zone\n",
    "- Introduce high excess costs (penalty) costs in order not to produce to much excess\n",
    "- Concatenate the electricity excess sinks with the reservoir spillage Sinks\n",
    "- Split into a dispatch and an investment data set\n",
    "- Drop the German electricity excess sink for dispatch modelling since it is explicitly modeled in _pommesdispatch_\n",
    "- Introduce hydrogen for the investment data set\n",
    "- Write the Sinks data to a csv and an Excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sinks_excess = pd.DataFrame(data = {'country': countries_modeled})\n",
    "sinks_excess['label'] = sinks_excess['country'] + '_sink_el_excess'\n",
    "sinks_excess['from'] = sinks_excess['country'] + '_bus_el'\n",
    "sinks_excess['excess_costs'] = excess_costs\n",
    "sinks_excess.set_index('label', inplace = True)\n",
    "\n",
    "sinks_excess = pd.concat([sinks_excess, sinks_reservoir_spillage])\n",
    "sinks_excess_dispatch = sinks_excess.copy(deep=True)\n",
    "sinks_excess_investment = sinks_excess.copy(deep=True)\n",
    "\n",
    "sinks_excess_dispatch.to_csv(\n",
    "    main_path[\"outputs\"] + output_file[\"sinks_excess\"] + \"_\" + str(year) + \".csv\"\n",
    ")\n",
    "sinks_excess_dispatch.to_excel(writer, sheet_name='sinks_excess' + \"_\" + str(year))\n",
    "\n",
    "# add hydrogen excess sink\n",
    "sinks_excess_investment.loc[\"DE_sink_hydrogen_excess\"] = {\n",
    "    \"country\": \"DE\", \"from\": \"DE_bus_hydrogen\", \"excess_costs\": excess_costs\n",
    "}\n",
    "\n",
    "# For the investment model, use predefined excess costs\n",
    "sinks_excess_investment[\"excess_costs\"] = excess_costs_investment\n",
    "\n",
    "# investment model\n",
    "sinks_excess_investment.to_csv(\n",
    "    main_path[\"outputs\"] + output_file[\"sinks_excess\"] + \".csv\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do some garbage collection\n",
    "try:\n",
    "    del (countries_modeled, peak_demand, sinks_demand_el, sinks_demand_el_ts, sinks_excess,\n",
    "         sinks_reservoir_spillage, ts_eu, ts_eu_el_demand)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Buses\n",
    "\n",
    "ave all buses elements which were already created above to csv and to Excel\n",
    "Links to sections were buses were created resp. adde:\n",
    "- [electricity buses](#Introduce-electricity-buses) &rarr; instanciation of buses data set\n",
    "- [hydro buses](#Prepare-data-for-usage-in-oemof.solph)\n",
    "- [commodity buses](#Commodity-sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buses_df = pd.DataFrame(index=buses)\n",
    "buses_df['country'] = [_[0] for _ in buses_df.index.str.split('_')]\n",
    "buses_df.index.name = 'label'\n",
    "buses_df.to_csv(\n",
    "    main_path[\"outputs\"] + output_file[\"buses\"] + \"_\" + str(year) + \".csv\"\n",
    ")\n",
    "\n",
    "buses_df.to_excel(writer, sheet_name='buses' + \"_\" + str(year))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_buses_df = pd.DataFrame(index=all_buses)\n",
    "all_buses_df['country'] = [_[0] for _ in all_buses_df.index.str.split('_')]\n",
    "all_buses_df.index.name = 'label'\n",
    "all_buses_df.to_csv(\n",
    "    main_path[\"outputs\"] + output_file[\"buses\"] + \".csv\"\n",
    ")\n",
    "\n",
    "all_buses_df.to_excel(writer, sheet_name='buses')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Costs\n",
    "Costs are read in from raw data or already compiled collections of assumptions.\n",
    "Cost values are spreaded across all countries considered.\n",
    "\n",
    "> _Note:_\n",
    "> * _Since_ `pommesdispatch` _includes a detailed consideration of RES payments in the market premium scheme where payments on a nominal term are made, nominal cost terms are used throughout the model and converted back to real terms after a model run. We also provide the cost values in real terms for other analyses._\n",
    "> * _As historical real values do not vary significantly between the years, it is assumed that nominal terms do not change between 2017 and 2020. For converting to real values, everything is expressed in terms of 2020._\n",
    "\n",
    "Cost categories considered:\n",
    "- OPEX\n",
    "- Fuel costs (commodity only)\n",
    "- Emissions costs\n",
    "- Storage OPEX (assumption)\n",
    "\n",
    "Steps applied for each of the cost categories:\n",
    "- Read in the raw data\n",
    "- Match index with data set (in most cases conventional power plants)\n",
    "- Convert from real to nominal terms\n",
    "- Round values\n",
    "- Save results to a csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operation costs and markups\n",
    "It is assumed that medium and peak load operators include slight markups in their bids in order to refinance the capacities.\n",
    "\n",
    "Steps applied:\n",
    "* Read in operation costs and markup assumptions\n",
    "* Spread across all years from 2017 to 2020\n",
    "* Add projection from 2021 onwards by applying inflation rate to calculate nominal values\n",
    "* Combine historical and projected data\n",
    "* Rearrange to account for commodity sources in the respective targe year\n",
    "\n",
    "Literature for assumptions:\n",
    "\n",
    "* Kost, Christoph; Shammugam, Shivenes; Fluri, Verena; Peper, Dominik; Memar, Aschkan Davoodi; Schlegl, Thomas (2021): Stromgestehungskosten Erneuerbare Energien. Juni 2021, Fraunhofer ISE, https://www.ise.fraunhofer.de/content/dam/ise/de/documents/publications/studies/DE2021_ISE_Studie_Stromgestehungskosten_Erneuerbare_Energien.pdf, accessed 28.02.2022.\n",
    "* Pietzcker, Robert, Osorio, Sebastian, Rodrigues, Renato (2021): Tightening EU ETS targets in line with the European Green Deal: Impacts on the decarbonization of the EU power sector, in: Applied Energy 293 (2021), https://doi.org/10.1016/j.apenergy.2021.116914"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources_commodity['fuel'] = [\n",
    "    commodity_bus[1] \n",
    "    for commodity_bus in sources_commodity['to'].str.rsplit('_', 1)\n",
    "]\n",
    "sources_commodity['technology'] = [\n",
    "    commodity_bus[1]\n",
    "    for commodity_bus in sources_commodity['to'].str.rsplit('_', 1)\n",
    "]\n",
    "all_sources_commodity['fuel'] = [\n",
    "    commodity_bus[1]\n",
    "    for commodity_bus in all_sources_commodity['to'].str.rsplit('_', 1)\n",
    "]\n",
    "all_sources_commodity['technology'] = [\n",
    "    commodity_bus[1]\n",
    "    for commodity_bus in all_sources_commodity['to'].str.rsplit('_', 1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in assumption and repeat status quo values for 2017-2020\n",
    "opex = pd.read_csv(\n",
    "    main_path[\"inputs\"] + sub_path[\"costs\"] + input_file[\"operation_costs_assumptions\"], \n",
    "    index_col=0, sep=\";\", decimal=\",\"\n",
    ")\n",
    "markups = pd.read_csv(\n",
    "    main_path[\"inputs\"] + sub_path[\"costs\"] + input_file[\"markup_assumptions\"], \n",
    "    index_col=0, sep=\";\", decimal=\",\"\n",
    ")\n",
    "\n",
    "# Include markups in opex for the sake of convenience\n",
    "opex[\"opex\"] = opex[\"opex\"] + pd.Series(index=opex.index, data=markups[\"markup\"]).fillna(0)\n",
    "\n",
    "opex[[\n",
    "    iter_year for iter_year in range(2017, 2021)\n",
    "]] = opex.opex.values.repeat(len(range(2017, 2021))).reshape(\n",
    "    len(opex), len(range(2017, 2021))\n",
    ")\n",
    "opex = opex[[\n",
    "    iter_year for iter_year in range(2017, 2021)\n",
    "]]\n",
    "opex = opex.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to real terms\n",
    "opex_real = opex.copy(deep=True)\n",
    "for iter_year in opex.columns:\n",
    "    opex_real[iter_year] = opex[iter_year].div(inflation_rate ** (iter_year - 2020)).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add OPEX projection (nominal values)\n",
    "prediction_horizon = range(opex.columns[-1] + 1, 2051)\n",
    "opex_pred = pd.DataFrame(index=opex.index, columns=prediction_horizon)\n",
    "opex_pred_real = opex_pred.copy(deep=True)\n",
    "for energy_carrier in opex_pred.index:\n",
    "    opex_pred.loc[energy_carrier] = opex.at[energy_carrier, 2020]\n",
    "    opex_pred_real.loc[energy_carrier] = opex.at[energy_carrier, 2020]\n",
    "    opex_pred.loc[energy_carrier] = opex_pred.loc[energy_carrier].astype(np.float32).mul(\n",
    "        inflation_rate ** np.arange(1, len(opex_pred.columns) + 1)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine status quo and projection data and reformat\n",
    "# Nominal costs\n",
    "costs_opex = pd.concat([opex, opex_pred], axis=1)\n",
    "costs_opex = tools.reformat_costs_values(costs_opex, sources_commodity)\n",
    "costs_opex.to_csv(\n",
    "    main_path[\"outputs\"] + output_file[\"costs_operation\"] + \"_nominal_\" + str(year) + \".csv\"\n",
    ")\n",
    "costs_opex.to_excel(writer, sheet_name='costs_operation_nominal' + \"_\" + str(year))\n",
    "\n",
    "# Transform to time series\n",
    "costs_opex_ts = tools.transform_values_to_annual_time_series(costs_opex)\n",
    "costs_opex_ts.to_csv(\n",
    "    main_path[\"outputs\"] + output_file[\"costs_operation\"] \n",
    "    + \"_nominal_indexed_ts.csv\"\n",
    ")\n",
    "\n",
    "# Real costs\n",
    "costs_opex_real = pd.concat([opex_real, opex_pred_real], axis=1)\n",
    "costs_opex_real = tools.reformat_costs_values(opex_pred_real, sources_commodity)\n",
    "costs_opex_real.to_csv(\n",
    "    main_path[\"outputs\"] + output_file[\"costs_operation\"] + \"_real_\" + str(year) + \".csv\"\n",
    ")\n",
    "costs_opex_real.to_excel(writer, sheet_name='costs_operation_real' + \"_\" + str(year))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storage opex are assumed to equal to 0.1 €/MWh\n",
    "# Note: These are more or less model technicalities to prevent excessive storage use\n",
    "opex_storages_real = pd.DataFrame(\n",
    "    index=storages_el.index, columns=range(2017, 2051), data=0.1\n",
    ")\n",
    "opex_storages_real.to_csv(\n",
    "    main_path[\"outputs\"] + output_file[\"costs_operation_storages\"] + \"_real_\" + str(year) + \".csv\"\n",
    ")\n",
    "opex_storages_real.to_excel(writer, sheet_name='opex_storages_real')\n",
    "\n",
    "opex_storages_nominal = opex_storages_real.copy(deep=True)\n",
    "for iter_year in opex_storages_nominal.columns:\n",
    "    opex_storages_nominal[iter_year] = opex_storages_real[iter_year].mul(inflation_rate ** (iter_year - 2020)).round(3)\n",
    "opex_storages_nominal.to_csv(\n",
    "    main_path[\"outputs\"] + output_file[\"costs_operation_storages\"] + \"_nominal_\" + str(year) + \".csv\"\n",
    ")\n",
    "opex_storages_nominal.to_excel(writer, sheet_name='opex_storages_nominal')\n",
    "\n",
    "# Transform to time series\n",
    "opex_storages_nominal_ts = tools.transform_values_to_annual_time_series(opex_storages_nominal)\n",
    "opex_storages_nominal_ts.to_csv(\n",
    "    main_path[\"outputs\"] + output_file[\"costs_operation_storages\"] \n",
    "    + \"_nominal_indexed_ts.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fuel costs\n",
    "The sources for fuel costs vary by type of fuel considered. The sources for historical values are as follows:\n",
    "\n",
    "| fuel | sources |\n",
    "| ---- | ---- |\n",
    "| oil, natural gas, hard coal | BMWK Energiedaten (2022), Table 26;<br>based on data from BAFA & Mineralölwirtschaftsverband |\n",
    "| waste, water, solarthermal, otherfossil | assumption |\n",
    "| uranium | assumption based on Egerer et al. (2014), p. 32 |\n",
    "| lignite | assumption based on Schulte et al. (2021), p. 10 |\n",
    "| biomass | assumption based on Prognos AG 2013, p.31, Teske et al. 2015, p. 66,<br> Bang et al. 2013, p. 63, Koch et al. 2018, pp. 26ff. |\n",
    "\n",
    "For future, projections, the following alternatives are available:\n",
    "* historical trend extrapolation\n",
    "* price trends derived from the IEA's latest world energy outlook 2021, p. 101\n",
    "\n",
    "For future projection, the approach again differs by energy source is proceeded as follows:\n",
    "* For oil, natural gas and hard coal, price trends are derived.\n",
    "* For uranium, lignite, biomass, waste, water, solarthermal, constant values are assumed and the values are only corrected by the assumed inflation.\n",
    "* For mixed fuels, mean values for lignite, hard coal, natural gas and biomass are used.\n",
    "* For other fossil, the average price trend for oil, natural gas and hard coal is used in addition to correcting by inflation.\n",
    "\n",
    "The following future projections can be selected from:\n",
    "\n",
    "| projection | explanation | characteristics |\n",
    "| ---- | ---- | ---- |\n",
    "| NZE | Net Zero Emissions scenario from IEA's world energy outlook 2021 | comparatively low commodity prices |\n",
    "| SDS | Sustainable Development scneario from IEA's world energy outlook 2021 | comparatively low commodity prices; sligthly higher than NZE |\n",
    "| APS | Announced Pledges Scenario from IEA's world energy outlook 2021 | medium price development, decline in prices between 2030 and 2050 |\n",
    "| STEPS | Stated Policies Scenario from IEA's world energy outlook 2021 | highest price development, esp. for oil and natgas |\n",
    "| regression | Linear regression based on historic commodity prices from 1991-2020 | compared to IEA's scenarios, close to upper range of projections |\n",
    "\n",
    "For historical years, monthly values for fuel prices are derived from DESTATIS long-term price series. For future years, prices are assumed to be constant for the whole year.\n",
    "\n",
    "Literature:\n",
    "* BMWK (2022): Energiedaten. Gesamtausgabe, as of 20.01.2022, https://www.bmwi.de/Redaktion/DE/Artikel/Energie/energiedaten-gesamtausgabe.html, accessed 25.02.2022.\n",
    "* Bang, Christian; Vitina, Aisma; Gregg, Jay Sterling; Lindboe, Hans Henrik (2013): Analysis of biomass prices. Future danish prices for straw, wood chips and wood pellets \"final report\". Kopenhagen.\n",
    "* Egerer, J.; Gerbaulet, C.; Ihlenburg, R.; Kunz, F.; Reinhard, B.; Hirschhausen, C. von et al. (2014): Electricity Sector Data for Policy-Relevant Modeling. Data Documentation and Applications to the German and European Electricity Markets. Berlin (72)\n",
    "* IEA (2021): World Energy Outlook 2021.\n",
    "* Koch, Matthias; Hennenberg, Klaus; Hünecke, Katja; Haller, Markus; Hesse; Tilman (2018): Rolle der Bioenergie im Strom-und Wärmemarkt bis 2050 unter Einbeziehung des zukünftigen Gebäudebestandes. Wissenschaftlicher Endbericht. Freiburg, Darmstadt.\n",
    "* Prognos AG (2013): Entwicklung von Stromproduktionskosten. Die Rolle von Freiflächen-Solarkraftwerken in der Energiewende. Unter Mitarbeit von Leonard Krampe und Inka Ziegenhagen. Berlin.\n",
    "* Teske, Sven; Sawyer, Steve; Schäfer, Oliver (2015): energy [r]evolution. A sustainable world energy outlook 2015."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process historical raw data\n",
    "Steps applied:\n",
    "* Read in commodity price data, data for transportation costs and fuel costs assumptions\n",
    "* Do some renaming and convert units to $ €/MWh_{th, Hu} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fuel = pd.read_excel(\n",
    "    main_path[\"inputs\"] + sub_path[\"costs\"] + input_file[\"energiedaten\"],\n",
    "    index_col=0, skiprows=6, nrows=7\n",
    ")\n",
    "transportation_costs_assumptions = pd.read_csv(\n",
    "    main_path[\"inputs\"] + sub_path[\"costs\"] + input_file[\"transportation_costs\"],\n",
    "    index_col=0, sep=\";\", decimal=\".\"\n",
    ")\n",
    "fuel_costs_assumptions = pd.read_csv(\n",
    "    main_path[\"inputs\"] + sub_path[\"costs\"] + input_file[\"fuel_cost_assumptions\"],\n",
    "    index_col=0, sep=\";\", decimal=\".\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fuels_dict = {\n",
    "    \"  - Rohöl\": \"oil\",\n",
    "    \"  - Erdgas\": \"natgas\",\n",
    "    \"  - Steinkohlen\": \"hardcoal\",\n",
    "}\n",
    "conversion_factors_to_MWh_th = {\n",
    "    \"t OE\": 11.63,\n",
    "    \"TJ\": 10 ** 6 / 3600,\n",
    "    \"t SKE\": 8.14 \n",
    "}\n",
    "fuels_units = {\n",
    "    \"oil\": \"t OE\",\n",
    "    \"natgas\": \"TJ\",\n",
    "    \"hardcoal\": \"t SKE\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fuel = fuel.loc[[\n",
    "    fuel for fuel in fuels_dict.keys()], range(1991, 2021)\n",
    "].rename(fuels_dict)\n",
    "for f in fuels_dict.values():\n",
    "    fuel.loc[f] = fuel.loc[f].div(\n",
    "        conversion_factors_to_MWh_th[fuels_units[f]]\n",
    "    ) + transportation_costs_assumptions.at[f, \"transportation\"]\n",
    "\n",
    "for f in fuel_costs_assumptions.index:\n",
    "    fuel.loc[f, :] = fuel_costs_assumptions.at[f, \"commodity\"]\n",
    "\n",
    "fuel.loc[\"mixedfuels\"] = fuel.loc[[\"oil\", \"natgas\", \"hardcoal\", \"biomass\"]].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use WEO price trend estimates\n",
    "Use latest world energy outlook price projections to derive future estimates:\n",
    "* Read in data and filter for data for European Union as proxy\n",
    "* Calculate normalized price trends\n",
    "* Convert from real to nominal terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weo_prices = pd.read_excel(\n",
    "    main_path[\"inputs\"] + sub_path[\"costs\"] + input_file[\"prices_iea_weo\"],\n",
    "    index_col=[0, 1], header=[0, 1]\n",
    ")\n",
    "\n",
    "# Slice data needed and normalize (2020 = 1.0)\n",
    "weo_prices = weo_prices.loc[[\n",
    "    ('IEA crude oil (USD/barrel) ', 'IEA crude oil (USD/barrel) '),\n",
    "    ('Natural gas (USD/MBtu)', 'European Union '),\n",
    "    ('Steam coal (USD/tonne)', 'European Union ')\n",
    "]].iloc[:,1:]\n",
    "weo_prices = weo_prices.div(weo_prices[('historical', 2020)], axis=0)\n",
    "\n",
    "# Rename and tidy up\n",
    "weo_scenarios = {\n",
    "    \"Net Zero\\nEmissions\\nby 2050\": \"NZE\",\n",
    "    \"Sustainable\\nDevelopment\": \"SDS\",\n",
    "    \"Announced\\nPledges\": \"APS\",\n",
    "    \"Stated\\nPolicies\": \"STEPS\"\n",
    "}\n",
    "weo_prices = weo_prices.rename(\n",
    "    columns=weo_scenarios,\n",
    "    index={\n",
    "        \"IEA crude oil (USD/barrel) \": \"oil\",\n",
    "        \"Natural gas (USD/MBtu)\": \"natgas\",\n",
    "        \"Steam coal (USD/tonne)\": \"hardcoal\"\n",
    "    }\n",
    ").reset_index(level=1, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fuels_for_prediction = [\"oil\", \"natgas\", \"hardcoal\"]\n",
    "fuel_scens = [\n",
    "    (f, scen) \n",
    "    for f in fuels_for_prediction\n",
    "    for scen in weo_scenarios.values()\n",
    "]\n",
    "prediction_horizon = range(fuel.columns.values[-1], 2051)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weo_fuel_price_paths = pd.DataFrame(\n",
    "    index=prediction_horizon, \n",
    "    columns=pd.MultiIndex.from_tuples(fuel_scens)\n",
    ")\n",
    "\n",
    "# Create normalized path time series\n",
    "weo_fuel_price_paths.loc[2020] = 1.0\n",
    "for f, scen in fuel_scens:\n",
    "    weo_fuel_price_paths.at[2030, (f, scen)] = weo_prices.at[f, (scen, 2030)]\n",
    "    weo_fuel_price_paths.at[2050, (f, scen)] = weo_prices.at[f, (scen, 2050)]\n",
    "weo_fuel_price_paths = weo_fuel_price_paths.astype(\"float64\").interpolate()\n",
    "weo_fuel_price_paths = weo_fuel_price_paths.iloc[1:]\n",
    "\n",
    "# Convert real to nominal terms\n",
    "weo_fuel_price_paths_nominal = weo_fuel_price_paths.mul(\n",
    "    inflation_rate ** np.arange(len(weo_fuel_price_paths)), axis=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Include Ukraine war price (shock) effect\n",
    "\n",
    "Adjust prices to Ukraine war price shock. Use\n",
    "* EWI price scenarios and\n",
    "* current future notations, Calendar 2023 to Calendar 2025 resp. months Jan23 to Dec25 for oil,\n",
    "as a proxy / checkback.\n",
    "* For oil, stick to EWI price estimates which are highly similar to current future estimates.\n",
    "\n",
    "Steps applied:\n",
    "* Read in prices for natural gas, oil and hard coal from EWI scenarios\n",
    "* Read in current future prices and convert units; Conversion applied:\n",
    "    * hard coal: 1 USD/t SKE = 1/8.141 €/MWh\n",
    "    * oil: 1 USD/bbl = 1/159 USD/l * 0.883 kg/l * 41.9 MJ/kg * 3.6 MWh/MJ\n",
    "* Normalize them (2020 = 1.0)\n",
    "* Assume price effects to gradually stabilize until 2045 (climate neutrality).\n",
    "* Extract the high and the low estimate and calculate correction factors.\n",
    "* Use factors as correction for WEO 2021 and regression estimates made before the breakout of the Ukraine war.\n",
    "\n",
    "Sources:\n",
    "* EWI (2022): Szenarien für die Preisentwicklung von Energieträgern. Endbericht. Im Auftrag des Akademienprojekts „Energiesysteme der Zukunft“ (ESYS). Juli 2022.\n",
    "* Natural Gas: OTC Prices for THE, provided by Methanology, Calendar 2023-2025 (values in €/MWh), trading dates 03.01.2022 - 09.09.2022.\n",
    "* Hard coal: OTC Prices for API#2 (Amsterdam, Rotterdam, Antwerpen), provided by Spectron (values in USD/t), trading dates 03.01.2022 - 09.09.2022.\n",
    "* Oil: Exchange Prices for Crude Oil Brent, provided by ICE (values in USD/bbl), trading date 13.09.2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future_prices = {\"natgas\": {}, \"hardcoal\": {}, \"oil\": pd.DataFrame()}\n",
    "last_future_prices = {\"natgas\": {}, \"hardcoal\": {}, \"oil\": {}}\n",
    "for iter_year in range(2023, 2026):\n",
    "    future_prices[\"natgas\"][iter_year] = pd.read_csv(\n",
    "        f'{main_path[\"inputs\"]}{sub_path[\"costs\"]}{input_file[\"natgas_futures\"]} {iter_year}.csv', \n",
    "        sep=\";\", index_col=0\n",
    "    )\n",
    "    # Convert from USD/t to €/MWh, whereby 1 USD ~ 1 €\n",
    "    future_prices[\"hardcoal\"][iter_year] = pd.read_csv(\n",
    "        f'{main_path[\"inputs\"]}{sub_path[\"costs\"]}{input_file[\"hardcoal_futures\"]}{iter_year}.csv', \n",
    "        sep=\";\", index_col=0\n",
    "    ).div(8.141)    \n",
    "    last_future_prices[\"natgas\"][iter_year] = future_prices[\"natgas\"][iter_year].iloc[-1].values[0]\n",
    "    last_future_prices[\"hardcoal\"][iter_year] = future_prices[\"hardcoal\"][iter_year].iloc[-1].values[0]\n",
    "    \n",
    "future_prices[\"oil\"] = pd.read_excel(\n",
    "    f'{main_path[\"inputs\"]}{sub_path[\"costs\"]}{input_file[\"oil_futures\"]}.xlsx', \n",
    "    index_col=0, usecols=\"A:C\"\n",
    ")[\"Preis [USD/bbl]\"].div(159).mul(0.883).mul(41.9).mul(3.6)\n",
    "average_oil_prices = future_prices[\"oil\"].groupby(future_prices[\"oil\"].index.year).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "natgas_prices_ewi = pd.read_excel(\n",
    "    main_path[\"inputs\"] + sub_path[\"costs\"] + input_file[\"prices_ewi\"],\n",
    "    sheet_name=\"natgas\", index_col=0, usecols=\"A:E\"\n",
    ")\n",
    "# add price for 2020 which was around 76% of the price for 2019\n",
    "natgas_prices_ewi.loc[2020] = 0.76 * natgas_prices_ewi.loc[2019]\n",
    "natgas_prices_ewi = natgas_prices_ewi.sort_index()\n",
    "\n",
    "# Integrate last future prices for years 2023 to 2025 as a proxy\n",
    "for iter_year in range(2023, 2026):\n",
    "    natgas_prices_ewi.loc[iter_year] = last_future_prices[\"natgas\"][iter_year]\n",
    "\n",
    "natgas_prices_ewi.sort_index(inplace=True)\n",
    "    \n",
    "# normalize and interpolate\n",
    "natgas_prices_ewi = natgas_prices_ewi.div(natgas_prices_ewi.loc[2020])\n",
    "\n",
    "natgas_prices_ewi_all_years = pd.DataFrame(\n",
    "    index=range(2021, 2051), columns=natgas_prices_ewi.columns\n",
    ")\n",
    "for col in natgas_prices_ewi.columns:\n",
    "    natgas_prices_ewi_all_years[col] = natgas_prices_ewi[col]\n",
    "\n",
    "natgas_prices_ewi_all_years.loc[2045] = 1.0\n",
    "natgas_prices_ewi_all_years = natgas_prices_ewi_all_years.interpolate(how=\"linear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oil_prices_ewi = pd.read_excel(\n",
    "    main_path[\"inputs\"] + sub_path[\"costs\"] + input_file[\"prices_ewi\"],\n",
    "    sheet_name=\"oil\", index_col=0\n",
    ")\n",
    "\n",
    "# normalize and interpolate\n",
    "oil_prices_ewi = oil_prices_ewi.div(oil_prices_ewi.loc[2020])\n",
    "\n",
    "oil_prices_ewi_all_years = pd.DataFrame(\n",
    "    index=range(2021, 2051), columns=oil_prices_ewi.columns\n",
    ")\n",
    "for col in oil_prices_ewi.columns:\n",
    "    oil_prices_ewi_all_years[col] = oil_prices_ewi[col]\n",
    "\n",
    "oil_prices_ewi_all_years.loc[2045] = 1.0\n",
    "oil_prices_ewi_all_years = oil_prices_ewi_all_years.interpolate(how=\"linear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hardcoal_prices_ewi = pd.read_excel(\n",
    "    main_path[\"inputs\"] + sub_path[\"costs\"] + input_file[\"prices_ewi\"],\n",
    "    sheet_name=\"hardcoal\", index_col=0\n",
    ")\n",
    "\n",
    "# Integrate last future prices for years 2023 to 2025 as a proxy\n",
    "for iter_year in range(2023, 2026):\n",
    "    hardcoal_prices_ewi.loc[iter_year] = last_future_prices[\"hardcoal\"][iter_year]\n",
    "\n",
    "hardcoal_prices_ewi.sort_index(inplace=True)\n",
    "    \n",
    "# normalize and interpolate\n",
    "hardcoal_prices_ewi = hardcoal_prices_ewi.div(hardcoal_prices_ewi.loc[2020])\n",
    "\n",
    "hardcoal_prices_ewi_all_years = pd.DataFrame(\n",
    "    index=range(2021, 2051), columns=hardcoal_prices_ewi.columns\n",
    ")\n",
    "for col in hardcoal_prices_ewi.columns:\n",
    "    hardcoal_prices_ewi_all_years[col] = hardcoal_prices_ewi[col]\n",
    "\n",
    "hardcoal_prices_ewi_all_years.loc[2045] = 1.0\n",
    "hardcoal_prices_ewi_all_years = hardcoal_prices_ewi_all_years.interpolate(how=\"linear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_shock_mapping = {\n",
    "    \"high\":\n",
    "    {\n",
    "        \"natgas\": natgas_prices_ewi_all_years[\"mEL-oRU\"],\n",
    "        \"oil\": oil_prices_ewi_all_years[\"oRu\"],\n",
    "        \"hardcoal\": hardcoal_prices_ewi_all_years[\"oRu\"]\n",
    "    },\n",
    "    \"low\":\n",
    "    {\n",
    "        \"natgas\": natgas_prices_ewi_all_years[\"hEL-nRU\"],\n",
    "        \"oil\": oil_prices_ewi_all_years[\"nRu\"],\n",
    "        \"hardcoal\": hardcoal_prices_ewi_all_years[\"nRu\"]\n",
    "    }\n",
    "}\n",
    "price_shock_factors = {}\n",
    "for key, val in price_shock_mapping.items():\n",
    "    price_shock_factors_df = pd.DataFrame(\n",
    "        index=[\"natgas\", \"oil\", \"hardcoal\"],\n",
    "        columns=range(2021, 2051)\n",
    "    )\n",
    "    for f, column in val.items():\n",
    "        price_shock_factors_df.loc[f] = column\n",
    "\n",
    "    price_shock_factors[key] = price_shock_factors_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add hydrogen costs\n",
    "* In **POMMES**, a distinction is made between domestic green hydrogen and a mix of blue, grey and green hydrogen that is imported.\n",
    "* To model this, two different hydrogen \"sources\" are modelled feeding the same bus, but both equiped with different price paths and (average) emission factors. The domestic green hydrogen in turn is produced by electrolyzers that are explicitly modelled.\n",
    "* Generation plants in turn consume from the hydrogen bus which pools all different sorts of hydrogen. Thus, a detailled modelling of hydrogen origin is not taken care of.\n",
    "\n",
    "Hydrogen costs:\n",
    "* As a first rough estimate, we use some rough estimate (in USD/kg H2), loosely based on prognoses from IEA and a recent paper from EWI \n",
    "* Assume IEA value range for 2060 to be reached already in 2050\n",
    "* Use a lower heating value of 33.3 kWh/kg and factor 1 000 for conversion to €/MWh\n",
    "* Include hydrogen cost estimate in all fuel price scenarios.\n",
    "\n",
    "Sources:\n",
    "* IEA (2020): Global average levelised cost of hydrogen production by energy source and technology, 2019 and 2050, https://www.iea.org/data-and-statistics/charts/global-average-levelised-cost-of-hydrogen-production-by-energy-source-and-technology-2019-and-2050, accessed 02.09.2022.\n",
    "* Brändle, Georg, Schönfisch, Max, Schulte, Simon (2020): Estimating Long-Term Global Supply Costs for Low-Carbon Hydrogen, EWI EWI Working Paper, No 20/04, November 2020, \n",
    " https://www.ewi.uni-koeln.de/cms/wp-content/uploads/2021/03/EWI_WP_20-04_Estimating_long-term_global_supply_costs_for_low-carbon_Schoenfisch_Braendle_Schulte-1.pdf, accessed 02.09.2022.\n",
    "\n",
    "**_TODO: Replace first rough estimate with something more sophisticated! https://www.fchobservatory.eu/observatory/technology-and-market/levelised-cost-of-hydrogen-green-hydrogen-costs also seems promising._**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2_costs_2020 = 6\n",
    "h2_costs_2060 = 1.5\n",
    "lower_heating_value_h2 = 33.3\n",
    "\n",
    "h2_costs_ts = pd.DataFrame(\n",
    "    index=pd.date_range(\n",
    "        start=\"2019-01-01\", end=\"2051-01-01\", freq=\"AS\"\n",
    "    ), dtype=\"float64\", columns=[\"real\", \"nominal\"]\n",
    ")\n",
    "h2_costs_ts.at[\"2019-01-01\", \"real\"] = h2_costs_2020 / lower_heating_value_h2 * 1000\n",
    "h2_costs_ts.at[\"2020-01-01\", \"real\"] = h2_costs_2020 / lower_heating_value_h2 * 1000\n",
    "h2_costs_ts.at[\"2050-01-01\", \"real\"] = h2_costs_2060 / lower_heating_value_h2 * 1000\n",
    "h2_costs_ts = h2_costs_ts.interpolate()\n",
    "h2_costs_ts.reset_index(drop=False, inplace=True)\n",
    "h2_costs_ts[\"nominal\"] = h2_costs_ts[\"real\"].mul(inflation_rate ** (h2_costs_ts.index - 1))\n",
    "h2_costs_ts.set_index(\"index\", drop=True, inplace=True)\n",
    "\n",
    "h2_costs_ts.to_csv(\n",
    "    main_path[\"outputs\"] + output_file[\"costs_hydrogen_ts\"] + \".csv\"\n",
    ")\n",
    "# Add \"historic\" hydrogen costs\n",
    "fuel.at[\"hydrogen\", 2019] = h2_costs_ts.at[\"2019-01-01\", \"nominal\"]\n",
    "fuel.at[\"hydrogen\", 2020] = h2_costs_ts.at[\"2020-01-01\", \"nominal\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine to projections per energy carrier (WEO)\n",
    "Assign the projections, whereby method differs dependent on energy carrier:\n",
    "* For oil, natural gas and hard coal, calculate values using the normalized nominal price trend and 2020 price information\n",
    "* For energy carriers with constant cost values, just correct by inflation to derive nominal costs\n",
    "* For mixed fuels, use the average for oil, natural gas, hard coal and biomass\n",
    "* For other fossil, multiply assumed value with the average trend for oil, natural gas and hard coal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fuel_price_projections = dict()\n",
    "fuel_price_projections_real = dict()\n",
    "fuel_prices = dict()\n",
    "fuel_prices_real = dict()\n",
    "\n",
    "for scen in weo_scenarios.values():\n",
    "    for price_level, price_shock_factor in price_shock_factors.items():\n",
    "        # fuels contained in WEO\n",
    "        fuel_pred = pd.DataFrame(index=fuel.index, columns=range(fuel.columns.values[-1] + 1, 2051))\n",
    "        fuel_pred_real = fuel_pred.copy(deep=True)\n",
    "        for f in fuels_for_prediction:\n",
    "            # nominal; WEO estimate and additional price shock\n",
    "            fuel_pred.loc[f] = fuel.at[f, 2020]\n",
    "            fuel_pred.loc[f] = fuel_pred.loc[f].mul(weo_fuel_price_paths_nominal[(f, scen)])\n",
    "            fuel_pred.loc[f] = fuel_pred.loc[f].mul(price_shock_factor.loc[f])\n",
    "            # real; WEO estimate and additional price shock\n",
    "            fuel_pred_real.loc[f] = fuel.at[f, 2020]\n",
    "            fuel_pred_real.loc[f] = fuel_pred_real.loc[f].mul(weo_fuel_price_paths[(f, scen)])\n",
    "            fuel_pred_real.loc[f] = fuel_pred_real.loc[f].mul(price_shock_factor.loc[f])\n",
    "\n",
    "        # fuels that are assigned constant values (corrected by inflation)\n",
    "        fuels_with_constant_costs = [\n",
    "            \"uranium\", \"biomass\", \"water\", \"waste\", \"solarthermal\", \"otherRES\", \"lignite\"\n",
    "        ]\n",
    "        for f in fuels_with_constant_costs:\n",
    "            # nominal\n",
    "            fuel_pred.loc[f] = fuel.at[f, 2020]\n",
    "            fuel_pred.loc[f] = fuel_pred.loc[f].mul(\n",
    "                inflation_rate ** np.arange(len(fuel_pred.columns))\n",
    "            )\n",
    "            # real\n",
    "            fuel_pred_real.loc[f] = fuel.at[f, 2020]\n",
    "\n",
    "        # For mixed fuels, use average    \n",
    "        fuel_pred.loc[\"mixedfuels\"] = fuel_pred.loc[[\"oil\", \"natgas\", \"hardcoal\", \"biomass\"]].mean()\n",
    "        fuel_pred_real.loc[\"mixedfuels\"] = fuel_pred_real.loc[[\"oil\", \"natgas\", \"hardcoal\", \"biomass\"]].mean()\n",
    "\n",
    "        # For otherfossil, use average price trend to increment assumption\n",
    "        # nominal\n",
    "        average_fossil_price_trend = weo_fuel_price_paths_nominal.loc[:,\n",
    "            weo_fuel_price_paths_nominal.columns[\n",
    "                weo_fuel_price_paths_nominal.columns.get_level_values(1) == scen\n",
    "            ]\n",
    "        ].mean(axis=1)\n",
    "        fuel_pred.loc[\"otherfossil\"] = fuel.at[\"otherfossil\", 2020]\n",
    "        fuel_pred.loc[\"otherfossil\"] = fuel_pred.loc[\"otherfossil\"].mul(average_fossil_price_trend)\n",
    "        # add hydrogen price assumption\n",
    "        fuel_pred.loc[\"hydrogen\"] = h2_costs_ts.loc[\"2021-01-01\":\"2050-01-01\", \"nominal\"].values\n",
    "        \n",
    "        # real\n",
    "        average_fossil_price_trend_real = weo_fuel_price_paths.loc[:,\n",
    "            weo_fuel_price_paths.columns[\n",
    "                weo_fuel_price_paths.columns.get_level_values(1) == scen\n",
    "            ]\n",
    "        ].mean(axis=1)\n",
    "        fuel_pred_real.loc[\"otherfossil\"] = fuel.at[\"otherfossil\", 2020]\n",
    "        fuel_pred_real.loc[\"otherfossil\"] = fuel_pred_real.loc[\"otherfossil\"].mul(average_fossil_price_trend_real)\n",
    "        # add hydrogen price assumption\n",
    "        fuel_pred_real.loc[\"hydrogen\"] = h2_costs_ts.loc[\"2021-01-01\":\"2050-01-01\", \"real\"].values\n",
    "\n",
    "        fuel_price_projections[scen+\"_\"+price_level] = fuel_pred.astype(\"float64\")\n",
    "        fuel_price_projections_real[scen+\"_\"+price_level] = fuel_pred_real.astype(\"float64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine to projections per energy carrier (regression)\n",
    "* Perform a linear regression for oil, natural gas and hard coal as an alternative to using WEO price scenarios\n",
    "* Combine prognosis and historical data and reshape fuel data to match the commodity sources\n",
    "* Write the fuel prices to csv files (one per price scenario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert historical fuel prices to real terms\n",
    "fuel_real = fuel.copy()\n",
    "for iter_year in fuel_real.columns:\n",
    "    fuel_real[iter_year] = fuel[iter_year].div(inflation_rate ** (iter_year - 2020))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do a linear regression as fossil fuels price estimate\n",
    "prediction_horizon = range(fuel.columns.values[-1] + 1, 2051)\n",
    "fuel_pred = pd.DataFrame(index=prediction_horizon)\n",
    "fuel_pred_real = fuel_pred.copy(deep=True)\n",
    "\n",
    "for price_level, price_shock_factor in price_shock_factors.items():\n",
    "    for f in fuels_for_prediction:\n",
    "        # nominal; Regression plus additional price shock\n",
    "        X = fuel.columns.astype(\"float64\").values\n",
    "        Y = fuel.loc[f].astype(\"float64\").values\n",
    "\n",
    "        X = sm.add_constant(X)\n",
    "\n",
    "        fuel_regression = sm.OLS(Y, X).fit()\n",
    "\n",
    "        X_pred = sm.add_constant(prediction_horizon)\n",
    "        price_prediction = fuel_regression.predict(X_pred)\n",
    "        fuel_pred[f] = price_prediction\n",
    "        fuel_pred[f] = price_prediction * price_shock_factor.loc[f].values\n",
    "\n",
    "        # real; Regression plus additional price shock\n",
    "        X = fuel_real.columns.astype(\"float64\").values\n",
    "        Y = fuel_real.loc[f].astype(\"float64\").values\n",
    "\n",
    "        X = sm.add_constant(X)\n",
    "\n",
    "        fuel_regression = sm.OLS(Y, X).fit()\n",
    "\n",
    "        X_pred = sm.add_constant(prediction_horizon)\n",
    "        price_prediction = fuel_regression.predict(X_pred)\n",
    "        fuel_pred_real[f] = price_prediction\n",
    "        fuel_pred_real[f] = price_prediction * price_shock_factor.loc[f].values\n",
    "\n",
    "    # Add remainder (same as above)\n",
    "    for f in fuels_with_constant_costs:\n",
    "        fuel_pred[f] = fuel.at[f, 2020]\n",
    "        fuel_pred[f] = fuel_pred[f].mul(\n",
    "            inflation_rate ** np.arange(1, len(fuel_pred.index) + 1)\n",
    "        )\n",
    "        fuel_pred_real[f] = fuel.at[f, 2020]\n",
    "\n",
    "    # For mixed fuels, use average    \n",
    "    fuel_pred[\"mixedfuels\"] = fuel_pred[[\"oil\", \"natgas\", \"hardcoal\", \"biomass\"]].mean(axis=1)\n",
    "    fuel_pred_real[\"mixedfuels\"] = fuel_pred_real[[\"oil\", \"natgas\", \"hardcoal\", \"biomass\"]].mean(axis=1)\n",
    "\n",
    "    # For otherfossil, use average price trend to increment assumption\n",
    "    average_fossil_price_trend = fuel_pred.div(fuel.loc[fuels_for_prediction, 2020]).mean(axis=1)\n",
    "    average_fossil_price_trend_real = fuel_pred_real.div(fuel_real.loc[fuels_for_prediction, 2020]).mean(axis=1)\n",
    "\n",
    "    fuel_pred[\"otherfossil\"] = fuel.at[\"otherfossil\", 2020]\n",
    "    fuel_pred[\"otherfossil\"] = fuel_pred[\"otherfossil\"].mul(average_fossil_price_trend)\n",
    "\n",
    "    fuel_pred_real[\"otherfossil\"] = fuel_real.at[\"otherfossil\", 2020]\n",
    "    fuel_pred_real[\"otherfossil\"] = fuel_pred_real[\"otherfossil\"].mul(average_fossil_price_trend_real)\n",
    "    \n",
    "    # add hydrogen price assumption\n",
    "    fuel_pred[\"hydrogen\"] = h2_costs_ts.loc[\"2021-01-01\":\"2050-01-01\", \"nominal\"].values\n",
    "    fuel_pred_real[\"hydrogen\"] = h2_costs_ts.loc[\"2021-01-01\":\"2050-01-01\", \"real\"].values\n",
    "\n",
    "    fuel_price_projections[\"regression_\" + price_level] = fuel_pred.T.astype(\"float64\")\n",
    "    fuel_price_projections_real[\"regression_\" + price_level] = fuel_pred_real.T.astype(\"float64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for scen, projection in fuel_price_projections.items():\n",
    "    costs_fuel = pd.concat([fuel[range(2017, 2021)].astype(\"float64\"), projection], axis=1)\n",
    "    fuel_prices[scen] = costs_fuel\n",
    "    costs_fuel = tools.reformat_costs_values(costs_fuel, all_sources_commodity, index=\"source\")\n",
    "    costs_fuel.to_csv(\n",
    "        (\n",
    "            main_path[\"outputs\"] + output_file[\"costs_fuel\"] \n",
    "            + \"_\" + str(scen) + \"_nominal_\" + str(year) + \".csv\"\n",
    "        )\n",
    "    )\n",
    "    costs_fuel.to_excel(writer, sheet_name='costs_fuel' + \"_\" + str(scen) + '_nominal')\n",
    "    \n",
    "    # Transform to time series\n",
    "    costs_fuel_ts = tools.transform_values_to_annual_time_series(costs_fuel)\n",
    "    costs_fuel_ts.to_csv(\n",
    "        main_path[\"outputs\"] + output_file[\"costs_fuel\"] \n",
    "            + \"_\" + str(scen) + \"_nominal_indexed_ts.csv\"\n",
    "    )\n",
    "    \n",
    "    # Include fuel price sensitivities\n",
    "    for sensitivity, multiplier in sensitivities.items():\n",
    "        costs_fuel_ts_sensitivity = costs_fuel_ts * multiplier\n",
    "        costs_fuel_ts_sensitivity.to_csv(\n",
    "            main_path[\"outputs\"] + output_file[\"costs_fuel\"] \n",
    "            + \"_\" + str(scen) + f\"_nominal_indexed_ts_sensitivity_{sensitivity}.csv\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for scen, projection in fuel_price_projections_real.items():\n",
    "    costs_fuel_real = pd.concat([fuel_real[range(2017, 2021)].astype(\"float64\"), projection], axis=1)\n",
    "    fuel_prices_real[scen] = costs_fuel_real\n",
    "    costs_fuel_real = tools.reformat_costs_values(costs_fuel_real, all_sources_commodity, index=\"source\")\n",
    "    costs_fuel_real.to_csv(\n",
    "        (\n",
    "            main_path[\"outputs\"] + output_file[\"costs_fuel\"] \n",
    "            + \"_\" + str(scen) + \"_real_\" + str(year) + \".csv\"\n",
    "        )\n",
    "    )\n",
    "    costs_fuel_real.to_excel(writer, sheet_name='costs_fuel' + \"_\" + str(scen) + '_real')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "costs_plot = costs_fuel_real.loc[costs_fuel_real.index.str.contains(\"DE_\")]\n",
    "_ = costs_plot.T.plot()\n",
    "_ = plt.legend(bbox_to_anchor=[1.1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Include price variations within years (dispatch model)\n",
    "Include monthly pattern for historical years from DESTATIS data:\n",
    "* Use normalized time series: 1 is the average fuel price value for the particular year.\n",
    "* For future years, prices are constant, i.e. all values are equal to 1.\n",
    "* For fuels other than hardcoal, lignite, natgas and oil, constant fuel prices are used.\n",
    "\n",
    "> _Note: Using the respective year for normalization restricts pommesdispatch simulations to be within one year!_\n",
    "\n",
    "DESTATIS (2022): Prices. Data on energy price trends. - Long-time series from January 2005 to January 2022 -, https://www.destatis.de/EN/Themes/Economy/Prices/Publications/Downloads-Energy-Price-Trends/energy-price-trends-pdf-5619002.pdf?__blob=publicationFile, as of February 2022, accessed 25.02.2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_price_pattern = {\n",
    "    \"hardcoal\": \"5.1 Hard coal and lignite\",\n",
    "    \"lignite\": \"5.1 Hard coal and lignite\",\n",
    "    \"oil\": \"5.2 Mineral oil\",\n",
    "    \"natgas\": \"5.3.1 Natural gas - indices\"\n",
    "}\n",
    "price_timeseries = pd.DataFrame(\n",
    "    index=pd.date_range(start=f\"2017-01-01\", end=f\"2050-12-31\", freq=\"H\")\n",
    ")\n",
    "\n",
    "for energy_source, sheet in monthly_price_pattern.items():\n",
    "    monthly_price_trends = pd.read_excel(\n",
    "        main_path[\"inputs\"] + sub_path[\"costs\"] + input_file[\"monthly_price_trends\"],\n",
    "        sheet_name=sheet, skiprows=3, index_col=0, nrows=20\n",
    "    )\n",
    "    # Format & slice\n",
    "    monthly_price_trends = monthly_price_trends.dropna(thresh=2).rename(\n",
    "        columns={\"Unnamed: 13\": \"yearly_average\"}\n",
    "    )\n",
    "    monthly_price_trends.index = (\n",
    "        monthly_price_trends.index.str.replace(\" ...\", \"\").astype(int)\n",
    "    )\n",
    "    monthly_price_trends = monthly_price_trends.loc[range(2017, 2022)]\n",
    "\n",
    "    # Create normalized pattern\n",
    "    monthly_price_trends = (\n",
    "        monthly_price_trends.div(\n",
    "            monthly_price_trends[\"yearly_average\"], axis=0\n",
    "        )\n",
    "    )\n",
    "    monthly_price_trends.drop(columns=\"yearly_average\", inplace=True)\n",
    "\n",
    "    time_series_to_concat = []\n",
    "\n",
    "    for iter_year in monthly_price_trends.index:\n",
    "        time_series = pd.DataFrame(\n",
    "            index=pd.date_range(\n",
    "                start=f\"{iter_year}-01-01\", end=f\"{iter_year}-12-31\", freq=\"MS\"\n",
    "            ),\n",
    "            data={\n",
    "                energy_source:\n",
    "                    monthly_price_trends.loc[iter_year].values\n",
    "            }\n",
    "        )\n",
    "        time_series_to_concat.append(time_series)\n",
    "    \n",
    "    full_time_series = pd.concat([ts for ts in time_series_to_concat])\n",
    "    constant_vals = pd.DataFrame(index=pd.date_range(\n",
    "        start=\"2022-01-01\", end=\"2051-01-01\", freq=\"MS\"\n",
    "    ), data=1, columns=[energy_source])\n",
    "    full_time_series = pd.concat([full_time_series, constant_vals])\n",
    "    price_timeseries[energy_source] = full_time_series.resample(\"H\").ffill()[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add missing fuels and match with commodity sources and write to csv\n",
    "price_timeseries[[\n",
    "    \"uranium\", \"biomass\", \"water\", \"waste\", \n",
    "    \"solarthermal\", \"otherRES\", \"mixedfuels\", \"otherfossil\", \"hydrogen\"\n",
    "]] = 1\n",
    "price_timeseries = price_timeseries.loc[:,all_sources_commodity['technology'].values]\n",
    "price_timeseries = price_timeseries.astype(np.float32).round(2)\n",
    "price_timeseries.columns = all_sources_commodity.index\n",
    "\n",
    "price_timeseries = price_timeseries.loc[price_timeseries.index.year == year]\n",
    "\n",
    "price_timeseries.to_csv(\n",
    "    main_path[\"outputs\"] + output_file[\"costs_fuel_ts\"] + \"_\" + str(year) + \".csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emissions costs\n",
    "* Historical costs are obtained from the primary auction results at EEX.\n",
    "* Projections are derived from a background document to the Ariadne Scenario report as well as a recent paper by Pietzcker et al.\n",
    "\n",
    "Literature:\n",
    "* EEX (2017-2021): Emission Spot Primary Market Auction Report 2017-2021, https://www.eex.com/en/market-data/environmental-markets/eua-primary-auction-spot-download, accessed 28.02.2022.\n",
    "* Pietzcker, Robert, Knopf, Brigitte, Osorio, Sebastian, Edenhofer, Ottmar et al. (2021): Ariadne-Hintergrund. Notwendige CO2-Preise zum Erreichen des europäischen Klimaziels 2030, issued by PIK, November 2021, https://doi.org/10.48485/pik.2021.007.\n",
    "* Pietzcker, Robert, Osorio, Sebastian, Rodrigues, Renato (2021): Tightening EU ETS targets in line with the European Green Deal: Impacts on the decarbonization of the EU power sector, in: Applied Energy 293 (2021), https://doi.org/10.1016/j.apenergy.2021.116914.\n",
    "\n",
    "### Process historical EUA price data\n",
    "Steps applied:\n",
    "* Read in primary market auction reports, filter for non EAA and non-Polish data and slice auction price and volume\n",
    "* Calculate volume-weigthed averages and store them in a DataFrame\n",
    "* Derive normalized time series from auction data. The volume-weighted average of each year is used for normalization\n",
    "* Interpolate linearly. Use assumption that future prices do not vary in time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emissions_costs_ts_to_concat = []\n",
    "emissions_costs = pd.DataFrame(\n",
    "    index=all_sources_commodity.index,\n",
    "    columns=range(2017, 2022),\n",
    ")\n",
    "\n",
    "for iter_year in range(2017, 2022):\n",
    "    emissions_costs_ts = pd.read_excel(\n",
    "        main_path[\"inputs\"] + sub_path[\"costs\"] + input_file[\"emissions_costs\"] + f\"-{iter_year}-data.xlsx\", \n",
    "        index_col=0, skiprows=5, usecols=\"B:V\"\n",
    "    )\n",
    "    emissions_costs_ts = emissions_costs_ts.loc[\n",
    "        (emissions_costs_ts[\"Contract\"] != \"EAA3\")\n",
    "        & ~(\n",
    "            emissions_costs_ts[\"Auction Name\"].isin([\n",
    "                \"Auction 4. Period CAP3 PL\",\n",
    "                'Auction 3. Period CAP2-PL',\n",
    "                \"Auction 3. Period PL\"\n",
    "            ])\n",
    "        )\n",
    "    ].sort_index()\n",
    "    emissions_costs_ts = emissions_costs_ts[[\n",
    "        \"Auction Price €/tCO2\", \"Auction Volume tCO2\"\n",
    "    ]].rename(\n",
    "        columns={\n",
    "            \"Auction Price €/tCO2\": \"price\",\n",
    "            \"Auction Volume tCO2\": \"volume\"\n",
    "        }\n",
    "    ).dropna(axis=0)\n",
    "    # Calculate volume-weigthed average and store it as annual mean price\n",
    "    wm = lambda x: np.average(x, weights=emissions_costs_ts.loc[x.index, 'volume'])\n",
    "    volume_weighted_mean_price = emissions_costs_ts.apply(wm).loc[\"price\"]\n",
    "    emissions_costs[iter_year] = volume_weighted_mean_price\n",
    "    \n",
    "    emissions_costs_ts = emissions_costs_ts[\"price\"].div(volume_weighted_mean_price)\n",
    "    \n",
    "    emissions_costs_ts_to_concat.append(emissions_costs_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert emissions costs to real terms\n",
    "emissions_costs_real = emissions_costs.copy(deep=True)\n",
    "for iter_year in emissions_costs_real.columns:\n",
    "    emissions_costs_real[iter_year] = emissions_costs[iter_year].div(inflation_rate ** (iter_year - 2020))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat, add artifical elements at the beginning and end and of the time series and convert to hourly frequency\n",
    "emissions_costs_ts = pd.concat([ec for ec in emissions_costs_ts_to_concat])\n",
    "emissions_costs_ts.loc[pd.Timestamp(\"2017-01-01\")] = emissions_costs_ts.iloc[0]\n",
    "emissions_costs_ts = emissions_costs_ts.sort_index()\n",
    "emissions_costs_ts.loc[pd.Timestamp(\"2022-01-01\")] = emissions_costs_ts.iloc[-1]\n",
    "emissions_costs_ts = emissions_costs_ts.resample(\"H\").interpolate()[:-1]\n",
    "emissions_costs_ts\n",
    "\n",
    "constant_vals = pd.Series(index=pd.date_range(\n",
    "    start=\"2022-01-01\", end=\"2050-12-23\", freq=\"H\"\n",
    "), data=1, name=\"price\")\n",
    "emissions_costs_ts = pd.concat([emissions_costs_ts, constant_vals]).astype(np.float32).round(3)\n",
    "\n",
    "# Slice the values for the simulation year\n",
    "emissions_costs_ts = emissions_costs_ts.loc[emissions_costs_ts.index.year == year]\n",
    "\n",
    "emissions_costs_ts.to_csv(\n",
    "    main_path[\"outputs\"] + output_file[\"costs_emissions_ts\"] + \"_\" + str(year) + \".csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add CO2 price projections\n",
    "\n",
    "The following CO2 price projections are included. For the first three ones, there is a low, medium and high estimate, while for the one derived from the Pietzcker et al. paper, only the medium estimate is given:\n",
    "\n",
    "| projection | explanation | characteristics |\n",
    "| ---- | ---- | ---- |\n",
    "| Fit_for_55_split (Ff55_split) | emissions split between ETS and non-ETS according to Fit for 55 package plans | low ETS prices, but high non-ETS prices until 2030 |\n",
    "| reductions_in_ETS_only (red_ETS) | reductions to meeting Fit for 55 emissions targets are brought by ETS emissions only | high ETS price until 2030 |\n",
    "| ESR_reduced (ESR_red) | Emissions in non-ETS (ESR: effort sharing regulation) are reduced compared to Fit for 55 goal (intermediate pathway) | medium ETS price until 2030 |\n",
    "| long-term | Price pathway from paper Pietzcker et al. 2021 conform with Fit for 55 goal | low ETS prices, projection until 2050 |\n",
    "\n",
    "Steps applied:\n",
    "* Read in projected values (from Ariadne background report and paper Pietzcker et al. 2021)\n",
    "* Convert to nominal terms\n",
    "* Assume linear development to derive scenarios until 2030 resp. 2050 (Pietzcker et al. 2021 only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emissions_costs_ariadne = pd.read_csv(\n",
    "    main_path[\"inputs\"] + sub_path[\"costs\"] + input_file[\"emission_costs_projection\"], \n",
    "    index_col=[0, 1], sep=\";\"\n",
    ")\n",
    "# Convert to nominal terms\n",
    "emissions_costs_ariadne[\"nominal_term_value_2030\"] = (\n",
    "    emissions_costs_ariadne[\"real_term_value_2030\"] * inflation_rate ** (2030 - 2021)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emissions_costs_projections = dict()\n",
    "emissions_costs_projections_normalized = dict()\n",
    "emissions_costs_projections_real = dict()\n",
    "\n",
    "for scen in emissions_costs_ariadne.index:\n",
    "    # nominal\n",
    "    emissions_costs_pred = pd.DataFrame(\n",
    "        index=all_sources_commodity.index,\n",
    "        columns=range(2021, 2031)\n",
    "    )\n",
    "    emissions_costs_pred[2021] = emissions_costs[2021]\n",
    "    emissions_costs_pred[2030] = emissions_costs_ariadne.at[\n",
    "        scen, \"nominal_term_value_2030\"\n",
    "    ]\n",
    "    emissions_costs_pred = (\n",
    "        emissions_costs_pred.astype(\"float64\").interpolate(axis=1).round(2)[range(2022, 2031)]\n",
    "    )\n",
    "    emissions_costs_projections[scen] = pd.concat(\n",
    "        [emissions_costs, emissions_costs_pred],\n",
    "        axis=1\n",
    "    ).round(2)\n",
    "    \n",
    "    emissions_costs_projections[scen].to_csv(\n",
    "        (\n",
    "            main_path[\"outputs\"] + output_file[\"costs_emissions\"] \n",
    "            + \"_\" + scen[0] + \"_\" + scen[1] + \"_nominal_\" + str(year) + \".csv\"\n",
    "        )\n",
    "    )\n",
    "    emissions_costs_projections[scen].to_excel(\n",
    "        writer, sheet_name='c_emis' + \"_\" + scen[0] + \"_\" + scen[1] + '_nominal'\n",
    "    )\n",
    "\n",
    "    # Transform to time series\n",
    "    emissions_costs_projections_normalized[scen] = (\n",
    "        tools.transform_values_to_annual_time_series(emissions_costs_projections[scen], end_year=2030)\n",
    "    )\n",
    "    emissions_costs_projections_normalized[scen].to_csv(\n",
    "        main_path[\"outputs\"] + output_file[\"costs_emissions\"] \n",
    "            + \"_\" + scen[0] + \"_\" + scen[1] + \"_nominal_indexed_ts.csv\"\n",
    "    )\n",
    "\n",
    "    # Include emissions costs sensitivities\n",
    "    for sensitivity, multiplier in sensitivities.items():\n",
    "        emissions_costs_projections_normalized_sensitivity = (\n",
    "            emissions_costs_projections_normalized[scen] * multiplier\n",
    "        )\n",
    "        emissions_costs_projections_normalized_sensitivity.to_csv(\n",
    "            main_path[\"outputs\"] + output_file[\"costs_emissions\"] \n",
    "            + \"_\" + scen[0] + \"_\" + scen[1] + f\"_nominal_indexed_ts_sensitivity_{sensitivity}.csv\"\n",
    "        )\n",
    "    \n",
    "    # real\n",
    "    emissions_costs_pred_real = pd.DataFrame(\n",
    "        index=all_sources_commodity.index,\n",
    "        columns=range(2021, 2031)\n",
    "    )\n",
    "    emissions_costs_pred_real[2021] = emissions_costs[2021]\n",
    "    emissions_costs_pred_real[2030] = emissions_costs_ariadne.at[\n",
    "        scen, \"real_term_value_2030\"\n",
    "    ]\n",
    "    emissions_costs_pred_real = (\n",
    "        emissions_costs_pred_real.astype(\"float64\").interpolate(axis=1).round(2)[range(2022, 2031)]\n",
    "    )\n",
    "    emissions_costs_projections_real[scen] = pd.concat(\n",
    "        [emissions_costs_real, emissions_costs_pred_real],\n",
    "        axis=1\n",
    "    ).round(2)\n",
    "    \n",
    "    emissions_costs_projections_real[scen].to_csv(\n",
    "        (\n",
    "            main_path[\"outputs\"] + output_file[\"costs_emissions\"] \n",
    "            + \"_\" + scen[0] + \"_\" + scen[1] + \"_real_\" + str(year) + \".csv\"\n",
    "        )\n",
    "    )\n",
    "    emissions_costs_projections_real[scen].to_excel(\n",
    "        writer, sheet_name='c_emis' + \"_\" + scen[0] + \"_\" + scen[1] + '_real'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emissions_costs_pietzcker = pd.read_csv(\n",
    "    main_path[\"inputs\"] + sub_path[\"costs\"] + input_file[\"emission_costs_long-term_projection\"], \n",
    "    index_col=0, sep=\";\"\n",
    ")\n",
    "# Convert to nominal terms\n",
    "emissions_costs_pietzcker[\"nominal_term_value\"] = (\n",
    "    emissions_costs_pietzcker[\"real_term_value\"]\n",
    "    * inflation_rate ** (emissions_costs_pietzcker.index - 2021)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nominal\n",
    "emissions_costs_pred = pd.DataFrame(\n",
    "    index=all_sources_commodity.index,\n",
    "    columns=range(2021, 2051)\n",
    ")\n",
    "emissions_costs_pred[2021] = emissions_costs[2021]\n",
    "\n",
    "for index in emissions_costs_pietzcker.index:\n",
    "    emissions_costs_pred[index] = emissions_costs_pietzcker.at[\n",
    "        index, \"nominal_term_value\"\n",
    "    ]\n",
    "emissions_costs_pred = (\n",
    "    emissions_costs_pred.astype(\"float64\").interpolate(axis=1).round(2)[range(2022, 2051)]\n",
    ")\n",
    "emissions_costs_projections[\"long-term\"] = pd.concat(\n",
    "    [emissions_costs, emissions_costs_pred],\n",
    "    axis=1\n",
    ").round(2)\n",
    "\n",
    "emissions_costs_projections[\"long-term\"].to_csv(\n",
    "    main_path[\"outputs\"] + output_file[\"costs_emissions\"] + \"_long-term_nominal_\" + str(year) + \".csv\"\n",
    ")\n",
    "emissions_costs_projections[\"long-term\"].to_excel(\n",
    "    writer, sheet_name='costs_emis_long-term_nominal'\n",
    ")\n",
    "\n",
    "# Transform to time series\n",
    "emissions_costs_projections_normalized[\"long-term\"] = (\n",
    "    tools.transform_values_to_annual_time_series(emissions_costs_projections[\"long-term\"])\n",
    ")\n",
    "emissions_costs_projections_normalized[\"long-term\"].to_csv(\n",
    "    main_path[\"outputs\"] + output_file[\"costs_emissions\"] \n",
    "        + \"_long-term_nominal_indexed_ts.csv\"\n",
    ")\n",
    "\n",
    "# Include emissions costs sensitivities\n",
    "for sensitivity, multiplier in sensitivities.items():\n",
    "    emissions_costs_projections_normalized_sensitivity = (\n",
    "        emissions_costs_projections_normalized[\"long-term\"] * multiplier\n",
    "    )\n",
    "    emissions_costs_projections_normalized_sensitivity.to_csv(\n",
    "        main_path[\"outputs\"] + output_file[\"costs_emissions\"] \n",
    "        + f\"_long-term_nominal_indexed_ts_sensitivity_{sensitivity}.csv\"\n",
    "    )\n",
    "\n",
    "# real\n",
    "emissions_costs_pred_real = pd.DataFrame(\n",
    "    index=all_sources_commodity.index,\n",
    "    columns=range(2021, 2051)\n",
    ")\n",
    "emissions_costs_pred_real[2021] = emissions_costs[2021]\n",
    "\n",
    "for index in emissions_costs_pietzcker.index:\n",
    "    emissions_costs_pred_real[index] = emissions_costs_pietzcker.at[\n",
    "        index, \"real_term_value\"\n",
    "    ]\n",
    "emissions_costs_pred_real = (\n",
    "    emissions_costs_pred_real.astype(\"float64\").interpolate(axis=1).round(2)[range(2022, 2051)]\n",
    ")\n",
    "emissions_costs_projections_real[\"long-term\"] = pd.concat(\n",
    "    [emissions_costs_real, emissions_costs_pred_real],\n",
    "    axis=1\n",
    ").round(2)\n",
    "\n",
    "emissions_costs_projections_real[\"long-term\"].to_csv(\n",
    "    main_path[\"outputs\"] + output_file[\"costs_emissions\"] + \"_long-term_real_\" + str(year) + \".csv\"\n",
    ")\n",
    "emissions_costs_projections_real[\"long-term\"].to_excel(\n",
    "    writer, sheet_name='costs_emis_long-term_real'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investment expenses\n",
    "Steps applied:\n",
    "* Read in and reshape data\n",
    "* Separate data for storage units which is considered in the next section.\n",
    "* Inflation is not considered since\n",
    "    * it is way outperformed by data insecurities and\n",
    "    * data sets are all from 2020/21 and thus depict the status quo estimates.\n",
    "* If study does not explicitly consider hydrogen generation, assume it to have the same costs as natural gas.\n",
    "\n",
    "Cost assumptions for investment expenses are taken from:\n",
    "* Pietzcker, Robert, Osorio, Sebastian, Rodrigues, Renato (2021): Tightening EU ETS targets in line with the European Green Deal: Impacts on the decarbonization of the EU power sector, in: Applied Energy 293 (2021), https://doi.org/10.1016/j.apenergy.2021.\n",
    "* Fraunhofer ISE (2020): Appendix to the study \"Wege zu einem klimaneutralen Energiesystem. Die deutsche Energiewende im Kontext gesellschaftlicher Verhaltensweisen.\", https://www.ise.fraunhofer.de/de/veroeffentlichungen/studien/wege-zu-einem-klimaneutralen-energiesystem.html, accessed 08.07.2022.\n",
    "* PyPSA-EUR assumptions on costs compiled from various (primary) sources: https://github.com/PyPSA/pypsa-eur/blob/master/data/costs.csv, accessed 01.07.2022.\n",
    "* dieterpy input data compiled from various (primary) sources: https://gitlab.com/diw-evu/dieter_public/dieterpy_reduced/-/blob/main/input/data_input.xlsx, accessed 01.07.2022.\n",
    "* A data set from the research project FlexMex, https://zenodo.org/record/5802178, including data compiled from various (primary) sources, accessed 01.07.2022.\n",
    "* As well as a collection of data from various (primary) sources from the research project UNSEEN (as of 03/2021).\n",
    "\n",
    "### Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "investment_expenses_pietzcker = pd.read_excel(\n",
    "    main_path[\"inputs\"] + sub_path[\"assumptions\"] \n",
    "    + input_file[\"pietzcker_technology_assumptions\"],\n",
    "    sheet_name=\"investment_expenses\",\n",
    "    index_col=0, header=0\n",
    ").T\n",
    "investment_expenses_pietzcker.rename(\n",
    "    index={\n",
    "        'Hard Coal CCS': 'ST_CCS_hardcoal', \n",
    "        'Lignite CCS': 'ST_CCS_lignite', \n",
    "        'Gas CC CCS': 'CC_CCS_natgas', \n",
    "        'Hydrogen FC': 'FC_hydrogen', \n",
    "        'BECCS': 'beccs',\n",
    "        'Wind Onshore': 'windonshore', \n",
    "        'Wind Offshore': 'windoffshore', \n",
    "        'PV': 'solarPV',\n",
    "        'CSP': 'csp',\n",
    "    },\n",
    "    inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_assumptions_pietzcker = pd.read_excel(\n",
    "    main_path[\"inputs\"] + sub_path[\"assumptions\"] \n",
    "    + input_file[\"pietzcker_technology_assumptions\"],\n",
    "    sheet_name=\"generation\",\n",
    "    index_col=0, header=[0, 1]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pietzcker_constant_costs = generation_assumptions_pietzcker[\n",
    "    [(\"Investment costs\", \"(€/kW)\")]].loc[\n",
    "        generation_assumptions_pietzcker[(\"Investment costs\", \"(€/kW)\")].notna()\n",
    "].rename(\n",
    "    {\n",
    "        'Nuclear': 'ST_uranium', \n",
    "        'Hard Coal': 'ST_hardcoal', \n",
    "        'Lignite': 'ST_lignite',\n",
    "        'Gas CC': 'CC_natgas', \n",
    "        'Gas CT': 'GT_natgas', \n",
    "        'Oil': 'ST_oil',\n",
    "        'Hydrogen CC': 'CC_hydrogen', \n",
    "        'Hydrogen CT': 'GT_hydrogen', \n",
    "        'Waste': 'ST_waste', \n",
    "        'Other gases': 'ST_otherfossil', \n",
    "        'Biomass': 'GT_biomass',\n",
    "        'Hydro': 'ror'\n",
    "    }\n",
    ")\n",
    "pietzcker_constant_costs.columns = [2020]\n",
    "pietzcker_constant_costs[2050] = pietzcker_constant_costs[2020]\n",
    "investment_expenses_pietzcker = pd.concat(\n",
    "    [investment_expenses_pietzcker, pietzcker_constant_costs]\n",
    ")\n",
    "investment_expenses_pietzcker[\"study_title\"] = \"Pietzcker_2021\"\n",
    "investment_expenses_pietzcker.set_index(\"study_title\", append=True, inplace=True)\n",
    "investment_expenses_pietzcker.interpolate(how=\"linear\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ise_generators = pd.read_excel(\n",
    "    main_path[\"inputs\"] + sub_path[\"assumptions\"] + input_file[\"ise_technology_assumptions\"],\n",
    "    sheet_name=\"generation\",\n",
    "    index_col=[0, 1, 2], header=0,\n",
    ")\n",
    "\n",
    "investment_expenses_ise = (\n",
    "    ise_generators.loc[ise_generators.index.get_level_values(1) == \"Investition \"]\n",
    ")\n",
    "investment_expenses_ise = investment_expenses_ise.reset_index(\n",
    "    level=[1, 2], drop=True\n",
    ").rename(index={\n",
    "    \"Wind Offshore \": \"windoffshore\",\n",
    "    \"Wind Onshore \": \"windonshore\",\n",
    "    \"Photovoltaik Dach Süd\": \"solarPV\",\n",
    "    \"Photovoltaik Dach Ost-West\": \"solarPV\",\n",
    "    \"Photovoltaik Freifläche Süd\": \"solarPV\",\n",
    "    \"GuD-Kraftwerke \": \"CC_natgas\",\n",
    "    \"Gasturbine \": \"GT_natgas\",\n",
    "    \"Braunkohlekraftwerk \": \"ST_lignite\",\n",
    "    \"Steinkohlekraftwerk \": \"ST_hardcoal\",\n",
    "    \"H2-Rückverstromung (Gasturbine)\": \"GT_hydrogen\"\n",
    "})\n",
    "investment_expenses_ise[\"study_title\"] = \"ISE_2020\"\n",
    "investment_expenses_ise.set_index(\"study_title\", append=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dieterpy_conventional_data = pd.read_excel(\n",
    "    main_path[\"inputs\"] + sub_path[\"assumptions\"] \n",
    "    + input_file[\"dieterpy_data\"],\n",
    "    sheet_name=\"con\",\n",
    "    index_col=0, header=[0, 1, 2], skiprows=2, nrows=4\n",
    ").dropna(how=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slice investments and convert from €/MW to €/kW\n",
    "dieterpy_investments = dieterpy_conventional_data.rename(index={\n",
    "    \"coal\": \"ST_hardcoal\",\n",
    "    \"ocgt\": \"GT_natgas\",\n",
    "    \"ccgt\": \"CC_natgas\",\n",
    "})[[('Overnight investment costs per MW ', '[EUR/MW]', 'oc')]].div(1000)\n",
    "dieterpy_investments.columns = [2020]\n",
    "for iter_year in range(2021, 2051):\n",
    "    dieterpy_investments[iter_year] = dieterpy_investments[2020]\n",
    "dieterpy_investments\n",
    "dieterpy_investments[\"study_title\"] = \"DIETER_2021\"\n",
    "dieterpy_investments.set_index(\"study_title\", append=True, inplace=True)\n",
    "\n",
    "# Add hydrogen costs\n",
    "dieterpy_investments = tools.add_hydrogen_cost_assumption(dieterpy_investments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flexmex_data = pd.read_csv(\n",
    "    main_path[\"inputs\"] + sub_path[\"assumptions\"] \n",
    "    + input_file[\"flexmex_data\"],\n",
    "    index_col=0, header=0\n",
    ")\n",
    "flexmex_investments = flexmex_data.loc[\n",
    "    (flexmex_data[\"Parameter\"].str.contains(\"Capex\")) \n",
    "    & (flexmex_data[\"Parameter\"].str.contains(\"EnergyConversion\")) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flexmex_investments = pd.concat(\n",
    "    [\n",
    "        flexmex_investments,\n",
    "        flexmex_investments[\"Parameter\"].str.split(\"_\", expand=True)\n",
    "    ], axis=1)\n",
    "flexmex_dict = {\n",
    "    \"CH4\": \"natgas\",\n",
    "    \"Nuclear\": \"uranium\",\n",
    "    \"H2\": \"hydrogen\",\n",
    "    \"ExCCGT\": \"CC\"\n",
    "}\n",
    "flexmex_investments.replace(flexmex_dict, inplace=True)\n",
    "flexmex_investments.rename(columns={\n",
    "    2: \"final_energy\",\n",
    "    3: \"fuel\",\n",
    "    4: \"technology\"\n",
    "}, inplace=True)\n",
    "\n",
    "flexmex_investments = flexmex_investments.loc[\n",
    "    (flexmex_investments[\"fuel\"]. isin(flexmex_dict.values()))\n",
    "    & (flexmex_investments[\"final_energy\"].isin([\"Electricity\", \"ElectricityHeat\"]))\n",
    "]\n",
    "flexmex_investments[\"new_index\"] = flexmex_investments[\"technology\"] + \"_\" + flexmex_investments[\"fuel\"] \n",
    "flexmex_investments.set_index(\"new_index\", inplace=True)\n",
    "# Convert from €/MW to €/kW\n",
    "flexmex_investments = flexmex_investments[[\"Value\"]].div(1000)\n",
    "flexmex_investments.columns = [2020]\n",
    "for iter_year in range(2021, 2051):\n",
    "    flexmex_investments[iter_year] = flexmex_investments[2020]\n",
    "    \n",
    "flexmex_investments[\"study_title\"] = \"FlexMex_2021\"\n",
    "flexmex_investments.set_index(\"study_title\", append=True, inplace=True)\n",
    "\n",
    "# Add hydrogen costs\n",
    "flexmex_investments = tools.add_hydrogen_cost_assumption(flexmex_investments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unseen_data = pd.read_csv(\n",
    "    main_path[\"inputs\"] + sub_path[\"assumptions\"] \n",
    "    + input_file[\"unseen_data\"],\n",
    "    index_col=[0, 1], header=0, sep=\";\"\n",
    ")\n",
    "unseen_data.rename(index={\n",
    "    \"Solid Biomass\": \"GT_biomass\",\n",
    "    \"Biogas\": \"GT_biomass\",\n",
    "    \"Pumped hydro\": \"storage_el_phes\",\n",
    "    \"Electrolyzers\": \"EL_hydrogen\",\n",
    "    \"Fuel cells\": \"FC_hydrogen\",\n",
    "    \"Open cycle gas turbines\": \"GT_natgas\",\n",
    "    \"Combinded-cycle gas turbines\": \"CC_natgas\",\n",
    "    \"Hard coal power plants\": \"ST_hardcoal\",\n",
    "    \"Lignite power plants\": \"ST_lignite\",\n",
    "    \"Lithium batteries\": \"storage_el_battery\"\n",
    "}, inplace=True)\n",
    "\n",
    "unseen_storages_data = unseen_data.loc[\n",
    "   unseen_data[\"Unit\"].str.contains(\"kWh\") \n",
    "]\n",
    "unseen_investment_data = unseen_data.loc[\n",
    "    ~unseen_data[\"Unit\"].str.contains(\"kWh\")\n",
    "]\n",
    "unseen_data_pivoted = unseen_investment_data.pivot_table(\n",
    "    index=unseen_investment_data.index, columns=\"Year\", values=\"Value\"\n",
    ")\n",
    "unseen_investments = pd.DataFrame(\n",
    "    index=unseen_data_pivoted.index,\n",
    "    columns=range(2020, 2051),\n",
    "    dtype=\"float64\"\n",
    ")\n",
    "for col in unseen_data_pivoted.columns:\n",
    "    unseen_investments[col] = unseen_data_pivoted[col]\n",
    "unseen_investments.interpolate(how=\"linear\", axis=1, inplace=True)\n",
    "unseen_investments.index = pd.MultiIndex.from_tuples(unseen_investments.index)\n",
    "unseen_investments.reset_index(level=1, inplace=True)\n",
    "unseen_investments[\"study_title\"] = \"UNSEEN_\" + unseen_investments[\"level_1\"]\n",
    "unseen_investments.set_index(\"study_title\", append=True, inplace=True)\n",
    "unseen_investments.drop(columns=\"level_1\", inplace=True)\n",
    "# Add hydrogen costs\n",
    "unseen_investments = tools.add_hydrogen_cost_assumption(unseen_investments)\n",
    "\n",
    "# Exclude storages for laters\n",
    "unseen_investments_storages = unseen_investments.loc[\n",
    "    unseen_investments.index.get_level_values(0).isin([\"storage_el_phes\", \"storage_el_battery\"])\n",
    "]\n",
    "unseen_investments = unseen_investments.loc[\n",
    "    ~unseen_investments.index.get_level_values(0).isin(\n",
    "        unseen_investments_storages.index.get_level_values(0)\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pypsa_eur_data = pd.read_csv(\n",
    "    main_path[\"inputs\"] + sub_path[\"assumptions\"] \n",
    "    + input_file[\"pypsa_eur\"],\n",
    "    index_col=0, header=0, sep=\",\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pypsa_eur_investments = pypsa_eur_data.loc[\n",
    "    pypsa_eur_data[\"parameter\"] == \"investment\", [\"value\"]\n",
    "]\n",
    "generators = {\n",
    "    \"onwind\": \"windonshore\",\n",
    "    \"offwind\": \"windoffshore\",\n",
    "    \"solar\": \"solarPV\",\n",
    "    \"biomass\": \"GT_biomass\",\n",
    "    \"geothermal\": \"geothermal\",\n",
    "    \"coal\": \"ST_hardcoal\",\n",
    "    \"lignite\": \"ST_lignite\",\n",
    "    \"solar-rooftop\": \"solarPV\", \n",
    "    \"solar-utility\": \"solarPV\",\n",
    "    \"ror\": \"ror\",\n",
    "    \"OCGT\": \"GT_natgas\",\n",
    "    \"nuclear\": \"ST_uranium\",\n",
    "    \"CCGT\": \"CC_natgas\",\n",
    "    \"oil\": \"ST_oil\",\n",
    "    \"fuel cell\": \"FC_hydrogen\",\n",
    "}\n",
    "pypsa_eur_investments.rename(\n",
    "    index=generators,\n",
    "    inplace=True\n",
    ")\n",
    "pypsa_eur_investments.drop(\n",
    "    index=[\n",
    "        el for el in pypsa_eur_investments.index \n",
    "        if el not in generators.values()\n",
    "    ],\n",
    "    inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pypsa_eur_investments.columns = [2020]\n",
    "for col in range(2021, 2051):\n",
    "    pypsa_eur_investments[col] = pypsa_eur_investments[2020]\n",
    "pypsa_eur_investments[\"study_title\"] = \"PyPSA_Eur\"\n",
    "pypsa_eur_investments.set_index(\"study_title\", append=True, inplace=True)\n",
    "# Add hydrogen costs\n",
    "pypsa_eur_investments = tools.add_hydrogen_cost_assumption(pypsa_eur_investments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storages\n",
    "> _Note: The distinction between the data sets is handled differently. This holds especially true for electrolyzers. Since these have similarities with transformer units in oemof.solph, we consider them as such and add them to the data set of generators._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storages_assumptions_pietzcker = pd.read_excel(\n",
    "    main_path[\"inputs\"] + sub_path[\"assumptions\"] \n",
    "    + input_file[\"pietzcker_technology_assumptions\"],\n",
    "    sheet_name=\"storages\",\n",
    "    index_col=0, header=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storages_time_dependent_pietzcker = pd.read_excel(\n",
    "    main_path[\"inputs\"] + sub_path[\"assumptions\"] \n",
    "    + input_file[\"pietzcker_technology_assumptions\"],\n",
    "    sheet_name=\"storages_time-dependent\",\n",
    "    index_col=[0, 1], header=0\n",
    ")\n",
    "storages_time_dependent_pietzcker.rename(\n",
    "    columns={\"2050–2070\": \"2050\"},\n",
    "    inplace=True\n",
    ")\n",
    "storages_time_dependent_pietzcker.columns = (\n",
    "    storages_time_dependent_pietzcker.columns.astype(\"int64\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_related_costs = storages_time_dependent_pietzcker.loc[\n",
    "    storages_time_dependent_pietzcker.index.get_level_values(1) == \"Reservoir (€/kWh)\"\n",
    "]\n",
    "power_related_costs = storages_time_dependent_pietzcker.loc[\n",
    "    storages_time_dependent_pietzcker.index.get_level_values(1) == \"Power (€/kW)\"\n",
    "]\n",
    "energy_related_costs.reset_index(level=1, drop=True, inplace=True)\n",
    "power_related_costs.reset_index(level=1, drop=True, inplace=True)\n",
    "power_related_costs.columns = pd.MultiIndex.from_product([\n",
    "    [\"power_costs\"], power_related_costs.columns\n",
    "])\n",
    "energy_related_costs.columns = pd.MultiIndex.from_product([\n",
    "    [\"capacity_costs\"], energy_related_costs.columns\n",
    "])\n",
    "storages_investment_expenses_pietzcker = pd.concat(\n",
    "    [power_related_costs, energy_related_costs], \n",
    "    axis=1\n",
    ")\n",
    "storages_investment_expenses_pietzcker.rename(\n",
    "    index={\n",
    "        \"Batteries\": \"storage_el_battery\",\n",
    "        \"Hydrogen Electrolysis\": \"EL_hydrogen\"\n",
    "    },\n",
    "    inplace=True\n",
    ")\n",
    "storages_investment_expenses_pietzcker[\"study_title\"] = \"Pietzcker_2021\"\n",
    "storages_investment_expenses_pietzcker.set_index(\"study_title\", append=True, inplace=True)\n",
    "\n",
    "# Correctly attribute electrolyzers\n",
    "investment_expenses_pietzcker.loc[(\"EL_hydrogen\", \"Pietzcker_2021\"), :] = (\n",
    "    storages_investment_expenses_pietzcker.loc[((\"EL_hydrogen\", \"Pietzcker_2021\"), \"power_costs\")].values\n",
    ")\n",
    "storages_investment_expenses_pietzcker.drop(index=\"EL_hydrogen\", inplace=True, level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ise_storages = pd.read_excel(\n",
    "    main_path[\"inputs\"] + sub_path[\"assumptions\"] \n",
    "    + input_file[\"ise_technology_assumptions\"],\n",
    "    sheet_name=\"storage_transformation\",\n",
    "    index_col=[0, 1, 2], header=0\n",
    ")\n",
    "ise_storages_investment = ise_storages.loc[\n",
    "    ise_storages.index.get_level_values(1) == \"Investition \"\n",
    "]\n",
    "storage_techs = {\n",
    "    \"Elektrolyse (MIX PEM/AEL/HTEL)\": \"EL_hydrogen\",\n",
    "    \"Stationäre Batterien \": \"storage_el_battery\",\n",
    "    \"Wasserstoffspeicher & -verdichter\": \"storage_hydrogen\",\n",
    "}\n",
    "ise_storages_investment.rename(\n",
    "    index=storage_techs,\n",
    "    inplace=True\n",
    ")\n",
    "ise_storages_investment = ise_storages_investment.loc[\n",
    "    ise_storages_investment.index.get_level_values(0).isin(\n",
    "        storage_techs.values()\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_related_costs = ise_storages_investment.loc[\n",
    "    ise_storages_investment.index.get_level_values(2).str.contains(\"kWh\")\n",
    "]\n",
    "power_related_costs = ise_storages_investment.loc[\n",
    "    ~ise_storages_investment.index.get_level_values(2).str.contains(\"kWh\")\n",
    "]\n",
    "energy_related_costs.reset_index(level=[1, 2], drop=True, inplace=True)\n",
    "power_related_costs.reset_index(level=[1, 2], drop=True, inplace=True)\n",
    "power_related_costs.columns = pd.MultiIndex.from_product([\n",
    "    [\"power_costs\"], power_related_costs.columns\n",
    "])\n",
    "energy_related_costs.columns = pd.MultiIndex.from_product([\n",
    "    [\"capacity_costs\"], energy_related_costs.columns\n",
    "])\n",
    "ise_storages_investment = pd.concat(\n",
    "    [power_related_costs, energy_related_costs], \n",
    "    axis=1\n",
    ")\n",
    "ise_storages_investment[\"study_title\"] = \"ISE_2020\"\n",
    "ise_storages_investment.set_index(\"study_title\", append=True, inplace=True)\n",
    "\n",
    "# Correctly attribute electrolyzers\n",
    "investment_expenses_ise.loc[(\"EL_hydrogen\", \"ISE_2020\"), :] = (\n",
    "    ise_storages_investment.loc[((\"EL_hydrogen\", \"ISE_2020\"), \"power_costs\")].values\n",
    ")\n",
    "ise_storages_investment.drop(index=\"EL_hydrogen\", inplace=True, level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dieterpy_storages_data = pd.read_excel(\n",
    "    main_path[\"inputs\"] + sub_path[\"assumptions\"] \n",
    "    + input_file[\"dieterpy_data\"],\n",
    "    sheet_name=\"sto\",\n",
    "    index_col=0, header=[0, 1, 2], skiprows=2, nrows=6\n",
    ").dropna(how=\"all\").dropna(how=\"all\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slice investments and convert from €/MW to €/kW\n",
    "dieterpy_storages_data.index = dieterpy_storages_data.index.str.split(\"_\", expand=True)\n",
    "dieterpy_storage_investments = dieterpy_storages_data.rename(index={\n",
    "    \"phs\": \"storage_el_phes\",\n",
    "    \"liion\": \"storage_el_battery\",\n",
    "    \"p2g2p\": \"storagel_el_p2g2p\",\n",
    "})[[('Overnight investment costs in energy ', '[EUR/MWh]', 'oc_energy'),\n",
    "     ('Overnight investment costs in capacity', '[EUR/MW]', 'oc_power'),]].div(1000)\n",
    "dieterpy_storage_investments.columns = pd.MultiIndex.from_tuples([\n",
    "    (\"capacity_costs\", 2020),\n",
    "    (\"power_costs\", 2020)\n",
    "])\n",
    "dieterpy_storage_investments[(\"capacity_costs\", 2050)] = dieterpy_storage_investments[(\"capacity_costs\", 2020)]\n",
    "dieterpy_storage_investments[(\"power_costs\", 2050)] = dieterpy_storage_investments[(\"power_costs\", 2020)]\n",
    "dieterpy_storage_investments = dieterpy_storage_investments.sort_index(axis=1, level=0)\n",
    "dieterpy_storage_investments.index.names = [\"technology\", \"kind\"]\n",
    "dieterpy_storage_investments[\"study_title\"] = \"DIETER_2021_\" + dieterpy_storage_investments.index.get_level_values(1)\n",
    "dieterpy_storage_investments.set_index(\"study_title\", append=True, inplace=True)\n",
    "dieterpy_storage_investments = dieterpy_storage_investments.droplevel(1)\n",
    "dieterpy_storage_investments = dieterpy_storage_investments.rename(index={np.nan: \"DIETER_2021\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flexmex_storage_investments = flexmex_data.loc[\n",
    "    ((flexmex_data[\"Parameter\"].str.contains(\"Storage\")) \n",
    "    | (flexmex_data[\"Parameter\"].str.contains(\"EnergyConversion\")))\n",
    "    & (flexmex_data[\"Parameter\"].str.contains(\"Capex_Electricity\"))\n",
    "    & ~(flexmex_data[\"Parameter\"].str.contains(r\"CH4|Nuclear\"))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flexmex_storage_investments = pd.concat(\n",
    "    [\n",
    "        flexmex_storage_investments,\n",
    "        flexmex_storage_investments[\"Parameter\"].str.split(\"_\", expand=True)\n",
    "    ], axis=1)\n",
    "flexmex_storages_dict = {\n",
    "    \"Hydro\": \"storage_el_reservoir\",\n",
    "    \"LiIonBattery\": \"storage_el_battery\",\n",
    "    \"LiIonBatteryCharge\": \"storage_el_battery_pump\",\n",
    "    \"LiIonBatteryDischarge\": \"storage_el_battery_turbine\",\n",
    "    \"LiIonBatteryStorage\": \"storage_el_battery_capacity\",\n",
    "    \"ReservoirPump\": \"pump\",\n",
    "    \"ReservoirStorage\": \"capacity\",\n",
    "    \"ReservoirTurbine\": \"turbine\",\n",
    "    np.nan: \"\"\n",
    "}\n",
    "flexmex_storage_investments.replace(flexmex_storages_dict, inplace=True)\n",
    "flexmex_storage_investments.rename(columns={\n",
    "    2: \"final_energy\",\n",
    "    3: \"fuel\",\n",
    "    4: \"technology\"\n",
    "}, inplace=True)\n",
    "flexmex_storage_investments[\"tech_fuel\"] = (\n",
    "    flexmex_storage_investments[\"technology\"] + \"_\" + flexmex_storage_investments[\"fuel\"]\n",
    ").str.strip(\"_\")\n",
    "\n",
    "flexmex_storage_investments.set_index(\"tech_fuel\", inplace=True)\n",
    "flexmex_storage_investments[(\"capacity_costs\", 2020)] = np.where(\n",
    "    flexmex_storage_investments[\"Unit\"].str.contains(\"h\"), \n",
    "    flexmex_storage_investments[\"Value\"],\n",
    "    np.nan\n",
    ")\n",
    "flexmex_storage_investments[(\"power_costs\", 2020)] = np.where(\n",
    "    ~flexmex_storage_investments[\"Unit\"].str.contains(\"h\"), \n",
    "    flexmex_storage_investments[\"Value\"],\n",
    "    np.nan\n",
    ")\n",
    "flexmex_storage_investments[(\"capacity_costs\", 2050)] = flexmex_storage_investments[(\"capacity_costs\", 2020)]\n",
    "flexmex_storage_investments[(\"power_costs\", 2050)] = flexmex_storage_investments[(\"power_costs\", 2020)]\n",
    "flexmex_storage_investments = flexmex_storage_investments[[\n",
    "    (\"capacity_costs\", 2020), (\"capacity_costs\", 2050), \n",
    "    (\"power_costs\", 2020), (\"power_costs\", 2050)\n",
    "]]\n",
    "flexmex_storage_investments.columns = pd.MultiIndex.from_tuples(flexmex_storage_investments.columns)\n",
    "flexmex_storage_investments[\"study_title\"] = \"FlexMex_2021\"\n",
    "flexmex_storage_investments.set_index(\"study_title\", append=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unseen_investments_storages.columns = pd.MultiIndex.from_product([\n",
    "    [\"power_costs\"], unseen_investments_storages.columns\n",
    "])\n",
    "unseen_storages_data_pivoted = unseen_storages_data.pivot_table(\n",
    "    index=unseen_storages_data.index, columns=\"Year\", values=\"Value\"\n",
    ")\n",
    "unseen_storages_data_pivoted.columns = pd.MultiIndex.from_product([\n",
    "    [\"capacity_costs\"], unseen_storages_data_pivoted.columns\n",
    "])\n",
    "unseen_storages_data_pivoted.index = pd.MultiIndex.from_tuples(unseen_storages_data_pivoted.index)\n",
    "unseen_storages_data_pivoted.reset_index(level=1, inplace=True)\n",
    "unseen_storages_data_pivoted[\"study_title\"] = \"UNSEEN_\" + unseen_storages_data_pivoted[\"level_1\"]\n",
    "unseen_storages_data_pivoted.set_index(\"study_title\", append=True, inplace=True)\n",
    "unseen_storages_data_pivoted.drop(columns=\"level_1\", inplace=True)\n",
    "\n",
    "unseen_investments_storages = pd.concat(\n",
    "    [unseen_investments_storages, unseen_storages_data_pivoted], \n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pypsa_eur_storages_investments = pypsa_eur_data.loc[\n",
    "    pypsa_eur_data[\"parameter\"] == \"investment\", [\"value\", \"unit\"]\n",
    "]\n",
    "storages = {\n",
    "    \"PHS\": \"storage_el_phes\",\n",
    "    \"hydro\": \"storage_el_reservoir\",\n",
    "    \"battery storage\": \"storage_el_battery\",\n",
    "}\n",
    "pypsa_eur_storages_investments.rename(\n",
    "    index=storages,\n",
    "    inplace=True\n",
    ")\n",
    "pypsa_eur_storages_investments.drop(\n",
    "    index=[\n",
    "        el for el in pypsa_eur_storages_investments.index \n",
    "        if el not in storages.values()\n",
    "    ],\n",
    "    inplace=True\n",
    ")\n",
    "# Assume an exchange rate of 1 USD = 0.9 €\n",
    "pypsa_eur_storages_investments.loc[\"storage_el_battery\", \"value\"] *= 0.9 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pypsa_eur_storages_investments[(\"power_costs\", 2020)] = np.where(\n",
    "    ~pypsa_eur_storages_investments[\"unit\"].str.contains(\"h\"), \n",
    "    pypsa_eur_storages_investments[\"value\"],\n",
    "    np.nan\n",
    ")\n",
    "pypsa_eur_storages_investments[(\"capacity_costs\", 2020)] = np.where(\n",
    "    pypsa_eur_storages_investments[\"unit\"].str.contains(\"h\"), \n",
    "    pypsa_eur_storages_investments[\"value\"],\n",
    "    np.nan\n",
    ")\n",
    "pypsa_eur_storages_investments[(\"power_costs\", 2050)] = pypsa_eur_storages_investments[(\"power_costs\", 2020)]\n",
    "pypsa_eur_storages_investments[(\"capacity_costs\", 2050)] = pypsa_eur_storages_investments[(\"capacity_costs\", 2020)]\n",
    "pypsa_eur_storages_investments = pypsa_eur_storages_investments[[\n",
    "    (\"capacity_costs\", 2020), (\"capacity_costs\", 2050), \n",
    "    (\"power_costs\", 2020), (\"power_costs\", 2050)\n",
    "]]\n",
    "pypsa_eur_storages_investments.columns = pd.MultiIndex.from_tuples(pypsa_eur_storages_investments.columns)\n",
    "pypsa_eur_storages_investments[\"study_title\"] = \"PyPSA_Eur\"\n",
    "pypsa_eur_storages_investments.set_index(\"study_title\", append=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine and evaluate data sets\n",
    "Steps applied:\n",
    "* Iterate over data sets to create complete data sets.\n",
    "* Check investment expenses estimates per technology in form of a box plot as well as by calculating statistical metrics.\n",
    "* Calculate 5%, 50% and 95% estimates for investment expenses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add proxies for missing generators\n",
    "tech_fuel_proxies = {\n",
    "    'ST_biomass': 'ST_hardcoal',\n",
    "    'GT_oil': 'GT_natgas',\n",
    "    'ST_biomass': 'ST_hardcoal',\n",
    "}\n",
    "# Use proxies for generators and None for storages (capacity & energy)\n",
    "proxies = [tech_fuel_proxies, None, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize collections\n",
    "investment_expenses_comparison = pd.DataFrame(\n",
    "    columns=range(2020, 2051), index=pd.MultiIndex(\n",
    "        levels=[[], []], codes=[[], []]\n",
    "    )\n",
    ")\n",
    "storages_investment_expenses_comparison = pd.DataFrame(\n",
    "    columns=pd.MultiIndex.from_product([\n",
    "        [\"power_costs\", \"capacity_costs\"],\n",
    "        range(2020, 2051)\n",
    "    ]), \n",
    "    index=pd.MultiIndex(\n",
    "        levels=[[], []], codes=[[], []]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "investment_studies = [\n",
    "    investment_expenses_pietzcker, \n",
    "    investment_expenses_ise, \n",
    "    dieterpy_investments,\n",
    "    flexmex_investments, \n",
    "    unseen_investments, \n",
    "    pypsa_eur_investments\n",
    "]\n",
    "storage_investment_studies = [\n",
    "    storages_investment_expenses_pietzcker, \n",
    "    ise_storages_investment, \n",
    "    dieterpy_storage_investments,\n",
    "    unseen_investments_storages, \n",
    "    pypsa_eur_storages_investments\n",
    "]\n",
    "\n",
    "# Convert to correct unit: €/MW! (resp. €/MWh for storages capacity part)\n",
    "investment_studies = [study.mul(1000) for study in investment_studies]\n",
    "storage_investment_studies = [study.mul(1000) for study in storage_investment_studies]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine to overall data set - generators\n",
    "for study in investment_studies:\n",
    "    # Convert to nominal terms\n",
    "    for iter_year in study.columns:\n",
    "        study[iter_year] = study[iter_year].mul(inflation_rate ** (iter_year - 2020)).round(2)\n",
    "    investment_expenses_comparison = tools.add_study_to_comparison(\n",
    "        investment_expenses_comparison, \n",
    "        study\n",
    "    )\n",
    "# Combine to overall data set - storages\n",
    "for study in storage_investment_studies:\n",
    "    # Convert to nominal terms\n",
    "    for col in study.columns:\n",
    "        study[col] = study[col].mul(inflation_rate ** (col[1] - 2020)).round(2)\n",
    "    storages_investment_expenses_comparison = tools.add_study_to_comparison(\n",
    "        storages_investment_expenses_comparison, \n",
    "        study\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define information to be evaluated within an iteration\n",
    "iterations = [\n",
    "    {\n",
    "        \"data_set\": investment_expenses_comparison, \n",
    "        \"columns\": list(range(2020, 2051)),\n",
    "        \"parameter\": \"investment_expenses\"\n",
    "    },\n",
    "    {\n",
    "        \"data_set\": storages_investment_expenses_comparison, \n",
    "        \"columns\": pd.MultiIndex.from_product([[\"power_costs\"], list(range(2020, 2051))]),\n",
    "        \"parameter\": \"investment_expenses_storages_power\"\n",
    "    },\n",
    "    {\n",
    "        \"data_set\": storages_investment_expenses_comparison, \n",
    "         \"columns\": pd.MultiIndex.from_product([[\"capacity_costs\"], list(range(2020, 2051))]),\n",
    "        \"parameter\": \"investment_expenses_storages_capacity\"\n",
    "    }\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate and combine to overall data sets; create plots and calculate summary statistics\n",
    "list_of_data_sets = []\n",
    "for number, iteration in enumerate(iterations):\n",
    "    data_sets = {}\n",
    "    for idx in iterations[number][\"data_set\"].index.get_level_values(0).unique():\n",
    "        data = iterations[number][\"data_set\"].loc[idx, iterations[number][\"columns\"]]\n",
    "        if type(data.columns) == pd.MultiIndex:\n",
    "            data.columns = data.columns.droplevel(0)\n",
    "        if len(data) > 1:\n",
    "            tools.plot_parameter_comparison(\n",
    "                data=data, \n",
    "                parameter=iterations[number][\"parameter\"],\n",
    "                category=idx,\n",
    "                savefig=True,\n",
    "                show=False\n",
    "            )\n",
    "        stats_data = tools.calculate_summary_statistics(\n",
    "            data=data,\n",
    "            path=main_path[\"outputs\"],\n",
    "            parameter=iterations[number][\"parameter\"],\n",
    "            category=idx,\n",
    "            save=False\n",
    "        )\n",
    "        data_sets[idx] = stats_data.copy()\n",
    "    list_of_data_sets.append(data_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract 5%, 50% and 95% estimate from summary statistics for each technology category\n",
    "for number, data_sets in enumerate(list_of_data_sets):\n",
    "    for estimate in [\"5%\", \"50%\", \"95%\"]:\n",
    "        overall_data_set = tools.combine_parameter_estimates(\n",
    "            col_names=list(range(2020, 2101)),\n",
    "            data_sets=data_sets,\n",
    "            parameter=iterations[number][\"parameter\"],\n",
    "            estimate=estimate,\n",
    "            path=main_path[\"outputs\"],\n",
    "            transform=True,\n",
    "            save=True,\n",
    "            proxies=proxies[number],\n",
    "            inflation_rate=inflation_rate,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixed costs\n",
    "Include fixed costs assumptions for technologies\n",
    "\n",
    "Sources evaluated (same as for investment expenses):\n",
    "* Pietzcker, Robert, Osorio, Sebastian, Rodrigues, Renato (2021): Tightening EU ETS targets in line with the European Green Deal: Impacts on the decarbonization of the EU power sector, in: Applied Energy 293 (2021), https://doi.org/10.1016/j.apenergy.2021.\n",
    "* Fraunhofer ISE (2020): Appendix to the study \"Wege zu einem klimaneutralen Energiesystem. Die deutsche Energiewende im Kontext gesellschaftlicher Verhaltensweisen.\", https://www.ise.fraunhofer.de/de/veroeffentlichungen/studien/wege-zu-einem-klimaneutralen-energiesystem.html, accessed 08.07.2022.\n",
    "* PyPSA-EUR assumptions on costs compiled from various (primary) sources: https://github.com/PyPSA/pypsa-eur/blob/master/data/costs.csv, accessed 01.07.2022.\n",
    "* dieterpy input data compiled from various (primary) sources: https://gitlab.com/diw-evu/dieter_public/dieterpy_reduced/-/blob/main/input/data_input.xlsx, accessed 01.07.2022.\n",
    "* A data set from the research project FlexMex, https://zenodo.org/record/5802178, including data compiled from various (primary) sources, accessed 01.07.2022.\n",
    "* As well as a collection of data from various (primary) sources from the research project UNSEEN (as of 03/2021).\n",
    "\n",
    "### Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pietzcker_rename_dict = {\n",
    "    'Hard Coal CCS': 'ST_CCS_hardcoal', \n",
    "    'Lignite CCS': 'ST_CCS_lignite', \n",
    "    'Gas CC': 'CC_natgas',\n",
    "    'Gas CC CCS': 'CC_CCS_natgas',\n",
    "    'Gas CT': 'GT_natgas',\n",
    "    'Hydrogen FC': 'FC_hydrogen', \n",
    "    'BECCS': 'beccs',\n",
    "    'Wind Onshore': 'windonshore', \n",
    "    'Wind Offshore': 'windoffshore', \n",
    "    'PV': 'solarPV', \n",
    "    'CSP': 'csp',\n",
    "    'Nuclear': 'ST_uranium', \n",
    "    'Hard Coal': 'ST_hardcoal', \n",
    "    'Lignite': 'ST_lignite',\n",
    "    'Gas CC': 'CC_natgas', \n",
    "    'Gas CT': 'GT_natgas', \n",
    "    'Oil': 'ST_oil',\n",
    "    'Hydrogen CC': 'CC_hydrogen', \n",
    "    'Hydrogen CT': 'GT_hydrogen', \n",
    "    'Waste': 'ST_waste',\n",
    "    'Other gases': 'ST_otherfossil', \n",
    "    'Biomass': 'GT_biomass',\n",
    "    'Hydro': 'ror'\n",
    "}\n",
    "pietzcker_fixed_costs = tools.extract_parameter_pietzcker(\n",
    "    generation_assumptions_pietzcker, ('Fixed O&M',  '(%/yr)'), pietzcker_rename_dict\n",
    ")\n",
    "tools.append_study_title_and_rename_column(\n",
    "    pietzcker_fixed_costs, \"Pietzcker_2021\", \"fixed_costs_percent_per_year\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ise_rename_dict = {\n",
    "    \"Wind Offshore \": \"windoffshore\",\n",
    "    \"Wind Onshore \": \"windonshore\",\n",
    "    \"Photovoltaik Dach Süd\": \"solarPV\",\n",
    "    \"Photovoltaik Dach Ost-West\": \"solarPV\",\n",
    "    \"Photovoltaik Freifläche Süd\": \"solarPV\",\n",
    "    \"GuD-Kraftwerke \": \"CC_natgas\", \n",
    "    \"Gasturbine \": \"GT_natgas\",\n",
    "    \"Braunkohlekraftwerk \": \"ST_lignite\",\n",
    "    \"Steinkohlekraftwerk \": \"ST_hardcoal\", \n",
    "    \"H2-Rückverstromung (Gasturbine)\": \"GT_hydrogen\"\n",
    "}\n",
    "fixed_costs_ise = tools.extract_parameter_ise(\n",
    "    ise_generators, 'M/O-Kosten ', ise_rename_dict\n",
    ")\n",
    "tools.append_study_title_and_rename_column(\n",
    "    fixed_costs_ise, \"ISE_2020\", \"fixed_costs_percent_per_year\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dieterpy_fixed_costs = dieterpy_conventional_data.rename(index={\n",
    "    \"coal\": \"ST_hardcoal\",\n",
    "    \"ocgt\": \"GT_natgas\",\n",
    "    \"ccgt\": \"CC_natgas\",\n",
    "})[[('Annual fixed cost per MW', '[EUR/MW]', 'fixed_costs'),\n",
    "    ('Overnight investment costs per MW ', '[EUR/MW]', 'oc')]]\n",
    "\n",
    "dieterpy_fixed_costs[\"fixed_costs_percent_per_year\"] = dieterpy_fixed_costs[\n",
    "    ('Annual fixed cost per MW', '[EUR/MW]', 'fixed_costs')\n",
    "].div(dieterpy_fixed_costs[\n",
    "    ('Overnight investment costs per MW ', '[EUR/MW]', 'oc')\n",
    "]).mul(100)\n",
    "dieterpy_fixed_costs = dieterpy_fixed_costs[[\"fixed_costs_percent_per_year\"]]\n",
    "dieterpy_fixed_costs.columns = dieterpy_fixed_costs.columns.droplevel([1, 2])\n",
    "tools.append_study_title_and_rename_column(\n",
    "    dieterpy_fixed_costs, \"DIETER_2021\", \"fixed_costs_percent_per_year\"\n",
    ")\n",
    "\n",
    "dieterpy_fixed_costs = tools.add_hydrogen_cost_assumption(dieterpy_fixed_costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flexmex_fixed_costs = tools.extract_parameter_flexmex(\n",
    "    flexmex_data, rename_dict=flexmex_dict, conditions=[\"FixOM\", \"EnergyConversion\"], mode=\"generators\"\n",
    ")\n",
    "tools.append_study_title_and_rename_column(\n",
    "    flexmex_fixed_costs, \"FlexMex_2021\", \"fixed_costs_percent_per_year\"\n",
    ")\n",
    "\n",
    "flexmex_fixed_costs = tools.add_hydrogen_cost_assumption(flexmex_fixed_costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unseen_rename_dict = {\n",
    "    \"Solid Biomass\": \"GT_biomass\",\n",
    "    \"Biogas\": \"GT_biomass\",\n",
    "    \"Pumped hydro\": \"storage_el_phes\",\n",
    "    \"Electrolyzers\": \"EL_hydrogen\",\n",
    "    \"Fuel cells\": \"FC_hydrogen\",\n",
    "    \"Open cycle gas turbines\": \"GT_natgas\",\n",
    "    \"Combinded-cycle gas turbines\": \"GT_natgas\",\n",
    "    \"Hard coal power plants\": \"ST_hardcoal\",\n",
    "    \"Lignite power plants\": \"ST_lignite\",\n",
    "    \"Lithium batteries\": \"storage_el_battery\"\n",
    "}\n",
    "\n",
    "unseen_fixed_costs, unseen_fixed_costs_storages = tools.extract_parameter_unseen(\n",
    "    main_path, sub_path, input_file, \"unseen_fixed_costs_data\", \n",
    "    unseen_rename_dict, \"fixed_costs_percent_per_year\"\n",
    ")\n",
    "# Add hydrogen costs\n",
    "unseen_fixed_costs = tools.add_hydrogen_cost_assumption(unseen_fixed_costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pypsa_eur_fixed_costs = tools.extract_parameter_pypsa_eur(pypsa_eur_data, \"FOM\", generators)\n",
    "\n",
    "tools.append_study_title_and_rename_column(\n",
    "    pypsa_eur_fixed_costs, \"PyPSA_Eur\", \"fixed_costs_percent_per_year\"\n",
    ")\n",
    "# Add hydrogen costs\n",
    "pypsa_eur_fixed_costs = tools.add_hydrogen_cost_assumption(pypsa_eur_fixed_costs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storages\n",
    "> _Note: The distinction between the data sets is handled differently. This holds especially true for electrolyzers. Since these have similarities with transformer units in oemof.solph, we consider them as such and add them to the data set of generators._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pietzcker_storages_rename_dict = {\n",
    "    'Pumped Storage': 'storage_el_phes', \n",
    "    'Batteries': 'storage_el_battery', \n",
    "    'Hydrogen electrolysis': 'El_hydrogen',\n",
    "}\n",
    "\n",
    "pietzcker_storages_fixed_costs = tools.extract_parameter_pietzcker(\n",
    "    storages_assumptions_pietzcker, ('Fixed O&M (%/a)'), pietzcker_storages_rename_dict\n",
    ")\n",
    "tools.append_study_title_and_rename_column(\n",
    "    pietzcker_storages_fixed_costs, \"Pietzcker_2021\", \"fixed_costs_percent_per_year\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ise_storages_fixed_costs = tools.extract_parameter_ise(\n",
    "    ise_storages, (\"M/O-Kosten \"), storage_techs, slice=True\n",
    ")\n",
    "tools.append_study_title_and_rename_column(\n",
    "    ise_storages_fixed_costs, \"ISE_2020\", \"fixed_costs_percent_per_year\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flexmex_storage_fixed_costs = tools.extract_parameter_flexmex(\n",
    "    flexmex_data, \n",
    "    rename_dict=flexmex_storages_dict, \n",
    "    conditions=[\"Storage\", \"EnergyConversion\", \"FixOM_Electricity\", r\"CH4|Nuclear\"],\n",
    "    mode=\"storages\"\n",
    ")\n",
    "tools.append_study_title_and_rename_column(\n",
    "    flexmex_storage_fixed_costs, \"FlexMex_2021\", \"fixed_costs_percent_per_year\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unseen fixed costs data for storages is already extracted above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pypsa_eur_storages_fixed_costs = tools.extract_parameter_pypsa_eur(pypsa_eur_data, \"FOM\", storages)\n",
    "\n",
    "tools.append_study_title_and_rename_column(\n",
    "    pypsa_eur_storages_fixed_costs, \"PyPSA_Eur\", \"fixed_costs_percent_per_year\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine and evaluate data sets\n",
    "Steps applied:\n",
    "* Define proxies for missing technology-fuel combinations.\n",
    "* Iterate over data sets to create complete data sets.\n",
    "* Check investment expenses estimates per technology in form of a box plot as well as by calculating statistical metrics.\n",
    "* Calculate 5%, 50% and 95% estimates for investment expenses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proxies for generators\n",
    "tech_fuel_proxies = {\n",
    "    'ST_biomass': 'GT_biomass',\n",
    "    'ST_natgas': 'ST_hardcoal',\n",
    "    'GT_oil': 'ST_oil',\n",
    "    'CC_oil': 'CC_natgas',\n",
    "    'GT_otherfossil': 'ST_oil',\n",
    "    'ST_mixedfuels': 'ST_oil',\n",
    "}\n",
    "# Use proxies for generators and None for storages\n",
    "proxies = [tech_fuel_proxies, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize collections\n",
    "fixed_costs_comparison = pd.DataFrame(\n",
    "    columns=[\"fixed_costs_percent_per_year\"], index=pd.MultiIndex(\n",
    "        levels=[[], []], codes=[[], []]\n",
    "    )\n",
    ")\n",
    "storages_fixed_costs_comparison = pd.DataFrame(\n",
    "    columns=[\"fixed_costs_percent_per_year\"], index=pd.MultiIndex(\n",
    "        levels=[[], []], codes=[[], []]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_costs_studies = [\n",
    "    pietzcker_fixed_costs, \n",
    "    fixed_costs_ise, \n",
    "    dieterpy_fixed_costs,\n",
    "    flexmex_fixed_costs, \n",
    "    unseen_fixed_costs, \n",
    "    pypsa_eur_fixed_costs\n",
    "]\n",
    "storage_fixed_costs_studies = [\n",
    "    pietzcker_storages_fixed_costs,\n",
    "    ise_storages_fixed_costs,\n",
    "    # No fixed costs for storages in dieterpy\n",
    "    flexmex_storage_fixed_costs,\n",
    "    unseen_fixed_costs_storages,\n",
    "    pypsa_eur_storages_fixed_costs\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine to overall data set - generators\n",
    "for study in fixed_costs_studies:\n",
    "    # Convert to nominal terms\n",
    "    fixed_costs_comparison = tools.add_study_to_comparison(\n",
    "        fixed_costs_comparison, \n",
    "        study\n",
    "    )\n",
    "# Combine to overall data set - storages\n",
    "for study in storage_fixed_costs_studies:\n",
    "    # Convert to nominal terms\n",
    "    storages_fixed_costs_comparison = tools.add_study_to_comparison(\n",
    "        storages_fixed_costs_comparison, \n",
    "        study\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define information to be evaluated within an iteration\n",
    "iterations = [\n",
    "    {\n",
    "        \"data_set\": fixed_costs_comparison,\n",
    "        \"parameter\": \"fixed_costs\"\n",
    "    },\n",
    "    {\n",
    "        \"data_set\": storages_fixed_costs_comparison,\n",
    "        \"parameter\": \"fixed_costs_storages\"\n",
    "    }\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate and combine to overall data sets; create plots and calculate summary statistics\n",
    "list_of_data_sets = []\n",
    "for number, iteration in enumerate(iterations):\n",
    "    data_sets = {}\n",
    "    for idx in iterations[number][\"data_set\"].index.get_level_values(0).unique():\n",
    "        data = iterations[number][\"data_set\"].loc[idx, [\"fixed_costs_percent_per_year\"]]\n",
    "        if type(data.columns) == pd.MultiIndex:\n",
    "            data.columns = data.columns.droplevel(0)\n",
    "        if len(data) > 1:\n",
    "            tools.plot_parameter_comparison(\n",
    "                data=data, \n",
    "                parameter=iterations[number][\"parameter\"],\n",
    "                category=idx,\n",
    "                savefig=True,\n",
    "                show=False\n",
    "            )\n",
    "        stats_data = tools.calculate_summary_statistics(\n",
    "            data=data,\n",
    "            path=main_path[\"outputs\"],\n",
    "            parameter=iterations[number][\"parameter\"],\n",
    "            category=idx,\n",
    "            save=False\n",
    "        )\n",
    "        data_sets[idx] = stats_data.copy()\n",
    "    list_of_data_sets.append(data_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract 5%, 50% and 95% estimate from summary statistics for each technology category\n",
    "for number, data_sets in enumerate(list_of_data_sets):\n",
    "    for estimate in [\"5%\", \"50%\", \"95%\"]:\n",
    "        overall_data_set = tools.combine_parameter_estimates(\n",
    "            col_names=[\"fixed_costs_percent_per_year\"],\n",
    "            data_sets=data_sets,\n",
    "            parameter=iterations[number][\"parameter\"],\n",
    "            estimate=estimate,\n",
    "            path=main_path[\"outputs\"],\n",
    "            proxies=proxies[number],\n",
    "            save=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable operation costs\n",
    "Include variable costs assumptions (variable operation and maintenace costs) for technologies\n",
    "\n",
    "Sources evaluated (same as for investment expenses):\n",
    "* Pietzcker, Robert, Osorio, Sebastian, Rodrigues, Renato (2021): Tightening EU ETS targets in line with the European Green Deal: Impacts on the decarbonization of the EU power sector, in: Applied Energy 293 (2021), https://doi.org/10.1016/j.apenergy.2021.\n",
    "* PyPSA-EUR assumptions on costs compiled from various (primary) sources: https://github.com/PyPSA/pypsa-eur/blob/master/data/costs.csv, accessed 01.07.2022.\n",
    "* dieterpy input data compiled from various (primary) sources: https://gitlab.com/diw-evu/dieter_public/dieterpy_reduced/-/blob/main/input/data_input.xlsx, accessed 01.07.2022.\n",
    "* A data set from the research project FlexMex, https://zenodo.org/record/5802178, including data compiled from various (primary) sources, accessed 01.07.2022.\n",
    "* As well as a collection of data from various (primary) sources from the research project UNSEEN (as of 03/2021).\n",
    "\n",
    "### Generators\n",
    "> _Note: ISE (2020) and dieterpy data set do not contain any variable costs for storages._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pietzcker_variable_costs = tools.extract_parameter_pietzcker(\n",
    "    generation_assumptions_pietzcker, ('Variable O&M',  '(e/MWh)'), pietzcker_rename_dict\n",
    ")\n",
    "tools.append_study_title_and_rename_column(\n",
    "    pietzcker_variable_costs, \"Pietzcker_2021\", \"variable_costs\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flexmex_variable_costs = tools.extract_parameter_flexmex(\n",
    "    flexmex_data, rename_dict=flexmex_dict, conditions=[\"VarOM\", \"EnergyConversion\"], mode=\"generators\"\n",
    ")\n",
    "# Convert from €/GWh to €/MWh\n",
    "flexmex_variable_costs = flexmex_variable_costs.div(1000)\n",
    "tools.append_study_title_and_rename_column(\n",
    "    flexmex_variable_costs, \"FlexMex_2021\", \"variable_costs\"\n",
    ")\n",
    "\n",
    "flexmex_variable_costs = tools.add_hydrogen_cost_assumption(flexmex_variable_costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unseen_variable_costs, unseen_variable_costs_storages = tools.extract_parameter_unseen(\n",
    "    main_path, sub_path, input_file, \"unseen_variable_costs_data\", \n",
    "    unseen_rename_dict, \"variable_costs\"\n",
    ")\n",
    "# Include conversion from €/kWh to €/MWh\n",
    "unseen_variable_costs = unseen_variable_costs.mul(1000)\n",
    "unseen_variable_costs_storages = unseen_variable_costs_storages.mul(1000)\n",
    "# Add hydrogen costs\n",
    "unseen_variable_costs = tools.add_hydrogen_cost_assumption(unseen_variable_costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pypsa_eur_variable_costs = tools.extract_parameter_pypsa_eur(pypsa_eur_data, \"FOM\", generators)\n",
    "\n",
    "tools.append_study_title_and_rename_column(\n",
    "    pypsa_eur_variable_costs, \"PyPSA_Eur\", \"variable_costs\"\n",
    ")\n",
    "# Add hydrogen costs\n",
    "pypsa_eur_variable_costs = tools.add_hydrogen_cost_assumption(pypsa_eur_variable_costs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storages\n",
    "> _Note: ISE (2020), dieterpy and PyPSA-Eur data set do not contain any variable costs for storages._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pietzcker_storages_variable_costs = tools.extract_parameter_pietzcker(\n",
    "    storages_assumptions_pietzcker, ('Variable O&M (€/MWh)'), pietzcker_storages_rename_dict\n",
    ")\n",
    "tools.append_study_title_and_rename_column(\n",
    "    pietzcker_storages_variable_costs, \"Pietzcker_2021\", \"variable_costs\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flexmex_storage_variable_costs = tools.extract_parameter_flexmex(\n",
    "    flexmex_data, \n",
    "    rename_dict=flexmex_storages_dict, \n",
    "    conditions=[\"Storage\", \"EnergyConversion\", \"VarOM_Electricity\", r\"CH4|Nuclear|Solar|Wind\"],\n",
    "    mode=\"storages\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flexmex_storage_variable_costs = tools.extract_parameter_flexmex(\n",
    "    flexmex_data, \n",
    "    rename_dict=flexmex_storages_dict, \n",
    "    conditions=[\"Storage\", \"EnergyConversion\", \"VarOM_Electricity\", r\"CH4|Nuclear|Solar|Wind\"],\n",
    "    mode=\"storages\"\n",
    ")\n",
    "tools.append_study_title_and_rename_column(\n",
    "    flexmex_storage_variable_costs, \"FlexMex_2021\", \"variable_costs\"\n",
    ")\n",
    "\n",
    "# Convert from €/GWh to €/kWh and tidy up\n",
    "flexmex_storage_variable_costs = flexmex_storage_variable_costs.div(1000)\n",
    "flexmex_storage_variable_costs.rename(index={\n",
    "    'Reservoir_storage_el_reservoir': 'storage_el_reservoir'\n",
    " }, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unseen variable costs data for storages is already extracted above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine and evaluate data sets\n",
    "Steps applied:\n",
    "* Iterate over data sets to create complete data sets.\n",
    "* Check investment expenses estimates per technology in form of a box plot as well as by calculating statistical metrics.\n",
    "* Calculate 5%, 50% and 95% estimates for investment expenses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add gas motor plants proxy (not included for new-built units)\n",
    "tech_fuel_proxies['M_natgas'] = 'GT_natgas'\n",
    "proxies = [tech_fuel_proxies, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize collections\n",
    "variable_costs_comparison = pd.DataFrame(\n",
    "    columns=range(2020, 2051), index=pd.MultiIndex(\n",
    "        levels=[[], []], codes=[[], []]\n",
    "    )\n",
    ")\n",
    "storages_variable_costs_comparison = pd.DataFrame(\n",
    "    columns=range(2020, 2051), index=pd.MultiIndex(\n",
    "        levels=[[], []], codes=[[], []]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_costs_studies = [\n",
    "    pietzcker_variable_costs,\n",
    "    # No variable costs in ISE 2020\n",
    "    # No variable costs in dieterpy\n",
    "    flexmex_variable_costs, \n",
    "    unseen_variable_costs, \n",
    "    pypsa_eur_variable_costs\n",
    "]\n",
    "storage_variable_costs_studies = [\n",
    "    pietzcker_storages_variable_costs,\n",
    "    # No variable costs for storages in ISE 2020\n",
    "    # No variable costs for storages in dieterpy\n",
    "    flexmex_storage_variable_costs,\n",
    "    unseen_variable_costs_storages,\n",
    "    # No variable costs for storages in PyPSA-Eur\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine to overall data set - generators\n",
    "for study in variable_costs_studies:\n",
    "    # Convert to nominal terms\n",
    "    for iter_year in range(2020, 2051):\n",
    "        study[iter_year] = study[\"variable_costs\"] * inflation_rate ** (iter_year - 2020)\n",
    "    study = study.drop(columns=[\"variable_costs\"]).round(2)\n",
    "    variable_costs_comparison = tools.add_study_to_comparison(\n",
    "        variable_costs_comparison, \n",
    "        study\n",
    "    )\n",
    "\n",
    "# Combine to overall data set - storages\n",
    "for study in storage_variable_costs_studies:\n",
    "    # Convert to nominal terms\n",
    "    for iter_year in range(2020, 2051):\n",
    "        study[iter_year] = study[\"variable_costs\"] * inflation_rate ** (iter_year - 2020)\n",
    "    study = study.drop(columns=[\"variable_costs\"]).round(2)\n",
    "    storages_variable_costs_comparison = tools.add_study_to_comparison(\n",
    "        storages_variable_costs_comparison, \n",
    "        study\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define information to be evaluated within an iteration\n",
    "iterations = [\n",
    "    {\n",
    "        \"data_set\": variable_costs_comparison,\n",
    "        \"columns\": list(range(2020, 2051)),\n",
    "        \"parameter\": \"variable_costs\"\n",
    "    },\n",
    "    {\n",
    "        \"data_set\": storages_variable_costs_comparison,\n",
    "        \"columns\": list(range(2020, 2051)),\n",
    "        \"parameter\": \"variable_costs_storages\"\n",
    "    }\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate and combine to overall data sets; create plots and calculate summary statistics\n",
    "list_of_data_sets = []\n",
    "for number, iteration in enumerate(iterations):\n",
    "    data_sets = {}\n",
    "    for idx in iterations[number][\"data_set\"].index.get_level_values(0).unique():\n",
    "        data = iterations[number][\"data_set\"].loc[idx, iterations[number][\"columns\"]]\n",
    "        if type(data.columns) == pd.MultiIndex:\n",
    "            data.columns = data.columns.droplevel(0)\n",
    "        if len(data) > 1:\n",
    "            tools.plot_parameter_comparison(\n",
    "                data=data, \n",
    "                parameter=iterations[number][\"parameter\"],\n",
    "                category=idx,\n",
    "                savefig=True,\n",
    "                show=False\n",
    "            )\n",
    "        stats_data = tools.calculate_summary_statistics(\n",
    "            data=data,\n",
    "            path=main_path[\"outputs\"],\n",
    "            parameter=iterations[number][\"parameter\"],\n",
    "            category=idx,\n",
    "            save=False\n",
    "        )\n",
    "        data_sets[idx] = stats_data.copy()\n",
    "    list_of_data_sets.append(data_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract 5%, 50% and 95% estimate from summary statistics for each technology category\n",
    "for number, data_sets in enumerate(list_of_data_sets):\n",
    "    for estimate in [\"5%\", \"50%\", \"95%\"]:\n",
    "        overall_data_set = tools.combine_parameter_estimates(\n",
    "            col_names=range(2020, 2051),\n",
    "            data_sets=data_sets,\n",
    "            parameter=iterations[number][\"parameter\"],\n",
    "            estimate=estimate,\n",
    "            path=main_path[\"outputs\"],\n",
    "            transform=True,\n",
    "            proxies=proxies[number],\n",
    "            save=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted average cost of capital (WACC) and interest rates\n",
    "Introduce weighted average cost of capital, either on a technology-specific basis (`wacc_mode = \"technology_specific\"`) or spread among all technologies (`wacc_mode = \"unique\"`)\n",
    "\n",
    "Steps applied:\n",
    "* Check for `wacc_mode`\n",
    "* For `wacc_mode == \"technology_specific\"`, read in empirical data\n",
    "* Fill data gaps and adjust assumptions:\n",
    "    * Gas has been classified as a low-risk technology by Polzin et al. (2021), the major data source for WACC values. Nonetheless, this assumption has been made prior to Ukraine war and does not reflect the current situation. Thus, the WACC value used for gas turbines is doubled.\n",
    "    * Natural gas combined cycle is assumed to be the average of natgas gas turbine and hard coal steam turbine.\n",
    "    * Hydrogen values are assumed to be 120% the WACC of natgas values to reflect the risk of the \"new\" technology. For hydrogen fuel cells, 130% of natgas turbines are used as a proxy.\n",
    "    * Waste is assigned the same value than biomass.\n",
    "    * storage_el_battery is assigned a WACC of 3% which is comparatively low. The reason is that this comprises a mixture of industrial installations where interest rates are comparatively high and installations in the household sector coming at comparatively low return on equity demands.\n",
    "    * Run of river is assigned a WACC of 4% which is similar to reservoir energy storage.\n",
    "    * Hydrogen electrolysis is assigned a wacc of 5%.\n",
    "    * Demand Response (DR) in the industry sector is assumed a WACC of 5%. For commercial applications, a WACC of 3% is assumed. For the remaining demand response applications in the households sector, a WACC of 2% is assumed. The heating cluster for tcs and households is assigned a value of 2.3% as a mixture between the two, but with most heat pumps being installed in households.\n",
    "    * Uranium is not allowed for investments, but nonetheless, the attributed WACC seems to be underestimating the risk of new installations.\n",
    "* For `wacc_mode == \"unique\"`, overwrite data and spread a WACC of 5% for all technologies.\n",
    "* Use a constant rate of 2 % as interest rate which equals to the European inflation target.\n",
    "\n",
    "Sources:\n",
    "* Polzin, Friedemann; Sanders, Mark; Steffen, Bjarne; Egli, Florian; Schmidt, Tobias S.; Karkatsoulis, Panagiotis; Fragkos, Panagiotis; Paroussos, Leonidas (2021): The effect of differentiating costs of capital by country and technology on the European energy transition, in: Climatic Change 167 (2021). https://doi.org/10.1007/s10584-021-03163-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wacc = pd.read_csv(\n",
    "    main_path[\"inputs\"] + sub_path[\"costs\"] + input_file[\"wacc\"], \n",
    "    sep=\";\", decimal=\".\", index_col=0\n",
    ")\n",
    "wacc.rename(\n",
    "    index={\n",
    "        \"biomass\": \"ST_biomass\",\n",
    "        \"hydro\": \"storage_el_reservoir\",\n",
    "        \"coal-fired plant\": \"ST_hardcoal\",\n",
    "        \"gas plant\": \"GT_natgas\",\n",
    "        \"nuclear plant\": \"ST_uranium\"\n",
    "    }, columns={\n",
    "        \"wacc in %\": \"wacc in p.u.\"\n",
    "    }, inplace=True)\n",
    "\n",
    "# Fill data gaps & adjust values\n",
    "wacc.loc[\"GT_natgas\"] *= 2\n",
    "wacc.loc[\"CC_natgas\"] = (wacc.loc[\"GT_natgas\"] + wacc.loc[\"ST_hardcoal\"]) / 2\n",
    "wacc.loc[\"GT_hydrogen\"] = wacc.loc[\"GT_natgas\"] * 1.2\n",
    "wacc.loc[\"FC_hydrogen\"] = wacc.loc[\"GT_natgas\"] * 1.3\n",
    "wacc.loc[\"CC_hydrogen\"] = wacc.loc[\"CC_natgas\"] * 1.2\n",
    "wacc.loc[\"ST_oil\"] = wacc.loc[\"GT_natgas\"]\n",
    "wacc.loc[\"ST_waste\"] = wacc.loc[\"ST_biomass\"]\n",
    "wacc.loc[\"ST_lignite\"] = wacc.loc[\"ST_hardcoal\"]\n",
    "wacc.loc[\"storage_el_phes\"] = wacc.loc[\"storage_el_reservoir\"]\n",
    "wacc.loc[\"storage_el_battery\"] = 3\n",
    "wacc.loc[\"ROR_water\"] = 4\n",
    "wacc.loc[\"EL_hydrogen\"] = 5\n",
    "wacc.loc[\"DR_ind\"] = 5\n",
    "wacc.loc[\"DR_tcs\"] = 3\n",
    "wacc.loc[\"DR_hoho\"] = 2\n",
    "wacc.loc[\"DR_tcs+hoho\"] = 2.3\n",
    "wacc = wacc.div(100)\n",
    "  \n",
    "if wacc_mode == \"technology_specific\":\n",
    "    # Data has already been compiled\n",
    "    pass\n",
    "\n",
    "elif wacc_mode == \"unique\":\n",
    "    wacc[\"wacc in p.u.\"] = 0.05\n",
    "    \n",
    "else:\n",
    "    raise ValueError(f\"'wacc_mode' {wacc_mode} not implemented.\")\n",
    "\n",
    "wacc.to_csv(\n",
    "    main_path[\"outputs\"] + output_file[\"wacc\"] + \".csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interest_rate = pd.DataFrame(data=0.02, columns=[\"interest_rate\"], index=[\"value\"])\n",
    "interest_rate.to_csv(\n",
    "    main_path[\"outputs\"] + output_file[\"interest_rate\"] + \".csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tidy up and save all DataFrames into one Excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do some garbage collection\n",
    "try:\n",
    "    del (X, X_pred, Y, average_fossil_price_trend, average_fossil_price_trend_real,\n",
    "         buses, buses_df, constant_vals, conversion_factors_to_MWh_th,\n",
    "         costs_fuel, costs_fuel_real, costs_opex, costs_opex_real, \n",
    "         emissions_costs, emissions_costs_real, emissions_costs_ariadne, emissions_costs_pietzcker, \n",
    "         emissions_costs_pred, emissions_costs_pred_real, \n",
    "         emissions_costs_projections, emissions_costs_projections_real, emissions_costs_ts, \n",
    "         emissions_costs_ts_to_concat, energy_carrier, energy_source,\n",
    "         f, fuel, fuel_real, fuel_costs_assumptions, fuel_pred, fuel_pred_real,\n",
    "         fuel_price_projections, fuel_price_projections_real, \n",
    "         fuel_prices, fuel_prices_real, fuel_scens, fuel_regression,\n",
    "         fuels_dict, fuels_for_prediction, fuels_units, fuels_with_constant_costs,\n",
    "         full_time_series, index, iter_year,\n",
    "         monthly_price_pattern, monthly_price_trends, \n",
    "         opex, opex_real, opex_pred, opex_pred_real, opex_storages_nominal, opex_storages_real, \n",
    "         prediction_horizon, price_prediction, price_timeseries, projection,\n",
    "         scen, sheet, sources_commodity, storages_el,\n",
    "         transportation_costs_assumptions, time_series, time_series_to_concat,\n",
    "         volume_weighted_mean_price,\n",
    "         weo_fuel_price_paths, weo_fuel_price_paths_nominal, weo_prices, weo_scenarios,\n",
    "        )\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the Pandas Excel writer and output the Excel file.\n",
    "#writer.save()\n",
    "#writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "175px",
    "width": "160px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "1086px",
    "left": "35px",
    "top": "111.133px",
    "width": "656.172px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false,
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 826,
   "position": {
    "height": "713.75px",
    "left": "1982.97px",
    "right": "20px",
    "top": "146.984px",
    "width": "384.719px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
