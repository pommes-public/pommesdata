{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation for POMMES\n",
    "This notebook contains the major routines used to prepare and provide input data for the fundamental power market model _POMMES_ <br>(<b>Po</b>wer <b>M</b>arket <b>M</b>odel <b>E</b>nergy and re<b>S</b>ources).\n",
    "\n",
    "It provides input data for (oemof.solph components used for modeling given in brackets)\n",
    "* conventional power plants (Transformers)\n",
    "* interconnectors (Transformers; formerly Links)\n",
    "* storages (GenericStorages)\n",
    "* commodity sources (Sources)\n",
    "* renewable generators (Sources / Transformers)\n",
    "* power demand (Sinks)\n",
    "\n",
    "A data sets for any year between 2017 and 2030 can be created.\n",
    "* For future years, power plant commissionings and decommissionings are taken into account and RES are expanded by a given pathway.\n",
    "* Note that so far, we have included **time series information only for 2017**. For any other year, 2017 time series are used and simply reindex. To have a comparable simulation year with 8760 hours, for leap years, the last day is simply neglected.\n",
    "\n",
    "> **Outline:** The correspondend elements, named components, used in the framework [oemof.solph](https://github.com/oemof/oemof-solph) determine the outline of the given notebook.\n",
    "> _NOTE: It is recommended to run the notebook sequentially since variables declared earlier may be needed later on, even in later sections._\n",
    "\n",
    "> **Raw data sources:** The raw data is taken from different sources, mostly [OPSD](https://open-power-system-data.org/) and [ENTSO-E](https://transparency.entsoe.eu/). The major sources and assumptions are documented for the respective power system elements (components) in the following. For power plant projections, data from the lastest approved network development plan for electricity for Germany [NEP Strom 2030 as of 2019](https://www.netzentwicklungsplan.de/de/netzentwicklungsplaene/netzentwicklungsplan-2030-2019) as well as data from the [TYNDP 2018](https://tyndp.entsoe.eu/maps-data) of ENTSO-E for are used.<br>\n",
    "An overview on the data licensing information can be found in the repository.\n",
    "\n",
    "> **Tasks:** Besides extraction, cleaning and combination of data from the sources and assumptions, significant parts of the notebooks are routines for transferring the data to a data format that can be handled by the framework [oemof.solph](https://github.com/oemof/oemof-solph) which serves as a basis for the model _POMMES_.\n",
    "\n",
    "> **Authors:** Corresponding authors: Yannick Werner, Johannes Kochems<br>Contributors: Leticia Encinas Rosa, Carla Spiller, Sophie Westphal, Julian Endres, Julien Faist, Timona Ghosh, Johannes Giehl, Christian Fraatz, Robin Claus, Daniel Peschel, Conrad Nicklisch, Benjamin Grosse, Joachim Müller-Kirchenbauer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Package imports and settings\n",
    "\n",
    "## Imports\n",
    "\n",
    "User defined modules used:\n",
    "* `tools`: a set of functions holding assisting routines; comprises functions for calculating (shortest) distances based on given GPS coordinates, for loading bidding zone shapes as well as ENTSO-E data, for assigning efficiencies as well as gradients and minimum loads and for setting NTC values\n",
    "* `eeg_transformers`: functions for modeling RES clusters bidding at the negative market premium (routines for reading in data, data aggregation, clustering and assigning market values)\n",
    "* `transformers_aggregation`: functions for clustering conventional power plants\n",
    "* `hydro`: functions to load, preprocess and upsample hydro generation and filling rate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from re import search, IGNORECASE\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import data_prep.tools as tools\n",
    "import data_prep.eeg_transformers as eeg\n",
    "import data_prep.transformer_aggregation as tf_agg\n",
    "import data_prep.hydro as hydro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Paths to input files\n",
    "main_path = {\n",
    "    'inputs': '../raw_data_input/',\n",
    "    'outputs': '../prepared_data/'\n",
    "}\n",
    "\n",
    "sub_path = {\n",
    "    'interconnectors': 'interconnectors/',\n",
    "    'pp_public': 'powerplants_public/',\n",
    "    'pp_er': 'powerplants_er/',\n",
    "    'timeseries': 'timeseries/',\n",
    "    'hydro': 'hydro/',\n",
    "    'renewables': 'renewables/',\n",
    "    'marketzones': 'market_zones_scandinavia/',\n",
    "    'costs': 'costs/',\n",
    "    'assumptions': 'assumptions/'\n",
    "}\n",
    "\n",
    "# Filenames of input files\n",
    "input_file = {\n",
    "    'opsd_de': 'KWlisteDE_OPSD_20191002.csv',\n",
    "    'opsd_de_new': 'conventional_power_plants_DE.csv',\n",
    "    'ppmatching_eu': 'powerplants_conventional_eu.csv',\n",
    "    'ppmatching_eu_new': 'powerplants_conventional_eu_20210105.csv',\n",
    "    'tyndp_eu': 'ENTSO Scenario 2018 Generation Capacities.xlsm',\n",
    "    'nep_de': 'KWliste_NEP2030_20191022.xlsx',\n",
    "    'techs_de': 'Technologies_ManRes_20170212.csv',\n",
    "    'peschel': 'KWliste_DP_20160304.csv',\n",
    "    'eff_de': 'Efficiencies_ManRes_20180918.csv',\n",
    "    'comm': 'transformers_commissioning.xlsx',\n",
    "    'decomm': 'transformers_decommissioning.csv',\n",
    "    'comm_decomm_BNetzA': 'BNetzA_Veroeff_ZuUndRueckbau_20200404.xlsx',\n",
    "    'KWSAL': 'KWSAL_20200415.csv',\n",
    "    'decomm_hardcoal': 'transformers_decommissioning_hardcoal.csv',\n",
    "    'decomm_lignite': 'transformers_decommissioning_lignite.csv',\n",
    "    'new_built': 'transformers_new-built.csv',\n",
    "    'interconnectors_ttc': 'interconnectors_ttc.csv',\n",
    "    'interconnectors_tech_profiles': 'interconnectors_tech_profiles.csv',\n",
    "    'interconnectors_timeseries': 'Interconnectors_timeseries_20191028.csv',\n",
    "    'renewables_nonfluctuating': 'capacity_factors_nonfluc_16082020.csv',\n",
    "    'eeg_powerplants': 'ee_powerplants_29102021.csv',#'ee_powerplants_16082020.csv',\n",
    "    'market_values': 'netztransparenz_market_values_2017.csv',\n",
    "    'REcap_eu': 'REcapacitiesEU_IRENA_20201120.csv',\n",
    "    'REcap_DK': 'DK_capacities_20191120.csv',\n",
    "    'RES_DE_Prognos': 'Prognos_et_al_2020_RES_data.csv',\n",
    "    'RES_DE_EEG_2021': 'EEG_2021_RES_capacity_targets.csv',\n",
    "    'onshore_tenders': 'Statistik_Onshore_20210716.xlsx',\n",
    "    'solar_tenders': 'Statistik_Solar_20210716.xlsx',\n",
    "    'common_tenders': 'Statistik_GemAV_20210716.xlsx',\n",
    "    'PV_2018_01': 'PV_DegressionsVergSaetze_Mai-Juli18.xlsx',\n",
    "    'PV_2019_01': 'PV_DegressionsVergSaetze_11-01_19.xlsx',\n",
    "    'PV_2019_02': 'PV_DegressionsVergSaetze_05-07_19.xlsx',\n",
    "    'PV_2020_01': 'PV_DegressionsVergSaetze_11-01_20.xlsx',\n",
    "    'PV_2020_02': 'PV_DegressionsVergSaetze_05-07_20.xlsx',\n",
    "    'PV_2021_01': 'PV_DegressionsVergSaetze_11-01_21.xlsx',\n",
    "    'PV_2021_02': 'PV_DegressionsVergSaetze_02-04_21.xlsx',\n",
    "    'PV_2021_03': 'PV_DegressionsVergSaetze_05-07_21.xlsx',\n",
    "    'RES_cost_projections': 'RES_cost_projections.xlsx',\n",
    "    'RES_cost_ISE': 'RES_cost_projections_ISE.csv',\n",
    "    'RES_market_development' : 'RES_market_development.csv',\n",
    "    'RES_costs_ISE21' : 'RES_cost_projections_ISE21.csv',\n",
    "    'middle_fuel_costs': 'middle_fuel_costs.csv',\n",
    "    'operation_costs': 'operation_costs.csv',\n",
    "    'ramping_costs': 'ramping_costs.csv',\n",
    "    'opsd_timeseries': 'Timeseries_OPSD_20191028.csv',\n",
    "    'entsoe_generation_de': 'entsoe_generation_DE_22072020.csv',\n",
    "    'efficiencies_el': 'efficiencies_el.csv',\n",
    "    'phes_assumptions': 'phes_assumptions.csv',\n",
    "    'tech_assumptions': 'tech_assumptions.csv',\n",
    "    'emf': 'Emissionfactors_20200807.csv',\n",
    "    'GHG_emissions': 'GHG_emissions.csv'\n",
    "} \n",
    "\n",
    "# Filenames of output files (used as model input)\n",
    "output_file = {\n",
    "    \"buses\": \"buses\",\n",
    "    \"linking_transformers\": \"linking_transformers\",\n",
    "    \"linking_transformers_ts\": \"linking_transformers_ts\",\n",
    "    \"sources_fluc_res\": \"sources_fluc_res\",\n",
    "    \"sources_commodity\": \"sources_commodity\",\n",
    "    \"emission_limits\": \"emission_limits\",\n",
    "    \"sources_shortage\": \"sources_shortage\",\n",
    "    \"sources_renewables\": \"sources_renewables\",\n",
    "    \"sources_renewables_ts\": \"sources_renewables_ts\",\n",
    "    \"sinks_demand_el\": \"sinks_demand_el\",\n",
    "    \"sinks_demand_el_ts\": \"sinks_demand_el_ts\",\n",
    "    \"sinks_excess\": \"sinks_excess\",\n",
    "    \"transformers\": \"transformers\",\n",
    "    \"transformers_minload_ts\": \"transformers_minload_ts\",\n",
    "    \"transformers_renewables\": \"transformers_renewables\",\n",
    "    \"storages_el\": \"storages_el\",\n",
    "    \"costs_operation\": \"costs_operation\",\n",
    "    \"costs_fuel_middle\": \"costs_fuel_middle\",\n",
    "    \"costs_ramping\": \"costs_ramping\",\n",
    "    \"costs_operation_renewables\": \"costs_operation_renewables\",\n",
    "    \"costs_operation_storages\": \"costs_operation_storages\",\n",
    "    \"costs_carbon\": \"costs_carbon\",\n",
    "    \"costs_market_values\": \"costs_market_values\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook workflow settings\n",
    "- The variable `year` determines, for which target year the power plant status shall be evaluated. A value between 2017 and 2030 can be selected.\n",
    "- For some plants appearing in the list(s) of power plants to be decommissioned, the decommissioning year is missing. This is defined by `shutdown_assumption`, which imposes a (single) decommissioning year for the respective plants.\n",
    "- `eeg_clusters_per_technology` determines the amount of clusters per eeg technology to use.\n",
    "- `res_capacity_projection` determines which estimate to use for RES capacity development for Germany until 2030 (\"Prognos\" or \"EEG_2021\")\n",
    "- `cluster_transformers_DE` determines whether or not to cluster power plants for Germany by their efficiencies. Clustered data can be used to speed up POMMES computations coming at the expense of model result precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2017  # between 2017 and 2030 \n",
    "shutdown_assumption = 2022  # between 2017 and 2030\n",
    "eeg_clusters_per_technology = 20\n",
    "res_capacity_projection = \"Prognos\"  # \"EEG_2021\"\n",
    "cluster_transformers_DE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (year < 2017 or year > 2030):\n",
    "    raise ValueError(f'year must be between 2017 and 2030. You chose {year}.')\n",
    "\n",
    "# Initialze an ExcelWriter to store all data in an .xlsx file\n",
    "writer = pd.ExcelWriter(\n",
    "    main_path[\"outputs\"] + 'input_data_complete_' + str(year) + '.xlsx', \n",
    "    engine='openpyxl'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers\n",
    "\n",
    "In this section, **conventional power plant** data for Germany and Europe is put together.<br>\n",
    "In [oemof.solph](https://github.com/oemof/oemof-solph) conventional power plants (in the most simple approach) can be represented through (generic) so called \"transformers\" which have one input and one or two ouputs (two outputs for CHP).\n",
    "\n",
    "## Data Import and initial preparation\n",
    "\n",
    "**Main data source**:<br>\n",
    "German and European Data on conventional powerplants (PPs) is taken from [Open Power System Data](https://data.open-power-system-data.org/conventional_power_plants/)\n",
    "\n",
    "* Plant status as of 2019:<br>\n",
    "\n",
    "Open Power System Data. 2020. Data Package Conventional power plants. Version 2020-10-01, downloaded on 2021-01-04. https://doi.org/10.25832/conventional_power_plants/2020-10-01\n",
    "\n",
    "* Plant status as of 2017:<br>\n",
    "\n",
    "Open Power System Data. 2018. Data Package Conventional power plants. Version 2018-12-20, downloaded on 2019-10-02. https://doi.org/10.25832/conventional_power_plants/2018-12-20\n",
    "\n",
    "### German Powerplants (OPSD)\n",
    "\n",
    "#### Read in data and do renaming\n",
    "Steps applied:\n",
    "- Data from conventional power plants from OPSD is read in.\n",
    "- Columns not needed are dropped.\n",
    "- A new country index 'DE' is assigned for all power plants, including those that are positioned in the netherlands and austria but feed into the German grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data set with plant status as of 2019\n",
    "if year > 2017:\n",
    "    opsd_de = pd.read_csv(\n",
    "        main_path[\"inputs\"] + sub_path[\"pp_public\"] + input_file[\"opsd_de_new\"],\n",
    "        index_col=0\n",
    "    )\n",
    "\n",
    "    opsd_de.drop(\n",
    "        columns=['capacity_gross_uba',\n",
    "                 'street', 'postcode', 'city', 'network_operator',\n",
    "                 'energy_source_level_1', 'energy_source_level_2', 'energy_source_level_3',\n",
    "                 'merge_comment', 'comment'], \n",
    "        inplace=True)\n",
    "\n",
    "    opsd_de['country2'] = 'DE'\n",
    "    opsd_de.rename(columns={'country': 'country_geographical', \n",
    "                            'country2': 'country',\n",
    "                            'energy_source': 'fuel'}, \n",
    "                   inplace=True)\n",
    "    eu_pp_feedin_de = opsd_de[opsd_de['country_geographical'] != 'DE'].index\n",
    "\n",
    "# Older data set; only used to evaluate plant status as of 2017\n",
    "else:\n",
    "    opsd_de = pd.read_csv(\n",
    "        main_path[\"inputs\"] + sub_path[\"pp_public\"] + input_file[\"opsd_de\"],\n",
    "        sep=\";\", decimal=\",\", index_col=0\n",
    "    )\n",
    "\n",
    "    opsd_de.drop(\n",
    "        columns=['capacity_gross_uba',\n",
    "                 'street', 'postcode', 'city', 'network_operator',\n",
    "                 'energy_source_level_1', 'energy_source_level_2', 'energy_source_level_3',\n",
    "                 'merge_comment', 'comment'], \n",
    "        inplace=True)\n",
    "\n",
    "    opsd_de['country'] = 'DE'\n",
    "    opsd_de.rename(columns={'country_code': 'country_geographical'}, inplace=True)\n",
    "    eu_pp_feedin_de = opsd_de[opsd_de['country_geographical'] != 'DE'].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> _NOTE: There are two duplicated BNetzA ids in the new OPSD data set which are obtained from the original BNetzA power plants list._\n",
    "> * _For the respective power plants, one block (or a share of blocks) is shutdown while the other is still operational_\n",
    "> * _The shutdown power blocks' BNetzA id is manipulated to account for that circumstance._\n",
    "\n",
    "Approach:\n",
    "* Append shutdown column to index to create a MultiIndex\n",
    "* Identify duplicates at index level 0 (BNetzA ID) with shutdown being not NaN.\n",
    "* Get the index location\n",
    "* Get list of original indices, replace the index location and reassign indices, using the string '\\_SD' to indicate plants for shutdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if year > 2017:\n",
    "    duplicated_idx = {}\n",
    "    \n",
    "    opsd_multi = opsd_de.set_index('shutdown', append=True)\n",
    "    \n",
    "    duplicated = opsd_multi[(opsd_multi.index.get_level_values(0).duplicated(keep=False))\n",
    "                             & (opsd_multi.index.get_level_values(1).notna())].index\n",
    "\n",
    "    for el in duplicated:\n",
    "        idx_el = opsd_multi.index.get_loc(el)\n",
    "        duplicated_idx[el[0]] = idx_el\n",
    "        \n",
    "    as_list = opsd_de.index.tolist()\n",
    "    for k, v in duplicated_idx.items():\n",
    "        as_list[v] = k + '_SD'\n",
    "    opsd_de.index = as_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare German conventional PP Raw Data\n",
    "\n",
    "Steps applied:\n",
    "- Exclude pumped storage, run-of-river and (other) storage technologies because they are specifically treated later on <br>&rarr; Create subsets for these technologies for later usage\n",
    "- Exclude non EEG units (i.e. biomass power plants) because biomass within the EEG scheme are treated as EE source later on <br>&rarr; Create a subset for EEG technologies for later usage\n",
    "- Replace fuel names from the OPSD list with the names used in _POMMES_ and rename some columns\n",
    "- Assign last commissioning dates for units which had a retrofit (last commissioning date = retrofitting date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_de = opsd_de[(~opsd_de['technology'].isin(['Pumped storage', 'Run-of-river', \n",
    "                                                'Storage technologies', 'RES', 'Reservoir'])) &\n",
    "                  (opsd_de['eeg'] == 'no')].copy()\n",
    "\n",
    "phes_de = opsd_de.loc[opsd_de['technology'] == 'Pumped storage']\n",
    "ror_de = opsd_de.loc[opsd_de['technology'] == 'Run-of-river']\n",
    "eeg_de = opsd_de.loc[(opsd_de['eeg'] == 'yes') & (opsd_de['technology'] != 'Run-of-river')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_fuels = {'Hard coal': 'hardcoal',\n",
    "              'Nuclear': 'uranium',\n",
    "              'Mixed fossil fuels': 'mixedfuels',\n",
    "              'Lignite': 'lignite',\n",
    "              'Natural gas': 'natgas',\n",
    "              'Oil': 'oil',\n",
    "              'Biomass and biogas': 'biomass',\n",
    "              'Other fossil fuels': 'otherfossil',\n",
    "              'Other fuels': 'otherfossil',\n",
    "              'Waste': 'waste'}\n",
    "\n",
    "conv_de.loc[:,'fuel'].replace(dict_fuels, inplace=True)\n",
    "\n",
    "conv_de.rename(columns={'capacity_net_bnetza': 'capacity',\n",
    "                        'eic_code_plant': 'eic_code',\n",
    "                        'name_bnetza': 'name'}, inplace=True)\n",
    "\n",
    "conv_de.loc[~conv_de['retrofit'].isna(),\n",
    "            'commissioned_last'] = conv_de.loc[~conv_de['retrofit'].isna(), 'retrofit']\n",
    "conv_de.loc[conv_de['retrofit'].isna(), \n",
    "            'commissioned_last'] = conv_de.loc[conv_de['retrofit'].isna(), 'commissioned']\n",
    "conv_de = conv_de.astype(dtype={'commissioned_last': 'object'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### European Powerplants (PyPSA-EUR PP matching)\n",
    "\n",
    "European power plants data is obtained from an existing [power plant matching tool](https://github.com/FRESNA/powerplantmatching) developped by the PyPSA developpers:\n",
    "\n",
    "F. Gotzens, H. Heinrichs, J. Hörsch, and F. Hofmann, Performing energy modelling exercises in a transparent way - The issue of data quality in power plant databases, Energy Strategy Reviews, vol. 23, pp. 1–12, Jan. 2019. <br>[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.3358985.svg)](https://doi.org/10.5281/zenodo.3358985).<br>Copyright 2018-2020 Fabian Gotzens (FZ Jülich), Jonas Hörsch (KIT), Fabian Hofmann (FIAS)<br>\n",
    "powerplantmatching is released as free software under the [GPLv3](http://www.gnu.org/licenses/gpl-3.0.en.html), see [LICENSE](https://github.com/FRESNA/powerplantmatching/blob/master/LICENSE) for further information.\n",
    "\n",
    "Steps applied:\n",
    "- Read in data set supplied as output of the power plant matching tool.\n",
    "- Drop data for countries which are not within the scope of _POMMES_.\n",
    "- Drop powerplants that feed into the German grid and are modeled in Germany.\n",
    "\n",
    "**Update**: Use data set as of end of 2020-26-11.\n",
    "* ids used are unique and consistent.\n",
    "* Keep the naming from the older data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_eu = {'Netherlands': 'NL', \n",
    "                'Denmark': 'DK', \n",
    "                'France': 'FR', \n",
    "                'Poland': 'PL', \n",
    "                'Switzerland': 'CH',\n",
    "                'Czech Republic': 'CZ', \n",
    "                'Norway': 'NO', \n",
    "                'Sweden': 'SE', \n",
    "                'Austria': 'AT', \n",
    "                'Belgium': 'BE'}\n",
    "\n",
    "# Data set with plant status as of 2019\n",
    "if year > 2017:\n",
    "    pp_eu = pd.read_csv(\n",
    "        main_path[\"inputs\"] + sub_path[\"pp_public\"] + input_file[\"ppmatching_eu_new\"],\n",
    "        index_col=0\n",
    "    )\n",
    "    \n",
    "    columns_eu_old = {'DateIn': 'YearCommissioned',\n",
    "                      'DateRetrofit': 'Retrofit'}\n",
    "    \n",
    "    pp_eu.rename(columns=columns_eu_old, inplace=True)\n",
    "    pp_eu.drop(index=pp_eu[~pp_eu['Country'].isin(countries_eu.keys())].index, inplace=True)\n",
    "    pp_eu['Country'].replace(countries_eu, inplace=True)\n",
    "\n",
    "    pp_eu.drop(index=pp_eu[pp_eu['projectID'].str.contains('|'.join(eu_pp_feedin_de))].index, inplace=True)\n",
    "\n",
    "# Older data set; only used to evaluate plant status as of 2017\n",
    "else:\n",
    "    pp_eu = pd.read_csv(\n",
    "        main_path[\"inputs\"] + sub_path[\"pp_public\"] + input_file[\"ppmatching_eu\"],\n",
    "        sep=\";\", decimal=\",\", index_col = 0\n",
    "    )\n",
    "\n",
    "    pp_eu.drop(index=pp_eu[~pp_eu['Country'].isin(countries_eu.keys())].index, inplace=True)\n",
    "    pp_eu['Country'].replace(countries_eu, inplace=True)\n",
    "\n",
    "    pp_eu.drop(index=pp_eu[pp_eu['projectID'].str.contains('|'.join(eu_pp_feedin_de))].index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bidding Zone Allocation\n",
    "For Sweden and Norway shape files are used, that contain the respective bidding zones. Those plants that are not in these multipolygons are allocated based on the bidding zone of their nearest neighbour. This happens quite often for Norwegian hydro power, where the fjords are not perfectly represented by the shapes.\n",
    "\n",
    "Shape data is obtained from the electricity map of Tomorrow. See this [GitHub issue](https://github.com/tmrowco/electricitymap-contrib/pull/1383) for Norwegian and Swedish data.<br>\n",
    "Copyright (c) 2020 Tomorrow<br>\n",
    "License is [MIT License](https://opensource.org/licenses/MIT).\n",
    "\n",
    "Steps applied:\n",
    "- Assign bidding zones for DK based on longitude\n",
    "- Assign bidding zones for Norway and Sweden based on geometry shape data\n",
    "- Assign missing bidding zone information based on smallest geodesic distance to the next bidding zone for Norway and Sweden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_eu['bidding_zone'] = pp_eu['Country']\n",
    "\n",
    "pp_eu.loc[pp_eu['Country'] == 'DK', 'bidding_zone'] = (\n",
    "    np.where(pp_eu.loc[pp_eu['Country'] == 'DK', 'lon'] <= 10.9, 'DK1', 'DK2'))\n",
    "\n",
    "country_dict = {'NO': 5, 'SE': 4}\n",
    "for country in country_dict.keys():\n",
    "    points = pp_eu.loc[(pp_eu['Country'] == country), ['lat','lon']]\n",
    "    points_df = gpd.GeoDataFrame(points, \n",
    "                                 geometry=gpd.points_from_xy(points.lon, points.lat), \n",
    "                                 crs=\"epsg:4326\")\n",
    "    \n",
    "    zones = pd.concat([tools.load_bidding_zone_shape(\n",
    "        country, country + str(number),\n",
    "        main_path[\"inputs\"] + sub_path[\"marketzones\"]) \n",
    "                       for number in range(1, country_dict[country] + 1)])\n",
    "    \n",
    "    points_df = gpd.sjoin(points_df, zones, how='left')\n",
    "    pp_eu.loc[points_df.index, 'bidding_zone'] = points_df['id'].values\n",
    "    \n",
    "    for idx in pp_eu[(pp_eu['Country'] == country) & (pp_eu['bidding_zone'].isna())].index:\n",
    "        lat_miss, lon_miss = pp_eu.loc[idx, ['lat', 'lon']]\n",
    "\n",
    "        idx_min_dist = pp_eu.loc[(pp_eu['Country'] == country) &\n",
    "                                   (~pp_eu['bidding_zone'].isna()), ['lat', 'lon']].apply(\n",
    "            lambda x: tools.calc_dist(lat_miss, lon_miss, x.lat, x.lon), axis=1).idxmin()\n",
    "\n",
    "        pp_eu.at[idx, 'bidding_zone'] = pp_eu.at[idx_min_dist, 'bidding_zone']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare European conventional PP Raw Data\n",
    "Steps applied:\n",
    "- Rename columns and fuels such that they match the (oemof.solph) terminology used in _POMMES_\n",
    "- Assign technology for hydro plants\n",
    "    - _**Assumption made**: Plants under 10 kW are ROR, those above are hydro storages (reservoir)_\n",
    "- Introduce subsets for some technologies (conventional and renewable ones)\n",
    "- Assign last commissioning dates for units which had a retrofit (last commissioning date = retrofitting date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_eu.rename(\n",
    "    columns={'Name': 'name', \n",
    "             'Fueltype': 'fuel', \n",
    "             'Technology': 'technology', \n",
    "             'Country': 'country',\n",
    "             'Capacity': 'capacity', \n",
    "             'Efficiency': 'efficiency_el', \n",
    "             'Duration': 'duration', \n",
    "             'Volume_Mm3':'volume_m3',\n",
    "             'DamHeight_m': 'damheight_m', \n",
    "             'YearCommissioned': 'commissioned', \n",
    "             'Retrofit': 'retrofit'}, \n",
    "    inplace=True)\n",
    "\n",
    "pp_eu.loc[(pp_eu['fuel'] == 'Hydro') & (pp_eu['technology'].isna()), 'technology'] = (\n",
    "    np.where(pp_eu.loc[(pp_eu['fuel'] == 'Hydro') & (pp_eu['technology'].isna()), 'capacity'] < 10,\n",
    "             'Run-Of-River',\n",
    "             'Reservoir'))\n",
    "\n",
    "conv_eu = pp_eu[(~pp_eu['fuel'].isin(['Wind', 'Solar', 'Hydro']))].copy()\n",
    "pumpedstorage_eu = pp_eu.loc[pp_eu['technology'] == 'Pumped Storage']\n",
    "reservoir_eu = pp_eu.loc[pp_eu['technology'] == 'Reservoir']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_eu['fuel'].replace(\n",
    "    {'Hard Coal': 'hardcoal',\n",
    "     'Nuclear': 'uranium',\n",
    "     'Lignite': 'lignite',\n",
    "     'Natural Gas': 'natgas',\n",
    "     'Oil': 'oil',\n",
    "     'Bioenergy': 'biomass',\n",
    "     'Other': 'otherfossil'}, \n",
    "    inplace=True)\n",
    "\n",
    "conv_eu.loc[~conv_eu['retrofit'].isna(), \n",
    "            'commissioned_last'] = conv_eu.loc[~conv_eu['retrofit'].isna(), 'retrofit']\n",
    "conv_eu.loc[conv_eu['retrofit'].isna(), \n",
    "            'commissioned_last'] = conv_eu.loc[conv_eu['retrofit'].isna(), 'commissioned']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do some garbage collection\n",
    "try:\n",
    "    del (as_list, columns_eu_old, country, country_dict, dict_fuels,\n",
    "         duplicated_idx, duplicated, el, eu_pp_feedin_de, idx, idx_el, idx_min_dist,\n",
    "         k, lat_miss, lon_miss, opsd_de, opsd_multi, points, points_df, v, zones)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add projection for near future power plant development\n",
    "\n",
    "The future estimate for power plants in 2030 for Germany is depicted using the following sources\n",
    "* NEP power plant list: ÜNB / BNetzA (2019): Kraftwerksliste zum ÜNB Entwurf des Szenariorahmens zum NEP 2030,  https://www.netzentwicklungsplan.de/sites/default/files/paragraphs-files/Kraftwerksliste_%C3%9CNB_Entwurf_Szenariorahmen_2030_V2019_2_0_0.pdf, downloaded on 2019-10-22.\n",
    "* Plans for commissioning of new plants put together by Julien Faist (JF). Primary data is combined from\n",
    "    * UBA (2019): Genehmigte oder im Genehmigungsverfahren befindliche Kraftwerksprojekte in Deutschland, as of 01/2019, https://www.umweltbundesamt.de/sites/default/files/medien/384/bilder/dateien/4_tab_genehmigte-in_genehmigung-kraftwerksprojekte_2019-04-04.pdf, accessed 03.11.2020.\n",
    "    * BDEW (2019): BDEW-Kraftwerksliste. In Bau oder Planung befindliche Anlagen ab 20 Megawatt (MW) Leistung, Anlage zur BDEW-Presseinformation vom 1.April 2019 zur Hannover Messe, https://www.bdew.de/media/documents/PI_20190401_BDEW-Kraftwerksliste.pdf, accessed 03.11.2020.\n",
    "    * BNetzA (2019): Kraftwerksliste Bundesnetzagentur zum erwarteten Zu- und Rückbau 2019 bis 2022, https://www.bundesnetzagentur.de/DE/Sachgebiete/ElektrizitaetundGas/Unternehmen_Institutionen/Versorgungssicherheit/Erzeugungskapazitaeten/Kraftwerksliste/kraftwerksliste-node.html, as of 07.03.2019, accessed 03.12.2019.\n",
    "    * company specific sources given in the table itself.\n",
    "* The power plant 'Datteln 4' has been taken into operation meanwhile. It is obtained from a data set on exogeneous decommissioning plans which has been put together by Julien Faist as well.\n",
    "\n",
    "The future capacity projections for Europe in 2030 are derived from the latest version of the TYNDP from ENTSO-E:<br>\n",
    "ENTSO-E (2019): ENTSOE Scenario 2018 Generation Capacities, https://tyndp.entsoe.eu/maps-data, accessed 03.11.2020.\n",
    "\n",
    "Updated sources:\n",
    "* BNetzA (2020): Kraftwerksliste Bundesnetzagentur zum erwarteten Zu- und Rückbau 2019 bis 2022, https://www.bundesnetzagentur.de/DE/Sachgebiete/ElektrizitaetundGas/Unternehmen_Institutionen/Versorgungssicherheit/Erzeugungskapazitaeten/Kraftwerksliste/kraftwerksliste-node.html, as of 01.04.2020, accessed 05.01.2021."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine new commissionings for Germany\n",
    "Steps applied:\n",
    "* Read in and use the NEP power plant data set.\n",
    "    * Replace values with unknown commissioning date in NEP list. &rarr; These are planned power plants for which no commissioning date is given; it is (arbitrarily) set to 2030.\n",
    "    * Filter out newly built power plants which are commissioned between 2019 and 2030.\n",
    "    * Drop power to heat units since only generation the demand-side is exogeneously fixed in _POMMES_ except for electrical demand response.\n",
    "    * Rename columns that shall be kept so that they match the conventional data set from OPSD.\n",
    "    * Create a new data set containing only the new built power plants.\n",
    "* Combine information on new-built power plants from NEP with the commissionings list put together by JF and the new commissionings information from BNetzA.\n",
    "    * Read in the commissionings lists (JF's list and BNetzA list).\n",
    "    * Do a pairwhise string comparison of the power plants names in order to identify duplicates, i.e. power plants that are listed in two power plants lists. This is necessary for mapping because the commissionings lists in contrary to the other lists does not have the BNA_Id as an index.\n",
    "    * If there are duplicates, assign the missing capacity information from the commissionings lists data set but use the NEP data set for the remainder. The priority of the data sources is as follows:\n",
    "        1. BNetzA list since it contains the most topical information.\n",
    "        2. NEP list since it is one single consistent source.\n",
    "        3. JF's list since it combines different sources.\n",
    "* Append information for the hard coal power plant 'Datteln 4' from the hardcoal shutdown list put together by JF.\n",
    "* The planned power plant Stade-Bützfleth must not be build due to the German coal phase out law (Kohleverstromungsbeendigungsgesetz - KVBG) forbidding new installations of coal-fired power plants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nep = pd.read_excel(\n",
    "    main_path[\"inputs\"] + sub_path[\"pp_public\"] + input_file[\"nep_de\"]\n",
    ")\n",
    "nep.drop_duplicates(subset='BNetzA-ID', inplace=True)\n",
    "nep.set_index('BNetzA-ID', inplace=True)\n",
    "\n",
    "nep.loc[:,'Inbetriebnahme (Jahr)'].replace({'Jahr unbestimmt': 2030}, inplace=True)\n",
    "nep_new = nep.loc[nep['Inbetriebnahme (Jahr)'].isin(range(2019, 2031))]\n",
    "\n",
    "dict_nep_col_names = {'Kraftwerksname': 'name',\n",
    "                      'Betreiber': 'company',\n",
    "                      'Bundesland': 'state',\n",
    "                      'Energieträger': 'fuel',\n",
    "                      'Wärmeauskopplung KWK (Ja/Nein)': 'NEP_chp_bool',\n",
    "                      'Annahmen ÜNB:\\nIndustriekraftwerk (Ja/Nein)': 'NEP_ipp_bool',\n",
    "                      'Inbetriebnahme (Jahr)': 'commissioned',\n",
    "                      'Nettonennleistung B2035 [MW]': 'capacity'}\n",
    "cols_to_drop = [col for col in nep_new.columns \n",
    "                if col not in dict_nep_col_names.keys()]\n",
    "nep_new = nep_new.rename(columns=dict_nep_col_names).drop(cols_to_drop, axis=1)\n",
    "\n",
    "empty_df_new = pd.DataFrame(columns=conv_de.columns)\n",
    "conv_de_new = pd.concat([empty_df_new, nep_new], axis=0)\n",
    "\n",
    "conv_de_new = conv_de_new[~(conv_de_new.name.str.contains('Power to heat', \n",
    "                            flags=IGNORECASE, regex=True))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comm_conv_de = pd.read_excel(\n",
    "    main_path[\"inputs\"] + sub_path[\"pp_er\"] + input_file[\"comm\"],\n",
    "    sheet_name='exo_com_transformers', \n",
    "    index_col='new_built_id',\n",
    "    usecols=\"A:P\")\n",
    "\n",
    "# Exclude hard coal from data set and drop line following column headers (np.nan)\n",
    "comm_conv_de = comm_conv_de[comm_conv_de['Primärenergie-Basis'] != 'Steinkohle'].drop(np.nan)\n",
    "\n",
    "comm_BNetzA = pd.read_excel(\n",
    "    main_path[\"inputs\"] + sub_path[\"pp_public\"] + input_file[\"comm_decomm_BNetzA\"],\n",
    "    sheet_name='Zu und Rückbau Bund Tab', skiprows=5, usecols='B:I', nrows=13\n",
    ")\n",
    "\n",
    "comm_BNetzA = comm_BNetzA[comm_BNetzA['Energieträger'] != 'Steinkohle']\n",
    "comm_BNetzA.loc[comm_BNetzA['Anlagenname'].isna(), 'Anlagenname'] = comm_BNetzA['Blockname']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the preparation for a string comparison of power plants names:\n",
    "* Define a string for some elements to be excluded from the comparison, i.e. general terms, such as 'KW' for 'Kraftwerk' (German for power plants).\n",
    "* Define some inclusion criteria in order not to use relevant information, i.e. 'GuD-Köln' shall be kept while 'GuD' as a general term shall be excluded.\n",
    "* For each data set, the NEP data set, the commissionings list by JF and the commissionings list by BNetzA, take the 'name' column and perform a string split (i.e. separate strings by whitespaces) and store the result to a names DataFrame.\n",
    "* For the names data frame, exclude general terms which are not relevant for the comparison by replacing them with nan values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude = r'GUD|CCPP|AG|.?KW|KWK|GK|in|[0-9]'\n",
    "include = r'GUD-|HKW-'\n",
    "\n",
    "names = conv_de_new['name'].str.split(expand=True).replace({None:np.nan})\n",
    "names_comm = comm_conv_de['Kraftwerksname'].str.split(expand=True).replace({None:np.nan})\n",
    "names_comm_BNetzA = comm_BNetzA['Anlagenname'].str.split(expand=True).replace({None:np.nan})\n",
    "\n",
    "names = names.apply(lambda x: x.str.strip('()') if x.dtype == \"object\" else x)\n",
    "names_comm = names_comm.apply(lambda x: x.str.strip('()') if x.dtype == \"object\" else x)\n",
    "names_comm_BNetzA = names_comm_BNetzA.apply(lambda x: x.str.strip('()') if x.dtype == \"object\" else x)\n",
    "\n",
    "for col in names.columns:\n",
    "    cond1 = names[col].str.match(exclude, flags=IGNORECASE, na=False)\n",
    "    cond2 = ~(names[col].str.match(include, flags=IGNORECASE, na=False))\n",
    "    names.loc[cond1 & cond2, col] = np.nan\n",
    "    \n",
    "for col in names_comm.columns:\n",
    "    cond1 = names_comm[col].str.match(exclude, flags=IGNORECASE, na=False)\n",
    "    cond2 = ~(names_comm[col].str.match(include, flags=IGNORECASE, na=False))\n",
    "    names_comm.loc[cond1 & cond2, col] = np.nan\n",
    "    \n",
    "for col in names_comm_BNetzA.columns:\n",
    "    cond1 = names_comm_BNetzA[col].str.match(exclude, flags=IGNORECASE, na=False)\n",
    "    cond2 = ~(names_comm_BNetzA[col].str.match(include, flags=IGNORECASE, na=False))\n",
    "    names_comm_BNetzA.loc[cond1 & cond2, col] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the actual string comparison of power plants names:\n",
    "* Create sets of unique values from all name DataFrames containing all parts of the power plants names except for the general terms that have been excluded above.\n",
    "* Create the pairwhise intersection of two sets and remove the nan value.\n",
    "* Combine the relevant matches to a regex in order to use this for filtering.\n",
    "* Filter out plants which are contained in two data sets and store them in separate DataFrames.\n",
    "* Do a manual correction: For the term 'Kessel', the matching is not perfectly well-engineered. &rarr; I.e. 'Kessel 13' in Flensburg is detected as a match, though there is only a match for 'Kessel 7' in Köln."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_set = set(names.values.flatten())\n",
    "names_comm_set = set(names_comm.values.flatten())\n",
    "names_comm_BNetzA_set = set(names_comm_BNetzA.values.flatten())\n",
    "\n",
    "# union1: compare JF's list and NEP data\n",
    "union1 = names_comm_set.intersection(names_set)\n",
    "union1.remove(np.nan)\n",
    "filter_plants1 = '|'.join(union1)\n",
    "\n",
    "# union2: compare BNetzA and NEP data\n",
    "union2 = names_comm_BNetzA_set.intersection(names_set)\n",
    "union2.remove(np.nan)\n",
    "filter_plants2 = '|'.join(union2)\n",
    "\n",
    "# union3: compare JF's list and BNetzA data\n",
    "union3 = names_comm_BNetzA_set.intersection(names_comm_set)\n",
    "union3.remove(np.nan)\n",
    "filter_plants3 = '|'.join(union3)\n",
    "\n",
    "# Filter out the plants which are contained in two of the data sets\n",
    "duplicated_comm1 = comm_conv_de[comm_conv_de['Kraftwerksname'].str.contains(filter_plants1)]\n",
    "duplicated_conv1 = conv_de_new[conv_de_new['name'].str.contains(filter_plants1)]\n",
    "\n",
    "duplicated_comm_BNetzA2 = comm_BNetzA[comm_BNetzA['Anlagenname'].str.contains(filter_plants2)]\n",
    "duplicated_conv2 = conv_de_new[conv_de_new['name'].str.contains(filter_plants2)]\n",
    "\n",
    "duplicated_comm_BNetzA3 = comm_BNetzA[comm_BNetzA['Anlagenname'].str.contains(filter_plants3)]\n",
    "duplicated_comm3 = comm_conv_de[comm_conv_de['Kraftwerksname'].str.contains(filter_plants3)]\n",
    "\n",
    "# Do correction: Power plant name matching doesn't cover all possibilities\n",
    "duplicated_comm1 = duplicated_comm1[~(duplicated_comm1['Kraftwerksname'] == 'Kessel 13')]\n",
    "duplicated_comm3 = duplicated_comm3[~(duplicated_comm3['Kraftwerksname'] == 'Kessel 13')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do a capacity matching and assign missing capacity information:\n",
    "* Assign missing capacity information from commissionings lists but use NEP power plants list as a default &rarr; i.e. drop the duplicates from the commissionings list in order to be able to combine the data sources\n",
    "* Check whether power plants capacities are (nearly) the same, i.e. deviate less than 20%, in addition to the plants names being very similar. <br>\n",
    "&rarr; _NOTE: This matching criterion is ignored for now, but might be integrated later. The respective results are stored in a dictionary mapping the indices to each other and containing a boolean indicating a capacity mathc or not._\n",
    "* Assign missing capacity values to NEP data set.<br>\n",
    "&rarr; _NOTE: The 1.2 GW capacity limit mentioned in the NEP data set is violated, but plants within the data set are not build and operated by TSOs but by third parties. Hence, they do not really affect the 1.2 GW capacity limit anyway. For more detailled information on the energy regulatory background, see the [following section](#Assign-missing-values-for-new-commissioned-plants)._\n",
    "\n",
    "> _NOTE: So far, this is not fully integrated and done for the comparison of JF's list and the NEP power plants list only. This seems sufficient, since so far the routine is only used for assigning missing capacity values which can be achieved._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx_mapping: key is the search string\n",
    "# values are indices in data sets and info on whether or not there is a capacity match\n",
    "idx_mapping1 = {}\n",
    "\n",
    "for el in union1:\n",
    "    # Address inaccuracy for string 'Kessel'\n",
    "    if el == 'Kessel':\n",
    "        el = 'Kessel 7'\n",
    "        \n",
    "    idx1 = list(comm_conv_de.loc[comm_conv_de['Kraftwerksname'].str.contains(el)].index)\n",
    "    idx2 = list(conv_de_new.loc[conv_de_new['name'].str.contains(el)].index)\n",
    "    \n",
    "    cap1 = comm_conv_de.loc[comm_conv_de['Kraftwerksname'].str.contains(el), \n",
    "                            'elektrische Nettoleistung (MW)'].values\n",
    "    cap2 = conv_de_new.loc[conv_de_new['name'].str.contains(el), \n",
    "                           'capacity'].values\n",
    "\n",
    "    if len(cap1) > 1:\n",
    "        cap1 = sum(cap1)\n",
    "    else:\n",
    "        cap1 = cap1[0]\n",
    "    if len(cap2) > 1:\n",
    "        cap2 = sum(cap2)\n",
    "    else:\n",
    "        cap2 = cap2[0]\n",
    "    \n",
    "    # Determine whether capacity is nearly the same\n",
    "    match = True\n",
    "    if not isinstance(cap1, str):\n",
    "        interval = [cap1 * 0.8, cap1 * 1.2]\n",
    "        if not isinstance(cap2, str):\n",
    "            match = (cap2 >= interval[0]) & (cap2 <= interval[1])\n",
    "        else:\n",
    "            cap2 = cap1\n",
    "            conv_de_new.loc[idx2, 'capacity'] = cap1\n",
    "\n",
    "    idx_mapping1[el] = (idx1, idx2, match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjust BNetzA commissionings list:\n",
    "* Add CHP information from JF's list.\n",
    "* Add IPP information based on company names.\n",
    "* Add unique IDs for the plants from the BNetzA list, building up on the scheme JF introduced.\n",
    "* Set CHP and IPP information for known industry power plants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comm_BNetzA[['KWK (falls bekannt)', 'IPP']] = 'nein'\n",
    "for el in union3:\n",
    "    # Address inaccuracy for string 'Kessel'\n",
    "    if el == 'Kessel':\n",
    "        el = 'Kessel 7'\n",
    "    \n",
    "    comm_BNetzA.loc[comm_BNetzA['Anlagenname'].str.contains(el), \n",
    "                    ['KWK (falls bekannt)', 'IPP', 'Fernwärme Leistung (MW)']] = (\n",
    "        comm_conv_de.loc[comm_conv_de['Kraftwerksname'].str.contains(el)\n",
    "                         & ~comm_conv_de['Kraftwerksname'].str.contains('West\\n'),\n",
    "                         ['KWK (falls bekannt)', 'IPP', 'Fernwärme Leistung (MW)']].values)\n",
    "    comm_BNetzA.rename(index={\n",
    "        k: comm_conv_de.loc[\n",
    "               comm_conv_de['Kraftwerksname'].str.contains(el)\n",
    "               & ~comm_conv_de['Kraftwerksname'].str.contains('West\\n')].index.values[0]\n",
    "           for k in comm_BNetzA.loc[comm_BNetzA['Anlagenname'].str.contains(el)].index}, \n",
    "                       inplace=True)\n",
    "    \n",
    "last_idx_comm = int(comm_conv_de.index.values[-1].split('_')[-1])\n",
    "\n",
    "keys_BNetzA = [el for el in comm_BNetzA.index if isinstance(el, int)]\n",
    "values_BNetzA = ['new_built_' + '{:03d}'.format(el) \n",
    "                 for el in range(last_idx_comm + 1, last_idx_comm + 1 + len(keys_BNetzA))]\n",
    "comm_BNetzA_newidx = dict(zip(keys_BNetzA, values_BNetzA))\n",
    "comm_BNetzA.rename(index=comm_BNetzA_newidx, inplace=True)\n",
    "\n",
    "comm_BNetzA.loc[comm_BNetzA['Anlagenname'].str.contains('HKW'), 'KWK (falls bekannt)'] = 'yes'\n",
    "comm_BNetzA.loc[comm_BNetzA['Unternehmen'].str.contains(\"|\".join('Volkswagen, Ineos')), 'IPP'] = 'yes'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the different data sets after the string matching of power plants names:\n",
    "* Drop the duplicates from the commissionings lists\n",
    "* Adjust the column names and some entries for consistency\n",
    "* Combine the commissionings lists with the new-built power plants obtained from the NEP power plants list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comm_conv_de.drop(duplicated_comm1.index.union(duplicated_comm3.index), inplace=True)\n",
    "conv_de_new.drop(duplicated_conv2.index, inplace=True)\n",
    "\n",
    "dict_comm_col_names = {\n",
    "    'Kraftwerksname': 'name',\n",
    "    'Unternehmen': 'company',\n",
    "    'Primärenergie-Basis': 'fuel',\n",
    "    'KWK (falls bekannt)': 'NEP_chp_bool',\n",
    "    'IPP': 'NEP_ipp_bool',\n",
    "    'Fernwärme Leistung (MW)': 'chp_capacity_uba',\n",
    "    'Anlagenart': 'technology',\n",
    "    'geplante Inbetriebnahme': 'commissioned',\n",
    "    'elektrische Nettoleistung (MW)': 'capacity'}\n",
    "\n",
    "dict_comm_BNetzA_names = {\n",
    "    'Anlagenname': 'name',\n",
    "    'Unternehmen': 'company',\n",
    "    'Energieträger': 'fuel',\n",
    "    'KWK (falls bekannt)': 'NEP_chp_bool',\n",
    "    'IPP': 'NEP_ipp_bool',\n",
    "    'Fernwärme Leistung (MW)': 'chp_capacity_uba',\n",
    "    'Voraussichtliche Aufnahme der kommerziellen Strom-einspeisung': 'commissioned',\n",
    "    'Geplante Netto-Nennleistung (elektrisch) der Investition in MW (Pumpspeicher: Turbinenbetrieb)': 'capacity'}\n",
    "\n",
    "cols_to_drop = [col for col in comm_conv_de.columns \n",
    "                if col not in dict_comm_col_names.keys()]\n",
    "cols_to_drop_BNetzA = [col for col in comm_BNetzA.columns \n",
    "                       if col not in dict_comm_BNetzA_names.keys()]\n",
    "\n",
    "comm_conv_de = comm_conv_de.rename(columns=dict_comm_col_names).drop(cols_to_drop, axis=1)\n",
    "comm_BNetzA = comm_BNetzA.rename(columns=dict_comm_BNetzA_names).drop(cols_to_drop_BNetzA, axis=1)\n",
    "\n",
    "# Replace CHP values in order to be able to use the same data prep steps\n",
    "comm_conv_de.loc[:, ['NEP_chp_bool', 'NEP_ipp_bool']].replace({'ja': 'Ja', 'nein': 'Nein'}, inplace=True)\n",
    "comm_BNetzA.loc[:, ['NEP_chp_bool', 'NEP_ipp_bool']].replace({'ja': 'Ja', 'nein': 'Nein'}, inplace=True)\n",
    "\n",
    "conv_de_new = pd.concat([conv_de_new, comm_conv_de, comm_BNetzA], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Append information on individual plants ('Datteln 4' & 'HKW Dieselstr.'):\n",
    "* Read in decommissioning data set for hardcoal\n",
    "* Access plant Datteln 4 and do some renaming\n",
    "* Append it to the new-built power plants data set for Germany\n",
    "* Assign CHP information (based on information from the plant operator Uniper: https://www.uniper.energy/de/datteln-4, accessed 03.11.2020)\n",
    "* Add missing capacity information for HKW Dieselstr. Halle from plant operators website:\n",
    "https://evh.de/privatkunden/unternehmen/energiepark/dieselstra%C3%9Fe, accessed 03.11.2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decomm_hardcoal_de = pd.read_csv(\n",
    "    main_path[\"inputs\"] + sub_path[\"pp_er\"] + input_file[\"decomm_hardcoal\"],\n",
    "    index_col=0, sep=\";\", decimal=\",\", usecols=list(range(13)))\n",
    "\n",
    "decomm_hardcoal_col_names = {\n",
    "    'Unternehmen': 'company',\n",
    "    'Kraftwerksname': 'name',\n",
    "    'Energieträger': 'fuel',\n",
    "    'Aufnahme der kommerziellen Stromerzeugung der derzeit in Betrieb befindlichen Erzeugungseinheit\\n(Datum/Jahr)':\n",
    "    'commissioned',\n",
    "    'Geplante Ausserbetriebnahme': 'shutdown',\n",
    "    'Netto-Nennleistung (elektrische Wirkleistung) in MW': 'capacity',\n",
    "}\n",
    "\n",
    "datteln4 = decomm_hardcoal_de[decomm_hardcoal_de['Kraftwerksname'] == 'Datteln 4'].rename(\n",
    "    columns=decomm_hardcoal_col_names).drop(\n",
    "    columns=[col for col in decomm_hardcoal_de.columns if col not in decomm_hardcoal_col_names.keys()])\n",
    "\n",
    "conv_de_new = conv_de_new.append(datteln4)\n",
    "conv_de_new.loc[conv_de_new['name'] == 'Datteln 4', \n",
    "                ['NEP_chp_bool', 'NEP_ipp_bool', 'chp_capacity_uba']] = ('Ja', 'Nein', 380)\n",
    "\n",
    "conv_de_new.loc[conv_de_new['name'] == 'HKW Dieselstr.\\n(Modernisierung)', ['capacity', 'chp_capacity_uba']] = (47, 34)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign missing values for new commissioned plants & drop the ones going into operation earlier than selected target year\n",
    "\n",
    "* Assign new values / missing values in the data set containing new plants:\n",
    "    * energy sources &rarr; Use names consistent with OPSD naming conventions\n",
    "    * country &rarr; Assign Germany (DE) for all plants\n",
    "    * status &rarr; Set status to operating (for 2030 consideration)\n",
    "    * eeg status &rarr; Assign no, i.e. no RES power plants under the German Renewable Energies Act (EEG)\n",
    "    * commissioning year &rarr; commissioned_last is the same as initial commissioning year\n",
    "    * technology &rarr; See below for the logic used\n",
    "    * capacity &rarr; Has to be distributed for some units (see below)\n",
    "* Drop power plants to be commissioned before the selected target year\n",
    "* Split data sets for power plants and pumped hydro as well as run of river plants since they are treated separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_de_new.loc[:,'fuel'].replace(\n",
    "    {'Pumpspeicher': 'Pumped storage',\n",
    "     'Erdgas': 'natgas',\n",
    "     'Kuppelgas': 'otherfossil',\n",
    "     'Sonstige': 'otherfossil',\n",
    "     'Mineralölprodukte': 'oil',\n",
    "     'Wasser': 'Run-of-river',\n",
    "     'Biomasse': 'biomass',\n",
    "     'Steinkohle': 'hardcoal',\n",
    "     'Mehrere Energieträger': 'natgas',\n",
    "     'Sonstige Energieträger\\n(nicht erneuerbar)': 'otherfossil'},\n",
    "    inplace=True)\n",
    "\n",
    "conv_de_new.loc[:,['country_geographical', 'country']] = 'DE'\n",
    "conv_de_new['status'] = 'operating'\n",
    "conv_de_new['eeg'] = 'no'\n",
    "conv_de_new['commissioned_last'] = conv_de_new['commissioned']\n",
    "conv_de_new = conv_de_new.loc[conv_de_new[\"commissioned\"] < year]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Technology is added according to the following logic:\n",
    "* If technology information does exist (new-built plants from the UBA list), assign a name that is consistent with the convention used here later on\n",
    "* If plant is a gas-fired combined cycle plant &rarr; Assign combined cycle (CC)\n",
    "* Else if plant is gas-fired &rarr; Assing gas turbine (GT)\n",
    "* Else &rarr; assign steam turbine (ST) for all remaining plants\n",
    "\n",
    "For some plants, no capacity values are given, instead a message _'be aware of 1.2 GW capacity limit'_ is stated.\n",
    "\n",
    "Energy regulatory background:\n",
    "* § 13k EnWG formerly allowed TSOs to build and operate gas-fired back up power plants up to an overall capacity of 2 GW resp. up to the amount of capacity which the German regulatory body (BNetzA) assumed to be necessary.\n",
    "* The BNetzA determined a maximum amount of 1.2 GW in 2017, see: https://www.bundesnetzagentur.de/SharedDocs/Downloads/DE/Sachgebiete/Energie/Unternehmen_Institutionen/Versorgungssicherheit/Berichte_Fallanalysen/BNetzA_Netzstabilitaetsanlagen13k.pdf?__blob=publicationFile&v=3 (accessed 22.10.2020)\n",
    "* These 1.2 GW served as a basis for the NEP 2030 (version 2019, 2nd draft) which is used here.\n",
    "* § 13k does no longer exist and is substituted by § 11 (3) EnWG which explicitly forbids TSOs to build and operate power plants by their own which would be not in line with the unbundling regulations.\n",
    "\n",
    "Approach applied here: The overall capacity of 1.2 GW for new-built gas power plants is equally split between the different plants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_de_new.loc[:, 'technology'].replace(\n",
    "    {'GuD': 'CC',\n",
    "     'GM': 'M',\n",
    "     'G/AK': 'CC',\n",
    "     'BGT': 'GT',\n",
    "     'BST': 'ST'}, \n",
    "    inplace=True)\n",
    "\n",
    "conditions = [(conv_de_new.name.str.contains('GUD|CCPP', \n",
    "                                          flags=IGNORECASE, \n",
    "                                          regex=True)) & (conv_de_new.fuel == 'natgas'),\n",
    "             (~conv_de_new.name.str.contains('GUD|CCPP', \n",
    "                                          flags=IGNORECASE, \n",
    "                                          regex=True)) & (conv_de_new.fuel == 'natgas')]\n",
    "choices = ['CC', 'GT']\n",
    "\n",
    "conv_de_new['technology_temp'] = np.select(conditions, choices, default='ST')\n",
    "\n",
    "conv_de_new.loc[conv_de_new['technology'].isna(), 'technology'] = conv_de_new['technology_temp']\n",
    "conv_de_new.drop(columns=['technology_temp'], inplace=True)\n",
    "\n",
    "amount = conv_de_new.loc[conv_de_new['capacity'] == '1,2-GW-Beschränkung beachten!'].shape[0]\n",
    "try:\n",
    "    abs_value = round(1200 / amount)\n",
    "    conv_de_new.loc[conv_de_new['capacity'] == '1,2-GW-Beschränkung beachten!', \n",
    "                'capacity'] = abs_value\n",
    "except ZeroDivisionError:\n",
    "    pass \n",
    "\n",
    "conv_de_new['capacity'] = conv_de_new['capacity'].astype(float)\n",
    "\n",
    "phes_de_new = conv_de_new.loc[conv_de_new['fuel'] == 'Pumped storage']\n",
    "ror_de_new = conv_de_new.loc[conv_de_new['fuel'] == 'Run-of-river']\n",
    "conv_de_new = conv_de_new.loc[~(conv_de_new.index.isin(phes_de_new.index.append(ror_de_new.index)))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### European power plant development\n",
    "\n",
    "For the European power plant park development, the TYNDP data is used.\n",
    "\n",
    "Steps applied:\n",
    "* Read in the TYNDP generation data (aggregated at the level of bidding zones):\n",
    "    * Use scenario data from scenario _Distributed Generation_ which has shares of 51% RES for electricity and 3.6% RES for gas in 2030\n",
    "    * Filter out the countries for closer consideration (using a regular expression)\n",
    "* Aggeregate capacities for country data set they belong to (for DK and FR)\n",
    "* Adjust naming of bidding zones to be consistent with the one used here\n",
    "* Rename column names such that they match the names for the energy carriers used here\n",
    "* Distribute aggregated data for Norway across the different bidding zones\n",
    "    * Capacities for Norway are aggregated for the bidding zones NO1, NO2 and NO5, but capacity information is needed at the level of individual bidding zones.\n",
    "    * As an assumption, capacities for the bidding zones are distributed in the same manner as capacities are distributed in the status quo. Therefore, the capacity shares in the status quo are determined in the first place.\n",
    "* Split the data set into conventional and RES as well as ROR data and do some renaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NO_dict = {}\n",
    "NO_zones = ['NO1', 'NO2', 'NO5']\n",
    "\n",
    "for country in NO_zones:\n",
    "    NO_dict[country] = conv_eu[\n",
    "        conv_eu['bidding_zone'] == country].capacity.sum()\n",
    "NO_cap = sum(NO_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_eu_2025_BEST = tools.extract_tyndp_capacities(\n",
    "    countries=countries_eu, no_dict=NO_dict, scenario=\"2025 BEST\",\n",
    "    path=main_path[\"inputs\"] + sub_path[\"pp_public\"] + input_file[\"tyndp_eu\"])\n",
    "\n",
    "pp_eu_2030_DG = tools.extract_tyndp_capacities(\n",
    "    countries=countries_eu, no_dict=NO_dict, scenario=\"2030 DG\",\n",
    "    path=main_path[\"inputs\"] + sub_path[\"pp_public\"] + input_file[\"tyndp_eu\"])\n",
    "\n",
    "RES_to_sep = ['solarPV', 'windonshore', 'windoffshore']\n",
    "Hydro_to_sep = ['PHES_capacity_pump', 'PHES_capacity', 'PHES_capacity_turbine']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_eu_2025_BEST = pp_eu_2025_BEST[(~pp_eu_2025_BEST['fuel'].isin(RES_to_sep + Hydro_to_sep))].copy()\n",
    "renewables_eu_2025_BEST = pp_eu_2025_BEST[pp_eu_2025_BEST['fuel'].isin(RES_to_sep)]\n",
    "pumpedstorage_eu_2025_BEST = pp_eu_2025_BEST.loc[pp_eu_2025_BEST['fuel'].isin(Hydro_to_sep)]\n",
    "\n",
    "conv_eu_2030_DG = pp_eu_2030_DG[(~pp_eu_2030_DG['fuel'].isin(RES_to_sep + Hydro_to_sep))].copy()\n",
    "renewables_eu_2030_DG = pp_eu_2030_DG[pp_eu_2030_DG['fuel'].isin(RES_to_sep)]\n",
    "pumpedstorage_eu_2030_DG = pp_eu_2030_DG.loc[pp_eu_2030_DG['fuel'].isin(Hydro_to_sep)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do some garbage collection\n",
    "try:\n",
    "    del (Hydro_to_sep, NO_cap, NO_dict, NO_zones, abs_value, amount, \n",
    "         cap1, cap2, choices, col, cols_to_drop, cols_to_drop_BNetzA,\n",
    "         comm_BNetzA, comm_BNetzA_newidx, comm_conv_de, cond1, cond2, conditions, countries_eu, country,\n",
    "         datteln4, decomm_hardcoal_col_names, dict_comm_BNetzA_names, dict_comm_col_names,\n",
    "         dict_nep_col_names, duplicated_comm1, duplicated_comm3, duplicated_comm_BNetzA2, duplicated_comm_BNetzA3,\n",
    "         duplicated_conv1, duplicated_conv2, el, empty_df_new, exclude, \n",
    "         filter_plants1, filter_plants2, filter_plants3,\n",
    "         idx1, idx2, idx_mapping1, include, interval, keys_BNetzA, last_idx_comm, match, \n",
    "         names, names_comm, names_comm_BNetzA, names_comm_BNetzA_set, names_comm_set, names_set,\n",
    "         nep_new, pp_eu, pp_eu_2025_BEST, pp_eu_2030_DG,\n",
    "         union1, union2, union3, values_BNetzA\n",
    "         )\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign technology\n",
    "\n",
    "- Steps applied for Germany\n",
    "    - Use data with results from manual research / matching of department (ER_man)<br>\n",
    "      _Caution: Data set contains duplicates &rarr; take the first occurence and drop the rest_\n",
    "    - Fill missing technology data from OPSD\n",
    "    \n",
    "- Steps applied for Europe\n",
    "    - Take technology data from OPSD\n",
    "    - Further missing values are assigned by energy carrier in the following way:<br>\n",
    "      natgas &rarr; GT<br>\n",
    "      rest &rarr; ST\n",
    "    \n",
    "- Additional steps taken for both\n",
    "    - Rename original technology name\n",
    "    - If plants technology is 'CC' and fuel is not in natgas or oil, 'ST' is assigned instead\n",
    "    - A technology and fuel combination is specified in order to assign efficiencies on this basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "techs = pd.read_csv(\n",
    "    main_path[\"inputs\"] + sub_path[\"pp_er\"] + input_file[\"techs_de\"],\n",
    "    sep=\";\", decimal=\",\"\n",
    ")\n",
    "dict_techs_ER_man = {'T': 'GT', 'SPP': 'ST'}\n",
    "techs = techs.drop_duplicates(subset='bnaID').set_index('bnaID').replace(dict_techs_ER_man)\n",
    "\n",
    "dict_techs_ospd = {'Steam turbine': 'ST', \n",
    "                   'Gas turbine': 'GT', \n",
    "                   'Combined cycle': 'CC', \n",
    "                   'Combustion Engine': 'M'}\n",
    "\n",
    "conv_de['technology'].replace(dict_techs_ospd, inplace=True)\n",
    "\n",
    "conv_de = conv_de.join(techs['ERman'], how='left')\n",
    "conv_de.loc[~conv_de['ERman'].isna(), 'technology'] = conv_de.loc[~conv_de['ERman'].isna(), 'ERman']\n",
    "conv_de.drop(columns=['ERman'], inplace=True)\n",
    "\n",
    "conv_de.loc[(conv_de['technology'] == 'CC')\n",
    "            & (~conv_de['fuel'].isin(['natgas', 'oil'])), 'technology'] = 'ST'\n",
    "\n",
    "conv_de['tech_fuel'] = conv_de['technology'] + '_' + conv_de['fuel']\n",
    "\n",
    "dict_techs_ppmatch = {'Steam Turbine': 'ST', \n",
    "                      'OCGT': 'GT', \n",
    "                      'CCGT': 'CC'}\n",
    "\n",
    "conv_eu['technology'].replace(dict_techs_ppmatch, inplace=True)\n",
    "\n",
    "dict_tech_eu = {'natgas': 'GT', \n",
    "                'hardcoal': 'ST', \n",
    "                'uranium': 'ST', \n",
    "                'otherfossil': 'ST',\n",
    "                'oil': 'ST', # efficiency later assigned on is worse than for GT\n",
    "                'lignite': 'ST', \n",
    "                'biomass': 'ST'}\n",
    "\n",
    "conv_eu.loc[conv_eu['technology'].isna(), 'technology'] = (\n",
    "    conv_eu.loc[conv_eu['technology'].isna(), 'fuel'].replace(dict_tech_eu))\n",
    "\n",
    "conv_eu.loc[(conv_eu['technology'] == 'CC')\n",
    "            & (~conv_eu['fuel'].isin(['natgas', 'oil'])), 'technology'] = 'ST'\n",
    "\n",
    "conv_eu['tech_fuel'] = conv_eu['technology'] + '_' + conv_eu['fuel']\n",
    "\n",
    "conv_de_new['tech_fuel'] = conv_de_new['technology'] + '_' + conv_de_new['fuel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do some garbage collection\n",
    "try:\n",
    "    del (dict_tech_eu, dict_techs_ER_man, dict_techs_ospd, dict_techs_ppmatch, techs)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign electrical efficiencies\n",
    "\n",
    "Data sources used and steps applied:\n",
    "- Basic data source is the given electrical efficiency information of OPSD\n",
    "- Missing values for efficiency are filled based on manual research by ER department. Several data sources are combined in the following priority order:\n",
    "    - manual researched efficiencies by Robin Claus (RC), 18.09.2018\n",
    "    - manual researched efficiencies from master thesis of Daniel Peschel (DP), 04.03.2016\n",
    "- Remaining missing values are filled by a linear regression approach from [DIW Data Documentation No. 72 from 2014](https://www.diw.de/documents/publikationen/73/diw_01.c.440963.de/diw_datadoc_2014-072.pdf) (see citation below)\n",
    "- For power plants with a latest commissioning date before 1950, the electrical efficiency value for the respective fuel / technology combination of 1990 is (arbitrarily) chosen.\n",
    "* For oil plants, a minimum efficiency of 30% is introduced.\n",
    "\n",
    "Egerer, Jonas, Gerbaulet, Clemens, Ihlenburg, Richard, Kunz, Friedrich, Reinhard, Benjamin, Hirschhausen, Christian von, Weber, Alexander, Weibezahn, Jens (2014): Electricity Sector Data for Policy-Relevant Modeling: Data Documentation and Applications to the German and European Electricity Markets, DIW and TU Berlin, WIP, DIW Data Documentation 72, Berlin, March 2014. © DIW Berlin, 2014.\n",
    "\n",
    "DIW regression / interpolation:\n",
    "- The efficiencies are based on the \"assumption table\" which includes the electrical efficiency information based on linear regressions by DIW.<br>\n",
    "- It includes values for: uranium (U/ST), lignite (L/ST), hard coal (H/ST), natural gas (NG/GT), oil (O/CB)\n",
    "- All other values were derived from research by Leticia Encinas Rosa (LER) and are time invariant (own assumptions)\n",
    "    \n",
    "For new-built power plants, the efficiency estimates from DIW are projected into the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data from manual research\n",
    "eff_RC = pd.read_csv(\n",
    "    main_path[\"inputs\"] + sub_path[\"pp_er\"] + input_file[\"eff_de\"], \n",
    "    index_col=0, sep=\";\", decimal=\",\").rename(\n",
    "        columns={'eta_el': 'efficiency_el_RC'})\n",
    "conv_de = conv_de.join(eff_RC['efficiency_el_RC'])\n",
    "\n",
    "chp_DP = pd.read_csv(\n",
    "    main_path[\"inputs\"] + sub_path[\"pp_er\"] + input_file[\"peschel\"], \n",
    "    index_col=1, sep=\";\", decimal=\",\"\n",
    ")\n",
    "chp_DP['efficiency_el_DP'] = chp_DP['Wirkungsgrad'].replace(0, np.nan)*0.01\n",
    "conv_de = conv_de.join(chp_DP['efficiency_el_DP'])\n",
    "\n",
    "conv_de.rename(columns = {'efficiency_data': 'efficiency_el_OPSD'}, inplace=True)\n",
    "\n",
    "conv_de['efficiency_el'] = np.nan\n",
    "\n",
    "# Fill the data gaps\n",
    "dict_eff_ordinv = ['efficiency_el_RC', 'efficiency_el_DP', 'efficiency_el_OPSD']\n",
    "for eff in dict_eff_ordinv:\n",
    "    conv_de.loc[(conv_de['efficiency_el'].isna()) & (~conv_de[eff].isna()), 'efficiency_el'] = (\n",
    "        conv_de.loc[(conv_de['efficiency_el'].isna()) & (~conv_de[eff].isna()), eff])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use DIW interpolation for efficiencies\n",
    "assumptions_eff_el = pd.read_csv(\n",
    "    main_path[\"inputs\"] + sub_path[\"assumptions\"] + input_file[\"efficiencies_el\"],\n",
    "    index_col=0, sep=\";\", decimal=\",\"\n",
    ")\n",
    "\n",
    "arr = conv_de.loc[conv_de['efficiency_el'].isna(), ['commissioned_last', 'tech_fuel']].to_numpy()\n",
    "conv_de.loc[conv_de['efficiency_el'].isna(), 'efficiency_el'] = (\n",
    "    [tools.assign_eff_el_interpol(yr, tf, assumptions_eff_el) \n",
    "     for yr, tf in arr])\n",
    "\n",
    "# Existing plants\n",
    "arr = conv_eu.loc[conv_eu['efficiency_el'].isna(), ['commissioned_last', 'tech_fuel']].to_numpy()\n",
    "conv_eu.loc[conv_eu['efficiency_el'].isna(), 'efficiency_el'] = (\n",
    "    [tools.assign_eff_el_interpol(yr, tf, assumptions_eff_el) \n",
    "     for yr, tf in arr])\n",
    "\n",
    "conv_de.drop(columns=['efficiency_el_OPSD', 'efficiency_source', 'efficiency_estimate',\n",
    "                      'efficiency_el_RC', 'efficiency_el_DP'], inplace=True)\n",
    "\n",
    "conv_de.loc[(conv_de['fuel'] == 'oil') & (conv_de['efficiency_el'] < 0.3), 'efficiency_el'] = 0.3\n",
    "\n",
    "# New built plants\n",
    "arr = conv_de_new.loc[:, ['commissioned_last', 'tech_fuel']].to_numpy()\n",
    "conv_de_new.loc[:, 'efficiency_el'] = (\n",
    "    [tools.assign_eff_el_interpol(yr, tf, assumptions_eff_el) \n",
    "     for yr, tf in arr])\n",
    "\n",
    "conv_de_new.drop(columns=['efficiency_data', 'efficiency_source', 'efficiency_estimate'], \n",
    "                 inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do some garbage collection\n",
    "try:\n",
    "    del (arr, assumptions_eff_el, chp_DP, dict_eff_ordinv, eff, eff_RC)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign CHP information\n",
    "\n",
    "Data sources used and steps applied:\n",
    "- The following CHP categories are distincted:\n",
    "    - industrial power plants (IPP)\n",
    "    - district heating power plants (CHP) and\n",
    "    - power plants that are dispatched solely on a electricity market basis, i.e. that do not have a minimum load profile to serve heat demand (EMB)\n",
    "- The following sources are evaluated in order to obtain information on CHP eligibility and type:\n",
    "    - OPSD power plant list (information if a plant uses CHP is from BNetzA; the type, i.e., IPP, CHP (district heating) is from UBA)\n",
    "    - NEP 2030 power plant list\n",
    "    - manual research information from master thesis of DP, 04.03.2016\n",
    "\n",
    "> _Note: CHP information is used in POMMES to assign minimum load values resp. profiles for these plants.<br>Heat usage in turn is out of the current scope of POMMES_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if year != 2017:\n",
    "    # Add missing IPP information for new power plants in OPSD list (ones not in list as of 2017)\n",
    "    companies = '|'.join(['BMW', 'K\\+S', 'Papierfabrik'])\n",
    "    diff_ix = ['BNA0083_SD', 'BNA0085b_SD', 'BNA0804', 'BNA1911', 'BNA1925', 'BNA1926',\n",
    "               'BNA1927', 'BNA1934', 'BNA1935', 'BNA1936', 'BNA1937', 'BNA1938',\n",
    "               'BNA1939', 'BNA1942', 'BNA1944', 'BNA1945', 'BNA1946', 'BNa1947']\n",
    "    conv_de.loc[conv_de.index.isin(diff_ix) & \n",
    "                conv_de['company'].str.contains(companies), 'type'] = 'IPP'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from OPSD\n",
    "conv_de.rename(columns={'chp': 'OPSD_chp_bool', \n",
    "                        'type': 'OPSD_chp_type'}, inplace=True)\n",
    "chp_type = conv_de[['OPSD_chp_bool', 'OPSD_chp_type']].copy()\n",
    "chp_type['OPSD_ipp_bool'] = np.where(chp_type['OPSD_chp_type'] == 'IPP', 'yes', 'no')\n",
    "\n",
    "# from NEP 2030\n",
    "chp_type = chp_type.join(nep[['Wärmeauskopplung KWK (Ja/Nein)', \n",
    "                              'Annahmen ÜNB:\\nIndustriekraftwerk (Ja/Nein)']]).rename(\n",
    "    columns={'Wärmeauskopplung KWK (Ja/Nein)': 'NEP_chp_bool',\n",
    "             'Annahmen ÜNB:\\nIndustriekraftwerk (Ja/Nein)': 'NEP_ipp_bool'})\n",
    "\n",
    "chp_type_new = conv_de_new[['NEP_chp_bool', 'NEP_ipp_bool']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Existing plants\n",
    "chp_type['chp_bool'] = np.where((chp_type['OPSD_chp_bool'] == 'yes') | (chp_type['NEP_chp_bool'] == 'Ja'),\n",
    "                                'yes', 'no')\n",
    "\n",
    "conditions = [(chp_type['chp_bool'] == 'yes') \n",
    "              & (chp_type['OPSD_ipp_bool'] != 'yes') & (chp_type['NEP_ipp_bool'] != 'Ja'),\n",
    "              (chp_type['OPSD_ipp_bool'] == 'yes') | (chp_type['NEP_ipp_bool'] == 'Ja')]\n",
    "choices = ['chp', 'ipp']\n",
    "\n",
    "chp_type['chp_type'] = np.select(conditions, choices, default='emb')\n",
    "\n",
    "conv_de.drop(columns=['OPSD_chp_bool', 'OPSD_chp_type'], inplace=True)\n",
    "conv_de = conv_de.join(chp_type[['chp_bool', 'chp_type']], how='left').rename(columns={'chp_type': 'type'})\n",
    "\n",
    "# New built plants\n",
    "chp_type_new['chp_bool'] = np.where(chp_type_new['NEP_chp_bool'] == 'Ja',\n",
    "                                    'yes', 'no')\n",
    "\n",
    "conditions = [(chp_type_new['chp_bool'] == 'yes') & (chp_type_new['NEP_ipp_bool'] == 'Nein'), \n",
    "              chp_type_new['NEP_ipp_bool'] == 'Ja']\n",
    "choices = ['chp', 'ipp']\n",
    "\n",
    "chp_type_new['chp_type'] = np.select(conditions, choices, default='emb')\n",
    "\n",
    "conv_de_new.drop(columns=['NEP_chp_bool', 'NEP_ipp_bool', 'type', 'chp'], inplace=True)\n",
    "conv_de_new = conv_de_new.join(chp_type_new[['chp_bool', 'chp_type']], how='left').rename(\n",
    "    columns={'chp_type': 'type'})\n",
    "\n",
    "# By assumption all pps in EU are emb\n",
    "conv_eu['type'] = 'emb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do some garbage collection\n",
    "try:\n",
    "    del (choices, chp_type, chp_type_new, companies, conditions, diff_ix, nep)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop information for power plants not needed\n",
    "\n",
    "Steps applied:\n",
    "- Remove plants that were shutdown prior to 2017 from the power plants list &rarr; _NOTE: 2017 is the base year used for historical backcasting. In priciple, any year up to 2018 (state of the data set) can be used here withouth affecting the future power plants data set that is created afterwards._\n",
    "- Drop all information not needed for the model run in _POMMES_ (for the German and the European power plants list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# German power plants\n",
    "conv_de.drop(index=conv_de[conv_de['shutdown'] < year].index, inplace=True)\n",
    "conv_de.drop(index=conv_de[~((conv_de['status']=='shutdown') |\n",
    "                             (conv_de['status']=='operating'))].index, inplace=True)\n",
    "\n",
    "conv_de.drop(columns=['name', 'block_bnetza', 'name_uba', 'company', 'state', 'lat', 'lon',\n",
    "                      'country_geographical',  'technology', 'chp_capacity_uba',\n",
    "                      'commissioned', 'commissioned_original', 'retrofit', 'eic_code',\n",
    "                      'eic_code_block', 'eeg', 'network_node', 'voltage', 'chp_bool'], inplace=True)\n",
    "\n",
    "# European power plants\n",
    "conv_eu = conv_eu[['fuel', 'capacity', 'efficiency_el', 'bidding_zone', 'tech_fuel', 'type']]\n",
    "conv_eu.rename(columns={'bidding_zone': 'country'}, inplace=True)\n",
    "\n",
    "# New built German power plants\n",
    "conv_de_new.drop(columns=['name', 'block_bnetza', 'name_uba', 'company', 'state', 'lat', 'lon',\n",
    "                      'country_geographical',  'technology', 'chp_capacity_uba',\n",
    "                      'commissioned', 'commissioned_original', 'retrofit', 'eic_code',\n",
    "                      'eic_code_block', 'eeg', 'network_node', 'voltage', 'chp_bool'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign commissioning and decomissioning estimates\n",
    "\n",
    "Steps applied:\n",
    "- For plants which don't have a commissioning date, assign 1990\n",
    "- Calculate year when plant should be commissioned based on a lifetime calculation\n",
    "- Create a table which is indexed by power plants (index) and years (columns)\n",
    "- Fill in commissioning / decommissioning information:\n",
    "    - If a plant is commissioned in a certain year, the respective cell contains its capacity with a positive sign\n",
    "    - If a plant is decommissioned in a certain year, the respective cell contains its capacity with a negative sign\n",
    "\n",
    "**Logic for determining decommissioning years:**\n",
    "* Check if decommissioning year is prior to the starting time of the simulation run\n",
    "* If so, check what type of plant is given:\n",
    "    * For some plants, shutdown information has been put together by Julien Faist in a decommissionings list. Primary data sources are:\n",
    "        * BNetzA (2019): Kraftwerksstilllegungsanzeigenliste, as of 01.04.2019, https://www.bundesnetzagentur.de/DE/Sachgebiete/ElektrizitaetundGas/Unternehmen_Institutionen/Versorgungssicherheit/Erzeugungskapazitaeten/KWSAL/KWSAL_node.html, accessed prior to 01.04.2020.\n",
    "        * BNetzA (2019): Kraftwerksliste zum erwarteten Zu. und Rückbau 2019 bis 2022, as of 01.04.2019, https://www.bundesnetzagentur.de/DE/Sachgebiete/ElektrizitaetundGas/Unternehmen_Institutionen/Versorgungssicherheit/Erzeugungskapazitaeten/Kraftwerksliste/kraftwerksliste-node.html, accessed prior to 01.04.2020.\n",
    "        * AtG: https://www.gesetze-im-internet.de/atg/, accessed 03.11.2020.\n",
    "    * For coal power plans, a shutdown plan is derived from the KVBG: https://www.gesetze-im-internet.de/kvbg/, accessed 03.11.2020, which in turn is based on the recommendations made by the so-called \"Kohlekommission\" (Abschlussbericht der Kommission „Wachstum, Strukturwandel und Beschäftigung“ 2019). The more ambitious strategy is used, i.e. a shutdown of all power plants until 2035 already instead of 2038. The data has been put together by Julien Faist in a decommissionings list for coal power plants.\n",
    "    * For the remaining plants, decommissionings occur in the x years (default: x=10) following a number of years (offset; default=5) after the start year, i.e. the one for which no OPSD data is available anymore. In each year, around 100/x% of the overall capacity to be decommissioned is actually decommissioned, whereby only entire blocks or plants are decommissioned. The order is determined by plant age and electrical efficiency.\n",
    "* Else set decommissioning year to commissioning year + unit lifetime\n",
    "\n",
    "> _NOTE:_\n",
    "> _For decommissionings based on unit age, the five years after the start year are chosen since_\n",
    "> * _Some plants have already exceeded their lifetime by far and_\n",
    "> * _Plant operators are obliged to report shutdowns planned within the next couple of years._\n",
    "\n",
    "Updated sources:\n",
    "* BNetzA (2020): Kraftwerksstilllegungsanzeigenliste, as of 15.04.2020, https://www.bundesnetzagentur.de/DE/Sachgebiete/ElektrizitaetundGas/Unternehmen_Institutionen/Versorgungssicherheit/Erzeugungskapazitaeten/KWSAL/KWSAL_node.html, accessed 05.01.2021.\n",
    "* BNetzA (2020): Kraftwerksliste Bundesnetzagentur zum erwarteten Zu- und Rückbau 2019 bis 2022, https://www.bundesnetzagentur.de/DE/Sachgebiete/ElektrizitaetundGas/Unternehmen_Institutionen/Versorgungssicherheit/Erzeugungskapazitaeten/Kraftwerksliste/kraftwerksliste-node.html, as of 01.04.2020, accessed 05.01.2021."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add exogeneous decommissioning info (KWSAL, AtG, other)\n",
    "\n",
    "Steps applied:\n",
    "* Include exogenous decommissioning information which has been put together by JF.\n",
    "* Include exogeneous decommissioning information provided by the BNetzA from the power plants list on expected commissionings and decommissionings as well as the power plants list for reported shutdown plans (KWSAL).\n",
    "* Handle duplicates in the BNetzA lists.\n",
    "* Exclude system relevant power plants (which are assumed to be kept online).\n",
    "* Determine the plants in the decommissioning data set which are still included in the OPSD data set.\n",
    "* Assign shutdown dates either from Julien's research or from the updated BNetzA list for those plants.\n",
    "* Assing decommissioning assumptions for the remaining plants:\n",
    "    * Filter for plants in the KWSAL which are not system-relevant and which do neither appear in the decommissionings lists already evaluated (JF's list and the BNetzA commissionings and decommissionings list) nor in the data set of the conventional plants for Germany.\n",
    "    * Assign a (single) shutdown year by assumption.\n",
    "\n",
    "> _NOTE:_\n",
    "> * _The system relevance decision has to be renewed at least every two years by the TSOS in agreement with the BNetzA. So the respective plants might be decommissioned later on._\n",
    "> * _There are some plants which are not included in the conv_de dataset anymore, but still appear in the KWSAL. This is due to the fact that their status is not 'operational' anymore. Hence, these plants are already some kinds of special cases outside the EOM such as security reserve of lignite units and thus do not (explicitly) need to be considered in POMMES._\n",
    "> _The remainder of plants is either dropped from the conventional data set due to its shutdown status or already covered by the other decommissionings lists._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Research by JF\n",
    "decomm_conv_de = pd.read_csv(\n",
    "    main_path[\"inputs\"] + sub_path[\"pp_er\"] + input_file[\"decomm\"],\n",
    "    index_col=0, sep=\";\"\n",
    ")\n",
    "\n",
    "decomm_conv_de = decomm_conv_de[decomm_conv_de['Systemrelevanz'].isna()]\n",
    "\n",
    "decomm_idx = decomm_conv_de.index.intersection(conv_de.index)\n",
    "\n",
    "conv_de.loc[decomm_idx,'shutdown'] = decomm_conv_de.loc[decomm_idx, 'Geplante Ausserbetriebnahme']\n",
    "\n",
    "# BNetzA list\n",
    "decomm_BNetzA = pd.read_excel(\n",
    "    main_path[\"inputs\"] + sub_path[\"pp_public\"] + input_file[\"comm_decomm_BNetzA\"],\n",
    "    sheet_name='Zu und Rückbau Bund Tab', skiprows=24, nrows=28, index_col=0\n",
    ")\n",
    "\n",
    "trimmed_idx = decomm_BNetzA.index.str.strip('*').str.split(', ')\n",
    "decomm_ids = [item for sublist in list(trimmed_idx) for item in sublist]\n",
    "\n",
    "# Create a new entry for each power plant in BNetzA list\n",
    "for idx in list(trimmed_idx):\n",
    "    if len(idx) > 1:\n",
    "        for subidx in idx:\n",
    "            decomm_BNetzA.loc[subidx] = decomm_BNetzA.loc[decomm_BNetzA.index.str.contains(subidx)].values[0]\n",
    "            \n",
    "conv_de.loc[[el for el in decomm_ids \n",
    "             if el in conv_de.index], 'shutdown'] = decomm_BNetzA[\n",
    "    'Voraussichtlicher Zeitpunkt der endgültigen Aufgabe (Jahr) gemäß Unternehmensplanung']\n",
    "\n",
    "# Do some corrections\n",
    "conv_de.at['BNA0969b', 'shutdown'] = decomm_BNetzA.at[\n",
    "    'BNA0969b*', \n",
    "    'Voraussichtlicher Zeitpunkt der endgültigen Aufgabe (Jahr) gemäß Unternehmensplanung']\n",
    "\n",
    "conv_de['shutdown'].replace({'2020 bis 2022': '2021',\n",
    "                             '2021 bis 2023': '2022'}, inplace=True)\n",
    "\n",
    "# KWSAL list (BDEW)\n",
    "decomm_KWSAL = pd.read_csv(\n",
    "    main_path[\"inputs\"] + sub_path[\"pp_public\"] + input_file[\"KWSAL\"],\n",
    "    encoding=\"latin_1\", index_col=0, delimiter=';', decimal=\",\"\n",
    ")\n",
    "\n",
    "unique_KWSAL_idx = decomm_KWSAL.index.str.split('\\n')\n",
    "unique_KWSAL_id_flat = [item for sublist in unique_KWSAL_idx for item in sublist]\n",
    "\n",
    "for idx in list(unique_KWSAL_idx):\n",
    "    if len(idx) > 1:\n",
    "        for subidx in idx:\n",
    "            decomm_KWSAL.loc[subidx] = decomm_KWSAL.loc[decomm_KWSAL.index.str.contains(subidx)].values[0]\n",
    "\n",
    "# Handle different approaches for HKW Wilmersdorf (blocks 2 and 3 are aggregated, while block 1 received a new index)\n",
    "duplicated_KWSAL_idx = {}\n",
    "\n",
    "decomm_KWSAL_multi = decomm_KWSAL.set_index('Kraftwerksblock', append=True)\n",
    "duplicated_KWSAL = decomm_KWSAL_multi[(decomm_KWSAL_multi.index.get_level_values(0).duplicated(keep=False))\n",
    "                         & (decomm_KWSAL_multi.index.get_level_values(1) == 'HKW Wilmersdorf GT 1')].index\n",
    "\n",
    "for el in duplicated_KWSAL:\n",
    "    idx_el = decomm_KWSAL_multi.index.get_loc(el)\n",
    "    duplicated_KWSAL_idx[el[0]] = idx_el\n",
    "\n",
    "as_list = decomm_KWSAL.index.tolist()\n",
    "for k, v in duplicated_KWSAL_idx.items():\n",
    "    as_list[v] = k + '_SD'\n",
    "decomm_KWSAL.index = as_list\n",
    "\n",
    "decomm_KWSAL = decomm_KWSAL.groupby(decomm_KWSAL.index).agg({\n",
    "    'Kraftwerksbetreiber': 'first', \n",
    "    'Kraftwerksblock': sum,\n",
    "    'Netto-Nennleistung in MW laut KW-Liste': sum,\n",
    "    'Stilllegungsanzeigentyp': 'first',\n",
    "    'Systemrelevanz von zur Stilllegung angezeigten KW-Blöcken gemäß ÜNB': 'first'})\n",
    "\n",
    "sd_plants = decomm_KWSAL[\n",
    "    ~(decomm_KWSAL['Systemrelevanz von zur Stilllegung angezeigten KW-Blöcken gemäß ÜNB'] == 'x')].index\n",
    "\n",
    "conv_de.loc[conv_de.shutdown.isna() & \n",
    "            conv_de.index.isin(conv_de.index.intersection(sd_plants)), 'shutdown'] = shutdown_assumption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add exogenous decommissioning for coal (KVBG)\n",
    "\n",
    "Steps applied:\n",
    "* Include exogenous decommissioning information which has been put together by Julien Faist from the German Kohleverstromungsbeendigungsgesetz (KVBG) based on the Kohlekommission's recommendations for a coal phase out plan until 2035.\n",
    "* Determine the plants from coal phase out list occuring in the conventional power plants list (from OPSD) but not in the already evaluated decommissioning list from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in and combine data sets for lignite and hardcoal\n",
    "decomm_lignite_de = pd.read_csv(\n",
    "    main_path[\"inputs\"] + sub_path[\"pp_er\"] + input_file[\"decomm_lignite\"],\n",
    "    index_col=0, sep=\";\", decimal=\",\", usecols=list(range(13)))\n",
    "\n",
    "decomm_coal_de = pd.concat([decomm_lignite_de, decomm_hardcoal_de])\n",
    "\n",
    "decomm_coal_idx = pd.Index([el for el in decomm_coal_de.index \n",
    "                            if el in conv_de.index \n",
    "                            and el not in decomm_idx])\n",
    "\n",
    "# Assign shutdown dates from Julien's research (possible path derived from KVBG)\n",
    "conv_de.loc[decomm_coal_idx,'shutdown'] = decomm_coal_de.loc[decomm_coal_idx, 'Geplante Ausserbetriebnahme']\n",
    "\n",
    "all_decomm_idx = decomm_coal_idx.union(decomm_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add exogeneous decommissionings based on unit lifetime or assumption\n",
    "\n",
    "Steps applied:\n",
    "* Combine data sets for existing and new-built units with some technology assumptions given in a separate file.\n",
    "* Unit lifetime is determined based on assumptions. &rarr; _NOTE: Unit lifetimes given here seem to be rather conservative estimates. It is assumed that unit lifetime estimates from the literature can be prolongued by 20 years through retrofits._\n",
    "* Decommissioning information for new-built plants &rarr; Solely based on unit age except for the one coal plant 'Datteln 4'; no extension of lifetimes is allowed through retrofits.\n",
    "* Plants which have exceeded their lifetime are to be decomissioned based on unit age: Decommissionings occur in the x years (default: x=10) following a number of years (offset; default=5) after the start year, i.e. the one for which no OPSD data is available anymore. In each year, around 100/x% of the overall capacity to be decommissioned is actually decommissioned, whereby only entire blocks or plants are decommissioned. The order is determined by plant age and electrical efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assumptions = pd.read_csv(\n",
    "    main_path[\"inputs\"] + sub_path[\"assumptions\"] + input_file[\"tech_assumptions\"], \n",
    "    index_col=0, sep=\";\", decimal=\",\"\n",
    ")\n",
    "assumptions.drop(columns=['min_load_factor'], inplace=True)\n",
    "\n",
    "conv_de = conv_de.merge(assumptions, left_on='tech_fuel', right_index=True, how='left')\n",
    "conv_de_new = conv_de_new.merge(assumptions, left_on='tech_fuel', right_index=True, how='left')\n",
    "\n",
    "# Assumption: power plants without commissioning year were commissioned in 1990\n",
    "conv_de.loc[conv_de['commissioned_last'].isna(), 'commissioned_last'] = 1990\n",
    "\n",
    "# start_year is the one following the latest OPSD data state (end of 2018)\n",
    "# unit lifetimes are assumed to be prolongued by 20 years through retroffiting\n",
    "start_year = 2019\n",
    "conv_de['decommissioned_calc'] = conv_de['commissioned_last'] + conv_de['unit_lifetime'] + 20\n",
    "conv_de.loc[conv_de['shutdown'].isna(), 'shutdown'] = conv_de['decommissioned_calc']\n",
    "\n",
    "plants_to_decomm = conv_de.loc[~(conv_de['shutdown'].isna()) \n",
    "                               & (conv_de['decommissioned_calc'] <= start_year) \n",
    "                               & ~(conv_de['fuel'] == 'uranium')]\n",
    "\n",
    "# New plants: Decommissioned based on unit age; no extension through retrofits\n",
    "conv_de_new['decommissioned_calc'] = conv_de_new['commissioned_last'] + conv_de_new['unit_lifetime']\n",
    "conv_de_new.loc[conv_de_new['shutdown'].isna(), 'shutdown'] = conv_de_new['decommissioned_calc']\n",
    "\n",
    "# Handle plants that already exceeded their calculated lifetimes\n",
    "plants_to_decomm = plants_to_decomm.sort_values(by=['commissioned_last','efficiency_el'], \n",
    "                                                ascending=True)\n",
    "\n",
    "# x is the number of years over which the decommissionings shall be spread\n",
    "# offset is the number of years after the start year to start with the decommissionings\n",
    "x = 10\n",
    "offset = 5\n",
    "number_plants = plants_to_decomm.shape[0]\n",
    "plants_per_year = round(number_plants / x)\n",
    "counter = 0\n",
    "\n",
    "for iter_year in range(start_year+offset, start_year+offset+x-1):    \n",
    "    idx = plants_to_decomm.iloc[counter:counter+plants_per_year].index\n",
    "    conv_de.loc[idx, 'shutdown'] = iter_year\n",
    "    counter += plants_per_year\n",
    "\n",
    "idx = plants_to_decomm.iloc[counter:].index\n",
    "conv_de.loc[idx, 'shutdown'] = year+offset+x-1\n",
    "\n",
    "conv_de['shutdown'] = conv_de['shutdown'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop the already decommissioned plants as of the target year:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decomm_idx = conv_de[conv_de['shutdown'] < year].index\n",
    "cap_for_decommissioning = conv_de.loc[decomm_idx, \"capacity\"].sum()\n",
    "decomm_new_idx = conv_de_new[conv_de_new['shutdown'] < year].index\n",
    "conv_de = conv_de.drop(decomm_idx)\n",
    "remaining_capacity_conv = conv_de.capacity.sum()\n",
    "\n",
    "print(f\"Summary statistics on capacity development for Germany until {year}:\")\n",
    "print(66 * \"-\")\n",
    "print(f'Overall capacity to be decommissioned:\\t\\t{round(cap_for_decommissioning):,} MW')\n",
    "print(f'Remaining capacity (excluding new-built units):\\t{round(remaining_capacity_conv):,} MW')\n",
    "print(f'Overall capacity of new-built power plants:\\t{round(conv_de_new.capacity.sum()):,} MW')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do some garbage collection\n",
    "try:\n",
    "    del (all_decomm_idx, as_list, cap_for_decommissioning, counter,\n",
    "         decomm_BNetzA, decomm_KWSAL, decomm_KWSAL_multi, \n",
    "         decomm_coal_de, decomm_coal_idx, decomm_conv_de,\n",
    "         decomm_hardcoal_de, decomm_ids, decomm_idx, decomm_lignite_de,\n",
    "         decomm_new_idx, duplicated_KWSAL, duplicated_KWSAL_idx, el, idx, idx_el,\n",
    "         iter_year, k, number_plants, offset, plants_per_year, plants_to_decomm,\n",
    "         sd_plants, shutdown_assumption, start_year, subidx, \n",
    "         trimmed_idx, unique_KWSAL_id_flat, unique_KWSAL_idx, v, x)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add new-built plants by assumption for preventing loss of load\n",
    "\n",
    "Please refer to [this section](#Transformers-(addition)) below since data for RES is needed for proper adequacy evaluation.\n",
    "\n",
    "> _NOTE:_\n",
    "> * _The overall capacity for the target year derived from plans for plants to be commissioned might not be sufficient to meet the demand._\n",
    "> * _Hence, if necessary, the capacity gaps are filled up according to the scheme of the capacity balance (sheet) for Germany._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign minimum load profiles\n",
    "Steps applied:\n",
    "- Non-CHP and elictricity market-based CHP ('emb') units are assigned a minimum load value of zero, hence, no minimum load is applicable for these units and they can be switched off.\n",
    "- District heating ('chp') plants and IPPs are assigned a time variant minimum load, depending on heating demand and industry shift profiles, respectively.\n",
    "\n",
    "**@YW: What about this section?! I see two options:**\n",
    "* Integrate your clean min loads notebook or\n",
    "* Leave it separate and remove this section here.\n",
    "\n",
    "**@YW: Be aware that we are only working with one data set now! It seems like BNA0086 is a shutdown candidate. If you still want to derive from that plant, you have to ensure to obtain its capacity earlier in the script (before decommissioning happens.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Industry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_de['min_load_LP'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_de.loc[conv_de['type'] == 'emb', 'min_load_LP'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "when2heat = pd.read_csv(\n",
    "    main_path[\"inputs\"] + sub_path[\"timeseries\"] + 'when2heat.csv', encoding='cp1252', sep=';',\n",
    "    usecols=['utc_timestamp', 'DE_heat_demand_total'], parse_dates=['utc_timestamp'],\n",
    "    index_col='utc_timestamp'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_entsoe_chp(name):\n",
    "    load = pd.read_csv('../raw_data_input/timeseries/min_loads/'+name+'.csv', index_col=0, parse_dates=[0])\n",
    "    df = pd.DataFrame(index=pd.date_range(start='2017-01-01 00:00:00', end='2017-12-31 23:00:00', freq='H', tz='UTC'))\n",
    "    df = df.join(load)\n",
    "    df = df.interpolate('linear')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "district_heat_demand = when2heat['DE_heat_demand_total'].loc['2013-01-01T00:00:00Z':'2013-12-31T23:00:00Z']\n",
    "district_heat_demand = district_heat_demand/district_heat_demand.max()\n",
    "\n",
    "min_load_ts = district_heat_demand.to_frame().rename(columns={'DE_heat_demand_total': 'chp'})\n",
    "# following only applies for oil other fossils etc.\n",
    "min_load_ts.loc[:, 'chp'] = np.maximum(min_load_ts.loc[:, 'chp'] - 0.5, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It seemed liked min loads are too high, so I subtract a bit here\n",
    "min_load_ts['chp_natgas'] = (load_entsoe_chp('BerlinMitte_BNA0073') / conv_de.loc['BNA0073', 'capacity']).values - 0.1\n",
    "min_load_ts['chp_lignite'] = 0.5*(\n",
    "    (load_entsoe_chp('Lippendorf_BNA0115') / conv_de.loc['BNA0115', 'capacity']).values +\\\n",
    "    (load_entsoe_chp('Lippendorf_BNA0116') / conv_de.loc['BNA0116', 'capacity']).values) - 0.1\n",
    "try:\n",
    "    min_load_ts['chp_hardcoal'] = 0.5*(\n",
    "        (load_entsoe_chp('ReuterWest_BNA0086') / conv_de.loc['BNA0086', 'capacity']).values +\\\n",
    "        (load_entsoe_chp('ReuterWest_BNA0087') / conv_de.loc['BNA0087', 'capacity']).values) - 0.1\n",
    "except KeyError:\n",
    "    pass\n",
    "\n",
    "min_load_ts['ipp'] = np.where((min_load_ts.index.hour > 6) & (min_load_ts.index.hour < 22), 0.9, 0.7)\n",
    "min_load_ts[min_load_ts < 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FR = tools.load_entsoe_generation_data('FR')\n",
    "AT = tools.load_entsoe_generation_data('AT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_load_ts['FR_natgas'] = np.round(\n",
    "    FR['Fossil Gas  - Actual Aggregated [MW]'] / FR['Fossil Gas  - Actual Aggregated [MW]'].max(), 4)\n",
    "min_load_ts['AT_natgas'] = np.round(\n",
    "    AT['Fossil Gas  - Actual Aggregated [MW]'] / AT['Fossil Gas  - Actual Aggregated [MW]'].max(), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_load_ts.index= pd.date_range(start='2017-01-01 00:00:00', end='2017-12-31 23:00:00', freq='H')\n",
    "\n",
    "min_load_ts = min_load_ts.round(4)\n",
    "min_load_ts = tools.reindex_time_series(min_load_ts, year)\n",
    "min_load_ts.to_csv(\n",
    "    main_path[\"outputs\"] + output_file[\"transformers_minload_ts\"] + \"_\" + str(year) + \".csv\"\n",
    ")\n",
    "min_load_ts.to_excel(writer, output_file[\"transformers_minload_ts\"] + \"_\" + str(year))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_ = district_heat_demand.plot()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do some garbage collection\n",
    "try:\n",
    "    del (FR, district_heat_demand, min_load_ts, when2heat)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign gradients and minimum loads\n",
    "\n",
    "Steps applied:\n",
    "- Drop columns not needed anymore from data set and do some column renaming\n",
    "- Add estimate for gradients from assumption table and convert values (from %/min to MW/hour)\n",
    "- Scale maximum load values for Germany based on historic market data\n",
    "- Assign minimum load values per technology for German plants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_keep = ['country', 'fuel', 'tech_fuel', 'capacity', \n",
    "                'efficiency_el', 'type', 'min_load_LP']\n",
    "cols_to_keep.extend(assumptions.columns)\n",
    "\n",
    "conv_de.drop(columns=conv_de.columns[~conv_de.columns.isin(cols_to_keep)], \n",
    "             inplace=True)\n",
    "\n",
    "# Assign gradients for Europe separately\n",
    "conv_eu = conv_eu.merge(assumptions, left_on='tech_fuel', right_index=True, how='left')\n",
    "\n",
    "conv_eu['grad_pos'] = np.minimum(1, 60 * conv_eu['load_grad_relative'])\n",
    "conv_eu['grad_neg'] = np.minimum(1, 60 * conv_eu['load_grad_relative'])\n",
    "\n",
    "conv_eu.drop(columns=['tech_fuel', 'load_grad_relative'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From exploration of German Entsoe electricity generation data:\n",
    "- min loads & max feedin (MW) (in total):\n",
    "    - lignite: [0.249181, 18316.25]\n",
    "    - hardcoal: [0.03616, 18447.75]\n",
    "    - nuclear: [0.329853, 10334.25]\n",
    "    - natural gas: [0.100636, 8684.75]\n",
    "    - pumped storage, pump: [0, 4475.25]\n",
    "    - pumped stroage, turbine: [0, 8680.75]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**@YW: This clearly needs a rework. Thank you.**\n",
    "\n",
    "<span style=\"color:red\">**COMMENT BY YW: Important: This needs a rework in case we ensure not minimal loads but fixed ones.**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save original aggregated capacites to estimate fixed costs later on\n",
    "aggregated_capacities_de = conv_de.groupby('fuel').agg({'capacity':sum})\n",
    "\n",
    "# Empirically observed maximum loads as shares of installed capatity\n",
    "max_load_dict = {'lignite': 18316.25/18900.0, \n",
    "                 'hardcoal': 18447.75/27202.04, \n",
    "                 'uranium': 10334.25/10800.0, \n",
    "                 'natgas': (8684.75+500)/24168.93}\n",
    "\n",
    "if year >= 2022:\n",
    "    max_load_dict.pop('uranium', None)\n",
    "\n",
    "for fuel in max_load_dict.keys():\n",
    "     conv_de.loc[(conv_de['country'] == 'DE') \n",
    "                 & (conv_de['fuel'] == fuel), 'capacity'] *= max_load_dict[fuel]\n",
    "\n",
    "min_load_dict = {'lignite': 0.4, \n",
    "                 'hardcoal': 0.25, \n",
    "                 'uranium': 0.5, \n",
    "                 'natgas': 0.2}\n",
    "\n",
    "# Convert gradients from %/min to MW/hour (relative values)\n",
    "conv_de = tools.assign_gradients_and_min_loads(conv_de, min_load_dict)\n",
    "\n",
    "conv_de_new.drop(columns=conv_de_new.columns[~conv_de_new.columns.isin(cols_to_keep)], \n",
    "                 inplace=True)\n",
    "\n",
    "try:\n",
    "    conv_de_new = tools.assign_gradients_and_min_loads(conv_de_new, min_load_dict)\n",
    "# For status quo analyses, there are no new-built plants to be considered\n",
    "except ValueError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do some garbage collection\n",
    "try:\n",
    "    del (aggregated_capacities_de, assumptions, fuel, max_load_dict)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Prepare conventional power plant data for usage in oemof.solph\n",
    "\n",
    "After finalizing the conventional power plants data set, this section serves to prepare the data for the usage in the oemof.solph terminology used by _POMMES_. Therefore, European power plant data is aggregated and identifiers as well as electricity buses are created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate European data and create a common data basis for the status quo\n",
    "\n",
    "Steps applied:\n",
    "- Aggregate European power plant data by country and fuel\n",
    "- Derive maximum, minimum and gradient values from historical ENTSO-E data. Maximum gradients are obtained from the 99% percentile of differences in power output between consecutive hours\n",
    "- Create a common data basis for Germany and Europe by concatenating the detailed German and the aggregated European power plant data\n",
    "\n",
    "**@YW: Summing up both capacity and gradients seems like duplicated here ...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wm = lambda x: np.average(x, weights=conv_eu.loc[x.index, 'capacity'])\n",
    "agg_dict = dict([(key, (wm)) for key in conv_eu.columns if key not in ['fuel', 'country']])\n",
    "for key in ['capacity', 'grad_pos', 'grad_neg']:\n",
    "    agg_dict[key] = 'sum'\n",
    "agg_dict['type'] = 'first'\n",
    "\n",
    "conv_eu_agg = conv_eu.groupby(['country', 'fuel'], as_index=False).aggregate(agg_dict)\n",
    "\n",
    "conv_eu_agg['min_load_factor'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get ENTSOE max, min and max gradient values for conventionals and adjust accordingly\n",
    "path = '../raw_data_input/hydro/inputs/'\n",
    "countries = ['AT', 'CH', 'FR', 'NO1', 'NO2', 'NO3', 'NO4', 'NO5', 'NL', 'CZ', 'DK1', 'DK2', 'PL']\n",
    "generation = {country: tools.load_entsoe_generation_data(country) for country in countries}\n",
    "\n",
    "fuels = {\n",
    "    'lignite': 'Fossil Brown coal/Lignite  - Actual Aggregated [MW]',\n",
    "    'natgas': 'Fossil Gas  - Actual Aggregated [MW]',\n",
    "    'hardcoal': 'Fossil Hard coal  - Actual Aggregated [MW]',\n",
    "    'oil': 'Fossil Oil  - Actual Aggregated [MW]',\n",
    "    'uranium': 'Nuclear  - Actual Aggregated [MW]'\n",
    "        }\n",
    "data = []\n",
    "for c in countries:\n",
    "    for fuel in fuels.keys():\n",
    "        try:\n",
    "            data.append(\n",
    "                [c,\n",
    "                 fuel,\n",
    "                 generation[c][fuels[fuel]].min(),\n",
    "                 generation[c][fuels[fuel]].max(),\n",
    "                 generation[c][fuels[fuel]].diff().quantile(0.99)])\n",
    "        except (KeyError, TypeError): \n",
    "            pass        \n",
    "        \n",
    "conv_data = pd.DataFrame(columns=['country', 'fuel', 'minimum', 'maximum', 'gradient_max'], data=data)\n",
    "conv_data['gradient_max'] /= conv_data['maximum']\n",
    "conv_data['minimum'] /= conv_data['maximum']\n",
    "# oil has zero maximum infeed\n",
    "conv_data.drop(index=conv_data[(conv_data['country'] == 'AT')\n",
    "                               & (conv_data['fuel'] == 'oil')].index, inplace=True)\n",
    "\n",
    "# overwrite data\n",
    "for i in conv_data.index:\n",
    "    c, f = conv_data.loc[i, ['country', 'fuel']]\n",
    "    conv_eu_agg.loc[(conv_eu_agg['country']==c) & (conv_eu_agg['fuel']==f),\n",
    "                ['capacity', 'min_load_factor', 'grad_pos']] = (\n",
    "        conv_data.loc[i, ['maximum', 'minimum', 'gradient_max']].values)\n",
    "conv_eu_agg['grad_neg'] = conv_eu_agg['grad_pos']\n",
    "\n",
    "# Reduce flexibility to real market-based observed values\n",
    "conv_de.loc[conv_de['fuel'] == 'uranium', ['grad_pos', 'grad_neg']] = 0.1729\n",
    "conv_de.loc[conv_de['fuel'] == 'lignite', ['grad_pos', 'grad_neg']] = 0.2543"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a European data projection for the future\n",
    "\n",
    "Steps applied:\n",
    "* Take the capacity information obtained from ENTSOE's TYNDP\n",
    "* Extract the capacity values for 2025 and 2030 and the aggregated status quo data\n",
    "* Interpolate in between and add the remainder of the data from the status quo data set\n",
    "* Add missing information and fill data gaps by mean values per fuel\n",
    "* Include some assumption on (slight) efficiency improvement due to decommissionings / new commissionings over time (efficiency is assumed to be 0.5% p.a. (not percent points) higher for all fuels)\n",
    "* Combine all data sets for the target year to be considered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set index only for matching purposes\n",
    "conv_eu_2025_BEST.set_index(['country', 'fuel'], inplace=True)\n",
    "conv_eu_2030_DG.set_index(['country', 'fuel'], inplace=True)\n",
    "conv_eu_agg.set_index(['country', 'fuel'], inplace=True)\n",
    "\n",
    "conv_eu_capacities = pd.DataFrame(index=conv_eu_agg.index.union(conv_eu_2025_BEST.index).union(conv_eu_2030_DG.index),\n",
    "                                  columns=list(range(2019, 2031)), dtype=\"float64\")\n",
    "\n",
    "conv_eu_capacities[2019] = conv_eu_agg[\"capacity\"]\n",
    "conv_eu_capacities[2025] = conv_eu_2025_BEST[\"capacity\"]\n",
    "conv_eu_capacities[2030] = conv_eu_2030_DG[\"capacity\"]\n",
    "conv_eu_capacities.loc[:, [2019, 2025, 2030]] = conv_eu_capacities.loc[:, [2019, 2025, 2030]].fillna(0)\n",
    "conv_eu_capacities = conv_eu_capacities.interpolate(axis=\"columns\").round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_concat = [col for col in conv_eu_agg.columns\n",
    "                  if col not in conv_eu_capacities.columns]\n",
    "\n",
    "conv_eu = pd.merge(conv_eu_capacities, conv_eu_agg[cols_to_concat], \n",
    "                        left_index=True, right_index=True, how='left')\n",
    "\n",
    "if year > 2019:\n",
    "    conv_eu[\"capacity\"] = conv_eu_capacities[year]\n",
    "else:\n",
    "    conv_eu = conv_eu.loc[conv_eu[\"capacity\"].notna()]\n",
    "\n",
    "conv_eu.drop(conv_eu_capacities.columns, axis=\"columns\", inplace=True)\n",
    "conv_eu.reset_index(drop=False, inplace=True)\n",
    "\n",
    "eu_fuels = conv_eu['fuel'].unique()\n",
    "\n",
    "for fuel in eu_fuels:\n",
    "    conv_eu.loc[conv_eu['fuel'] == fuel, \n",
    "                :] = conv_eu.loc[conv_eu['fuel'] == fuel, \n",
    "                                 :].fillna(conv_eu.loc[conv_eu['fuel'] == fuel, :].mean())\n",
    "    \n",
    "conv_eu['type'] = 'emb'\n",
    "conv_eu.fillna(conv_eu.mean(), inplace=True)\n",
    "\n",
    "if year > 2019:\n",
    "    conv_eu['efficiency_el'] = (1 + (year - 2019) * 0.005) * conv_eu['efficiency_el']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = pd.concat([conv_de, conv_de_new, conv_eu])\n",
    "conv = conv.reset_index().rename(columns={'index': 'identifier'})\n",
    "conv.loc[conv['country'] != 'DE', 'identifier'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduce a power plant identifier and include connection to oemof.solph elements\n",
    "Steps applied:\n",
    "- Introduce an identifier consisting of country, the string 'transformer', fuel type and BNetzA-ID (the latter for Germany only)\n",
    "- Add a connection to the respective country fuel bus\n",
    "- Add a connection to the respective country electricity bus\n",
    "- Write the resulting data set to a csv and and Excel file\n",
    "\n",
    "> _Note: See the [documentation of oemof.solph](https://oemof.readthedocs.io/en/latest/index.html) for further information on buses which are basically some kind of (balanced) bus bars to connect the elements of an energy system._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv['label'] = np.where(conv['country'] == 'DE', \n",
    "                         conv['country'] + '_transformer_' + conv['fuel'] + '_' + conv['identifier'],\n",
    "                         conv['country'] + '_transformer_' + conv['fuel'])\n",
    "conv.set_index('label', inplace=True)\n",
    "\n",
    "#Connection to buses\n",
    "conv['from'] = conv['country'] + '_bus_' + conv['fuel']\n",
    "conv['to_el'] = conv['country'] + '_bus_' + 'el'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **_NOTE:_** The power plants data for the target year is written later (see: [this section](#Write-transformer-data-for-2030)) since it may include some adequacy considerations and assumptions about new-built plants to meet the security of supply requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduce electricity buses\n",
    "Introduce an electrical bus for every bidding zone modelled. Also see [this section](#Buses) where all buses are written to csv and Excel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_modeled = ['DE', 'AT', 'BE', 'CH', 'CZ', 'DK1', 'DK2', 'FR', 'NL', \n",
    "                     'NO1', 'NO2', 'NO3', 'NO4', 'NO5', 'PL', 'SE1', 'SE2', 'SE3', 'SE4']\n",
    "\n",
    "buses = [country+'_bus_el' for country in countries_modeled]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do some garbage collection\n",
    "try:\n",
    "    del (agg_dict, c, cols_to_concat, cols_to_keep, conv_data, conv_eu,\n",
    "         conv_eu_2025_BEST, conv_eu_2030_DG, conv_eu_agg, conv_eu_capacities,\n",
    "         countries, data, eu_fuels, f, fuel, fuels, generation, i, key, min_load_dict, path)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linking Transformers (Interconnectors)\n",
    "\n",
    "In this section, **interconnector** data for cross-border power exchange modelling is put together.<br>\n",
    "In [oemof.solph](https://github.com/oemof/oemof-solph) interconnectors for cross-border exchange can be represented by simple transformers elements with one input and one ouput as well as (transportation) losses.\n",
    "> Note: A former version of _POMMES_ used the custom link component instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in interconnector data\n",
    "\n",
    "A NTC based approach is used in _POMMES_ which considers time-dependent availability of NTCs by applying technical profiles for cross-border exchanges.\n",
    "\n",
    "The following data is read in:\n",
    "- Available resp. total transfer capacities for the interconnectors:\n",
    "    - Data is obtained from Timona Ghosh's master thesis based on the year 2016.\n",
    "    - Data for exchanges not considered by Timona Ghosh is added from the following sources:\n",
    "        - Scandinavian countries: [Nordpool](https://www.nordpoolgroup.com/Market-data1/Dayahead/Capacities1/Capacities/KEY/Norway/?view=table), accessed 2018-02-23\n",
    "        - Belgium: [Elia](https://www.elia.be/en/electricity-market-and-system/electricity-market-facilitation/capacity-calculation)\n",
    "        - Other countries: ENTSO-E (Forcasted Transfer Capacities - Day Ahead)\n",
    "- Technical profiles:\n",
    "    - Technical profiles are derived from Timona Ghosh's master thesis. The original source are publications of the European TSOs and/or correspondence with TSOs.\n",
    "    - The technical profiles used are those prior to the introduction of FBMC.\n",
    "- Interconnector timeseries:\n",
    "    - The time series data contains the timeseries for the parameters used in the technical profiles.\n",
    "    - The time series are taken from Timona Gosh's master thesis and given for the year 2016. The original source is ENTSO-E resp. TSO transparency information.\n",
    "\n",
    "> _Note: Technical profiles determine the available NTC capacity which is dependent on wind power generation and/or load. The approach was used prior to flow-based market coupling (FBMC) and is still applied for the exchange with bidding zones which did not yet introduce FBMC. Since the domains for NTC and FBMC are quite similar, NTC still remains a good proxy for cross-border exchange._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interconn_TTC = pd.read_csv(\n",
    "    main_path[\"inputs\"] + sub_path[\"interconnectors\"] + input_file[\"interconnectors_ttc\"],\n",
    "    sep=\";\", decimal=\",\"\n",
    ")\n",
    "interconn_TTC.columns = list(interconn_TTC.columns)[:3] + list(range(2015, 2031))\n",
    "\n",
    "interconn_techprofiles = pd.read_csv(\n",
    "    main_path[\"inputs\"] + sub_path[\"interconnectors\"] + input_file[\"interconnectors_tech_profiles\"],\n",
    "    sep=\";\", decimal=\",\", index_col = 0\n",
    ")\n",
    "interconn_techprofiles.columns = list(interconn_techprofiles.columns)[:4] + list(range(2015, 2031))\n",
    "\n",
    "interconn_ts = pd.read_csv(\n",
    "    main_path[\"inputs\"] + sub_path[\"interconnectors\"] + input_file[\"interconnectors_timeseries\"],\n",
    "    parse_dates=True, index_col=0, sep=\";\", decimal=\",\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create data for linking transformer elements\n",
    "\n",
    "Provide data for transformer elements which model transshipment lines between the country electricity buses.\n",
    "\n",
    "Steps applied:\n",
    "-  Create labelling information:\n",
    "    - name of the linking transformer\n",
    "    - country bus, the line starts from (exporting country)\n",
    "    - country bus, the line leads to (importing country)\n",
    "- Apply a conversion factor of 0.95 in order to prevent excessive exchanges and model losses\n",
    "- Assign type of link:\n",
    "    - 'AC': applied for all connections for which technical profiles exist in order to depict time-dependent NTC values\n",
    "    - 'DC': applied for all other connections for which a fixed value is applied\n",
    "- Write the resulting data set into a csv and an Excel file\n",
    "\n",
    "> Example for naming convention used: DE &rarr; AT; name: DE_link_AT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "interconn_TTC['link'] = interconn_TTC['from'] + '_link_' + interconn_TTC['to'] \n",
    "interconn_TTC['from'] = interconn_TTC['from'] + '_bus_el'\n",
    "interconn_TTC['to'] = interconn_TTC['to'] + '_bus_el'\n",
    "\n",
    "interconn_TTC.drop(columns = 'action', inplace = True)\n",
    "interconn_TTC['conversion_factor'] = 0.95\n",
    "interconn_TTC.loc[interconn_TTC.index.isin(interconn_techprofiles.index), 'type'] = 'AC'\n",
    "interconn_TTC.loc[interconn_TTC['type'] != 'AC', 'type'] = 'DC'\n",
    "\n",
    "interconn_TTC.set_index('link', inplace = True)\n",
    "\n",
    "# Ensure limitless capacity to display a single market zone\n",
    "interconn_TTC.loc['DE_link_AT', [2016, 2017, 2018]] = 100000\n",
    "interconn_TTC.loc['AT_link_DE', [2016, 2017, 2018]] = 100000\n",
    "\n",
    "interconn_TTC.to_csv(\n",
    "    main_path[\"outputs\"] + output_file[\"linking_transformers\"] + \"_\" + str(year) + \".csv\"\n",
    ")\n",
    "interconn_TTC.to_excel(writer, sheet_name='interconn_TTC'  + \"_\" + str(year))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate time-dependent NTCs for AC interconnectors\n",
    "\n",
    "As stated above, the NTC values for AC interconnectors are determined based on technical profiles.\n",
    "\n",
    "Steps applied:\n",
    "- A DataFrame holding actual time-dependent NTC values is instanciated\n",
    "- A factor is introduced to account for interconnector expansion. Example:\n",
    "    - Max. NTC is 5,000 MW at the moment and it is planned to install another 1,000 MW\n",
    "    - factor 'network_expansion' has to be set to 6,000 / 5,000 = 1.2\n",
    "- Functions are introduced for determining the actual NTC value based on the technical profiles\n",
    "- The actual NTC time series values are set based on the technical profiles, i.e.\n",
    "    - based on German wind infeed for all German borders except for DE / DK1\n",
    "    - based on wind infeed and demand in TenneTs control area for DE / DK1\n",
    "- The NTC timeseries values are written to a csv and an Excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NTC_actual = pd.DataFrame(columns=interconn_techprofiles.index.unique(), index=interconn_ts.index)\n",
    "network_expansion = 1\n",
    "\n",
    "iter_year = interconn_ts.index.year\n",
    "for connector in NTC_actual.columns:\n",
    "    intercon = interconn_techprofiles.loc[connector].reset_index()\n",
    "    \n",
    "    if connector != 'DK1_link_DE' and connector != 'DE_link_DK1':\n",
    "        wind = interconn_ts['Wind_DE'].to_numpy()\n",
    "        \n",
    "        actual_capacity = (\n",
    "            np.array(\n",
    "                [tools.find_ntc_wind(intercon, wind, yr) \n",
    "                 for yr, wind in np.array([iter_year, wind]).T]) /\n",
    "            (interconn_TTC.at[connector,2016] * network_expansion))\n",
    "            \n",
    "    else:\n",
    "        wind = interconn_ts['Wind_TenneT'].to_numpy()\n",
    "        dem = interconn_ts['Demand_TenneT'].to_numpy()\n",
    "        \n",
    "        actual_capacity = (\n",
    "            np.array(\n",
    "                [tools.find_ntc_load_and_wind(intercon, wind, dem, yr) \n",
    "                 for yr, wind, dem in np.array([iter_year, wind, dem]).T]) / \n",
    "            (interconn_TTC.at[connector,2016] * network_expansion))                 \n",
    "\n",
    "    NTC_actual[connector] = actual_capacity\n",
    "    \n",
    "NTC_actual = tools.reindex_time_series(NTC_actual, year)\n",
    "\n",
    "NTC_actual.to_csv(\n",
    "    main_path[\"outputs\"] + output_file[\"linking_transformers_ts\"] + \"_\" + str(year) + \".csv\"\n",
    ")  \n",
    "NTC_actual.tz_localize(None).to_excel(writer, sheet_name='NTC_actual' + \"_\" + str(year))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do some garbage collection\n",
    "try:\n",
    "    del (NTC_actual, actual_capacity, connector, dem, intercon, interconn_TTC,\n",
    "         interconn_techprofiles, interconn_ts, iter_year, network_expansion, wind)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storages\n",
    "\n",
    "In this section, **power storages** data is put together.<br>\n",
    "In [oemof.solph](https://github.com/oemof/oemof-solph) storages can be represented through (generic) so called \"GenericStorage\" elements which have specific parameters.\n",
    "\n",
    "The following types of storages are covered in _POMMES_:\n",
    "- hydro reservoir storage\n",
    "- pumped hydro storage\n",
    "\n",
    "## Reservoir storages\n",
    "\n",
    "For modelling reservoir energy storages, generation data as well as data for the current (weekly) filling level is used.\n",
    "\n",
    "Steps applied for preparation:\n",
    "- Determine path where raw data can be found\n",
    "- Obtain all files stored in that folder (to read in based on subsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = main_path[\"inputs\"] + sub_path[\"hydro\"] +'inputs/'\n",
    "files = [f for f in listdir(path) if isfile(join(path, f))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain electricity generation from reservoir storages\n",
    "Steps applied:\n",
    "- Read in and prepare generation information from ENTSO-E per bidding zone<br>\n",
    "(see function `hydro.load_hydro_generation_data` for details)\n",
    "- Join data from all hydro generation data files and create subsets for reservoir resp. run of river (the latter will be treated later on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_generation = [f for f in files if 'entsoe_generation' in f]\n",
    "files_generation = [f for f in files_generation if search(('AT|CH|FR|NO'), f)]\n",
    "\n",
    "generation_df = pd.DataFrame(index=pd.date_range(start='2017-01-01 00:00:00',\n",
    "                                                 end='2017-12-31 23:00:00', \n",
    "                                                 freq='H'))\n",
    "\n",
    "for file in files_generation:\n",
    "    bidding_zone = file.partition('generation_')[2][0:2]\n",
    "    \n",
    "    if bidding_zone == 'NO':\n",
    "        bidding_zone = file.partition('generation_')[2][0:3]    \n",
    "              \n",
    "    generation_df = generation_df.join(\n",
    "        hydro.load_hydro_generation_data(bidding_zone=bidding_zone, path=path, filename=file), \n",
    "        how='left')\n",
    "    \n",
    "generation_reservoir = generation_df[generation_df.columns[generation_df.columns.str.contains('Reservoir')]]\n",
    "generation_ror = generation_df[generation_df.columns[generation_df.columns.str.contains('ROR')]]\n",
    "\n",
    "reservoir_load_factors = pd.DataFrame(data={'max_load_factor': generation_reservoir.max(),\n",
    "                                            'min_load_factor': generation_reservoir.min()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derive an hourly filling rate (time series)\n",
    "\n",
    "Steps applied:\n",
    "- Read in and prepare filling rate information from ENTSO-E per bidding zone<br>\n",
    "(see function `hydro.load_hydro_generation_data` for details)\n",
    "- Join data from all hydro reservoir data files\n",
    "- Set first value for filling rate of 2018 as last value for 2017\n",
    "- Transform weekly values for the filling rate to hourly ones\n",
    "- Determine minimum and maximum filling rates per country / bidding zone\n",
    "- Determine inflow of reservoir as:\n",
    "$$inflow = \\Delta filling \\space rate + losses + \\frac{electricity \\space generation}{efficiency}$$\n",
    "- Determine weekly average of inflow and assign this value for each hour of the week\n",
    "- Normalize the values by dividing through usable storage energy (which is defined as the difference between max. and min. filling rate)\n",
    "- Currently, there is no generation data for Sweden, therefore, average data from Norway's bidding zones is taken for the inflow for all four Swedish bidding zones\n",
    "- Rename columns of DataFrame eventually containing the weekly average inflow on an hourly basis\n",
    "\n",
    "> *Findings:*\n",
    "> - *Austria has a small storage capacity, but an extremely high production of hydro. However, the total amount should fit.*\n",
    "> - *Results are quite sensible to the evaporation rate, and turbine efficiency.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hydro_storage_loss_rate = 0.0000\n",
    "efficiency_turbine = 0.85\n",
    "\n",
    "files_reservoir = [f for f in files if 'entsoe_hydro_reservoir' in f]\n",
    "\n",
    "reservoir_df = pd.DataFrame(index=np.arange(0,52,1))\n",
    "\n",
    "for file in files_reservoir:\n",
    "    bidding_zone = file.partition('reservoir_')[2][0:2]\n",
    "    \n",
    "    if bidding_zone in ['NO', 'SE']:\n",
    "        bidding_zone = file.partition('reservoir_')[2][0:3]\n",
    "    \n",
    "    reservoir_df = reservoir_df.join(\n",
    "        hydro.load_hydro_reservoir_data(bidding_zone=bidding_zone, \n",
    "                                        years=['2017', '2018'], \n",
    "                                        path=path, filename=file),\n",
    "        how='left')\n",
    "\n",
    "end_values = reservoir_df.loc[0, reservoir_df.columns[reservoir_df.columns.str.contains('2018')]].values\n",
    "reservoir_df.drop(columns=reservoir_df.columns[reservoir_df.columns.str.contains('2018')], inplace=True)\n",
    "reservoir_df.index = pd.date_range(start='01-01-2017 00:00:00', periods=52, freq='7D')\n",
    "reservoir_df.loc[pd.Timestamp('2018-01-01 00:00:00', freq='7D')] = end_values\n",
    "\n",
    "# Resample to hourly values and obtain max and min values\n",
    "reservoir_df = reservoir_df.resample('H').interpolate('linear')\n",
    "min_storageable_energy, max_storageable_energy = (reservoir_df.min(), reservoir_df.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_SE = reservoir_df.columns[reservoir_df.columns.str.contains('SE')]\n",
    "cols_nonSE = reservoir_df.columns[~reservoir_df.columns.isin(cols_SE)]\n",
    "\n",
    "inflow_df = (reservoir_df.loc[:, cols_nonSE].diff()[1:].values \n",
    "    + reservoir_df[:-1][cols_nonSE] * hydro_storage_loss_rate \n",
    "    + generation_reservoir.values / efficiency_turbine)\n",
    "\n",
    "# Do interpolation of inflow (weekly averages)\n",
    "inflow_df_averaged = inflow_df.apply(lambda x: hydro.upsample_inflow(x))\n",
    "\n",
    "usable_storage = max_storageable_energy - min_storageable_energy\n",
    "reservoir_init_level = (reservoir_df.iloc[0] - min_storageable_energy) / usable_storage\n",
    "\n",
    "rel_avg_hourly_inflow = inflow_df_averaged / usable_storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derive correction factors and estimate Sweden\n",
    "Steps applied:\n",
    "* Determine aggregated (theoretical) electricty output of estimated inflow\n",
    "* For all except for SE, calculate difference of theoretical and real output, and divide by total storage to get a correction for the (hourly) natural inflow\n",
    "* Subtract this difference from the inflow values\n",
    "* Use the mean values for Norway as an estimate for Sweden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate aggregated (theoretical) electricty output os estimated inflow\n",
    "calculated_energy = (rel_avg_hourly_inflow * usable_storage.values).sum() * efficiency_turbine\n",
    "calculated_energy.drop(index=calculated_energy.loc[calculated_energy.index.str.contains('SE')].index, inplace=True)\n",
    "\n",
    "correction_factor = ((calculated_energy - generation_reservoir.sum().values) /\n",
    "    usable_storage.loc[usable_storage.index.str.contains('AT|CH|FR|NO')].values / 8760)\n",
    "\n",
    "rel_avg_hourly_inflow.loc[:, rel_avg_hourly_inflow.columns.str.contains('AT|CH|FR|NO')] -= correction_factor.values\n",
    "\n",
    "cols_NO = reservoir_df.columns[reservoir_df.columns.str.contains('NO')]\n",
    "rel_avg_hourly_inflow[cols_SE] = (\n",
    "    np.repeat(rel_avg_hourly_inflow[cols_NO].mean(axis=1).values.reshape((8760,1)), [4], axis=1))\n",
    "\n",
    "rel_avg_hourly_inflow.columns = [_[0] + '_source_hydro' for _ in rel_avg_hourly_inflow.columns.str.split('_')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data for usage in oemof.solph\n",
    "Determine parameters for reservoirs such that they can be used to create the respective oemof.solph storages.\n",
    "\n",
    "Steps applied:\n",
    "- Put data for usable storage energy amount (nominal_storable_energy) and initial storage state into a DataFrame\n",
    "- Add loss rate and efficiency for turbine based on assumptions\n",
    "- Create the hydro sources and buses per country / bidding zone needed for reservoir modelling\n",
    "- Add sink components for modelling hydro spillage\n",
    "- Group data per bidding zone\n",
    "- PL, BE, CZ hydro storages are excluded due to missing date for the filling rates\n",
    "- Add inflow and outflow bus information\n",
    "- Set storage type to 'reservoir'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hydro_countries = [_[0] for _ in usable_storage.index.str.split('_')]\n",
    "hydro_reservoir_data = pd.DataFrame(\n",
    "    index=[c+'_storage_el_reservoir' for c in hydro_countries],\n",
    "    columns=['country','nominal_storable_energy', 'initial_storage_level'],\n",
    "    data=np.array([hydro_countries, usable_storage.values, reservoir_init_level.values]).T)\n",
    "hydro_reservoir_data['loss_rate'] = hydro_storage_loss_rate\n",
    "hydro_reservoir_data.loc['CH_storage_el_reservoir', 'loss_rate'] = 0.00001\n",
    "\n",
    "hydro_reservoir_data['efficiency_turbine'] = efficiency_turbine\n",
    "hydro_reservoir_data['efficiency_pump'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Hydro sources and buses\n",
    "sources_hydro = pd.DataFrame(columns=['country', 'capacity'],\n",
    "                             data=np.array([hydro_countries, usable_storage.values]).T)\n",
    "sources_hydro = tools.nodes_to_oemof(sources_hydro, to='_bus_hydro', component='_source_hydro')\n",
    "\n",
    "buses.extend(sources_hydro['to'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sinks_reservoir_spillage = pd.DataFrame(data={'country': hydro_countries,\n",
    "                                              'from': sources_hydro['to'].to_list()})\n",
    "\n",
    "sinks_reservoir_spillage['label'] = sinks_reservoir_spillage['country'] + '_sink_hydro_excess'\n",
    "sinks_reservoir_spillage['excess_costs'] = 1\n",
    "sinks_reservoir_spillage.set_index('label', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reservoir_eu_agg = reservoir_eu.groupby(by='bidding_zone', as_index=False).agg({'capacity': 'sum'})\n",
    "reservoir_eu_agg.drop(index=reservoir_eu_agg[reservoir_eu_agg['bidding_zone'].isin(['BE', 'PL', 'CZ'])].index, \n",
    "                      inplace=True)\n",
    "reservoir_eu_agg = reservoir_eu_agg.set_index(reservoir_eu_agg['bidding_zone'] + '_storage_el_reservoir')\n",
    "\n",
    "reservoir_eu_agg['bus_inflow'] = reservoir_eu_agg['bidding_zone'] + '_bus_hydro'\n",
    "reservoir_eu_agg['bus_outflow'] = reservoir_eu_agg['bidding_zone'] + '_bus_el'\n",
    "reservoir_eu_agg['capacity_pump'] = 100000\n",
    "reservoir_eu_agg.rename(columns={'capacity': 'capacity_turbine', 'bidding_zone': 'country'}, inplace=True)\n",
    "\n",
    "reservoir_eu_agg['type'] = 'reservoir'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pumped hydro storages\n",
    "\n",
    "The basic pumped hydro storages information is obtained from the power plants lists (see above).\n",
    "\n",
    "Steps applied:\n",
    "- Group the European PHES by bidding zones\n",
    "- Include TYNDP capacity projections and interpolate in between the years\n",
    "- Assign values for 2025 to 2030 if TYNDP states no capacities. This is assumed to be a data error.\n",
    "- Combine into a common data set for the target year\n",
    "- Aggregate the German PHES units as well\n",
    "- Concatenate both data sets and set storage type to 'phes'\n",
    "- Add information on inflow and outflow bus as well as capacity information\n",
    "- Nominal storable energy is based on an assumption: It is assumed that the nominal storable energy equals 4times the plant capacity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pumpedstorage_eu_agg = pumpedstorage_eu.groupby('bidding_zone', as_index=False).agg({'capacity': 'sum'})\n",
    "pumpedstorage_eu_agg = pumpedstorage_eu_agg.set_index(pumpedstorage_eu_agg['bidding_zone'] + '_storage_el_PHS')\n",
    "pumpedstorage_eu_agg.rename(columns={'bidding_zone': 'country'}, inplace=True)\n",
    "\n",
    "# There is a large deviation between pumped capacity in AT which is fixed here\n",
    "pumpedstorage_eu_agg.loc['AT_storage_el_PHS', 'capacity'] =(\n",
    "    AT['Hydro Pumped Storage  - Actual Aggregated [MW]'].max())\n",
    "\n",
    "phes_de = phes_de.set_index(phes_de['country'] + '_storage_el_PHS_' + phes_de.index)\n",
    "phes_de.rename(columns={'capacity_net_bnetza': 'capacity'}, inplace=True)\n",
    "phes_de.drop(index=phes_de[phes_de['capacity'].isna()].index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine phes plants still operational in taget year\n",
    "* It is assumed that all plants operational in the status quo will be retroffited instead of shutdown.\n",
    "* New-built plant(s) is / are added for Germany.\n",
    "* The European data set is taken as it is given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pumpedstorage_eu_2025_BEST = pumpedstorage_eu_2025_BEST.pivot(index='country', columns='fuel', values='capacity')\n",
    "pumpedstorage_eu_2025_BEST = pumpedstorage_eu_2025_BEST.T.fillna(pumpedstorage_eu_2025_BEST.mean(axis=1)).T\n",
    "\n",
    "pumpedstorage_eu_2025_BEST['label'] = pumpedstorage_eu_2025_BEST.index.values + '_storage_el_PHS'\n",
    "pumpedstorage_eu_2025_BEST.reset_index(inplace=True)\n",
    "pumpedstorage_eu_2025_BEST.set_index('label', inplace=True)\n",
    "\n",
    "pumpedstorage_eu_2025_BEST.rename(columns={'PHES_capacity': 'capacity',\n",
    "                                           'PHES_capacity_pump': 'capacity_pump',\n",
    "                                           'PHES_capacity_turbine': 'capacity_turbine'}, inplace=True)\n",
    "\n",
    "pumpedstorage_eu_2030_DG = pumpedstorage_eu_2030_DG.pivot(index='country', columns='fuel', values='capacity')\n",
    "pumpedstorage_eu_2030_DG = pumpedstorage_eu_2030_DG.T.fillna(pumpedstorage_eu_2025_BEST.mean(axis=1)).T\n",
    "\n",
    "pumpedstorage_eu_2030_DG['label'] = pumpedstorage_eu_2030_DG.index.values + '_storage_el_PHS'\n",
    "pumpedstorage_eu_2030_DG.reset_index(inplace=True)\n",
    "pumpedstorage_eu_2030_DG.set_index('label', inplace=True)\n",
    "\n",
    "pumpedstorage_eu_2030_DG.rename(columns={'PHES_capacity': 'capacity',\n",
    "                                         'PHES_capacity_pump': 'capacity_pump',\n",
    "                                         'PHES_capacity_turbine': 'capacity_turbine'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pumpedstorage_eu_capacities = pd.DataFrame(\n",
    "    index=pumpedstorage_eu_agg.index.union(pumpedstorage_eu_2025_BEST.index).union(pumpedstorage_eu_2030_DG.index),\n",
    "    columns=list(range(2019, 2031)), dtype=\"float64\")\n",
    "\n",
    "pumpedstorage_eu_capacities[2019] = pumpedstorage_eu_agg[\"capacity\"]\n",
    "pumpedstorage_eu_capacities[2025] = pumpedstorage_eu_2025_BEST[\"capacity\"]\n",
    "\n",
    "# Correct erraneous TYNDP data: if there is no capacity given for 2030, take 2025 best estimate values\n",
    "pumpedstorage_eu_capacities[2030] = np.where(\n",
    "    pumpedstorage_eu_2030_DG[\"capacity\"].isna(), \n",
    "    pumpedstorage_eu_2025_BEST[\"capacity\"], \n",
    "    pumpedstorage_eu_2030_DG[\"capacity\"])\n",
    "\n",
    "pumpedstorage_eu_capacities.loc[:, [2019, 2025, 2030]] = (\n",
    "    pumpedstorage_eu_capacities.loc[:, [2019, 2025, 2030]].fillna(0))\n",
    "pumpedstorage_eu_capacities = pumpedstorage_eu_capacities.interpolate(axis=\"columns\").round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_concat = [col for col in pumpedstorage_eu_agg.columns\n",
    "                  if col not in pumpedstorage_eu_capacities.columns]\n",
    "\n",
    "pumpedstorage_eu_agg = pd.merge(pumpedstorage_eu_capacities, pumpedstorage_eu_agg[cols_to_concat], \n",
    "                            left_index=True, right_index=True, how='left')\n",
    "\n",
    "if year > 2019:\n",
    "    pumpedstorage_eu_agg[\"capacity\"] = pumpedstorage_eu_capacities[year]\n",
    "    pumpedstorage_eu_agg[\"country\"] = [pumpedstorage_eu_agg.index.str.split(\"_\")[i][0] \n",
    "                                       for i in range(len(pumpedstorage_eu_agg))]\n",
    "else:\n",
    "    pumpedstorage_eu_agg = pumpedstorage_eu_agg.loc[pumpedstorage_eu_agg[\"capacity\"].notna()]\n",
    "\n",
    "pumpedstorage_eu_agg.drop(pumpedstorage_eu_capacities.columns, axis=\"columns\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregation for German pumped storages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phes_de.rename(columns={'name_bnetza': 'name',\n",
    "                        'eic_code_plant': 'eic_code'}, inplace=True)\n",
    "phes_de = phes_de.append(phes_de_new)\n",
    "\n",
    "# Aggregation for German pumped storages\n",
    "phes_de_agg = phes_de.groupby('country', as_index=False).agg({'capacity': 'sum'}).rename(\n",
    "    index={0: 'DE_storage_el_PHS'})\n",
    "\n",
    "phes = pd.concat([phes_de_agg, pumpedstorage_eu_agg])\n",
    "phes['type'] = 'phes'\n",
    "\n",
    "phes['bus_inflow'] = phes['country'] + '_bus_el'\n",
    "phes['bus_outflow'] = phes['country'] + '_bus_el'\n",
    "phes['capacity_pump'] = phes['capacity']\n",
    "phes['capacity_turbine'] = phes['capacity']\n",
    "phes['nominal_storable_energy'] = 4 * phes['capacity_turbine']\n",
    "# Values obtained from ENTSOE\n",
    "phes.loc[phes['country'] == 'DE', ['capacity_pump', 'capacity_turbine']] = [8680.75, 4475.25]\n",
    "phes.loc[phes['country'] == 'DE', 'nominal_storable_energy'] = 25116\n",
    "\n",
    "phes.drop(columns='capacity', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add assumptions and store storages data\n",
    "\n",
    "Steps applied:\n",
    "- Put together the data for pumped hydro and reservoir storages\n",
    "- Add missing parameterization for pumped hydro energy storage based on assumptions (i.e. efficiencies, loss rate, ...)\n",
    "- Save the results in a csv and an Excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storages_el = pd.concat([phes, reservoir_eu_agg], axis=0, sort=False)\n",
    "\n",
    "assumptions_phes = pd.read_csv(\n",
    "    main_path[\"inputs\"] + sub_path[\"assumptions\"] + input_file[\"phes_assumptions\"],\n",
    "    index_col=0, sep=\"\\t\", decimal=\",\"\n",
    ")\n",
    "\n",
    "# Set missing parameters for phes based on assumptions\n",
    "for column in assumptions_phes.index:\n",
    "    storages_el[column] = assumptions_phes.at[column, 'value']\n",
    "\n",
    "# Set back values for reservoir storages which have been previously overwritten (again)\n",
    "storages_el.loc[hydro_reservoir_data.index, hydro_reservoir_data.columns] = hydro_reservoir_data.values\n",
    "\n",
    "# Restrict maximum and minimum load factors according to historical generation data\n",
    "reservoir_load_factors.index = [_[0]+'_storage_el_reservoir' for _ in reservoir_load_factors.index.str.split('_')]\n",
    "storages_el = storages_el.join(reservoir_load_factors, how='left')\n",
    "storages_el['max_load_factor'] /= storages_el['capacity_turbine']\n",
    "storages_el['min_load_factor'] /= storages_el['capacity_turbine']\n",
    "storages_el['max_load_factor'] = storages_el['max_load_factor'].fillna(1)\n",
    "storages_el['min_load_factor'] = storages_el['min_load_factor'].fillna(0)\n",
    "storages_el[['max_load_factor', 'min_load_factor']] = storages_el[['max_load_factor', 'min_load_factor']].round(2)\n",
    "\n",
    "storages_el.to_csv(\n",
    "    main_path[\"outputs\"] + output_file[\"storages_el\"]  + \"_\" + str(year) + \".csv\"\n",
    ")   \n",
    "storages_el.to_excel(writer, sheet_name='storages_el' + \"_\" + str(year))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do some garbage collection\n",
    "try:\n",
    "    del (AT, assumptions_phes, bidding_zone, calculated_energy, cols_NO, cols_SE, \n",
    "         cols_nonSE, cols_to_concat, column, correction_factor, efficiency_turbine, end_values,\n",
    "         file, files, files_generation, files_reservoir, generation_df,\n",
    "         generation_reservoir, hydro_countries, hydro_reservoir_data, hydro_storage_loss_rate,\n",
    "         inflow_df, inflow_df_averaged, max_storageable_energy, min_storageable_energy,\n",
    "         path, phes, phes_de_agg, phes_de_new, \n",
    "         pumpedstorage_eu, pumpedstorage_eu_2025_BEST, pumpedstorage_eu_2030_DG,\n",
    "         pumpedstorage_eu_agg, pumpedstorage_eu_capacities,\n",
    "         reservoir_df, reservoir_eu, reservoir_eu_agg, \n",
    "         reservoir_init_level, reservoir_load_factors, usable_storage)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources\n",
    "\n",
    "In this section, **sources** data is put together.<br>\n",
    "In [oemof.solph](https://github.com/oemof/oemof-solph) (generic) \"sources\" components can be used to represent any kind of energy source. In _POMMES_ sources for the commodities (fuels), for energy shortage and for RES are used. Shortage sources are there to account for situations when overall generation is not sufficient to meet demand. In reality this would result in imports from other European countries and/or activation of reserves as well as forced load sheddings (\"brownouts\") to prevent a blackout. Shortage sources carry high penalty costs and are used for preserving model feasibility.\n",
    "\n",
    "For **commodity sources**, see [this section below](#Commodity-sources) which includes commodity sources according to the power plant status of the target year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emission limits\n",
    "Historical emissions as well as political target emission levels can be used to impose optional overall emissions limit.\n",
    "\n",
    "There are different target emission limits possible:\n",
    "* BAU prognosis: Simple linear regression based on historical emissions\n",
    "* 80% reduction (KSP, EK): 80% linear reduction taking into account the sector goal from the Klimaschutzplan 2050 (2016) as well as the 2050 goal from the Energiekonzept (2010)\n",
    "* 95% reduction (KSP, EK): 95% linear reduction taking into account the sector goal from the Klimaschutzplan 2050 (2016) as well as the 2050 goal from the Energiekonzept (2010)\n",
    "* 100% reduction (KSG): 100% linear reduction taking into account the sector goals from the Klimaschutzgesetz (KSG) which was agreed upon at the end of 2019 and updated in mid 2021.\n",
    "\n",
    "Goals:\n",
    "* KSP 2050: 61-62 % reduction in the energy industry using 466 Mt emissions in 1990 as a starting value which leads to 175-183 Mt GHG emissions in 2030. The mean value (179 Mt) is used. (Table 2 on page 33)\n",
    "* KSG: Anlage 2 zu § 4: 280 Mt in 2020, 257 Mt in 2022, 108 Mt in 2030; carbon neutrality (which practically means 0 emissions in the energy sector) in 2045 (§ 1 KSG). The goal of 88% emissions reduction until 2040 is not introduced since it is assumed that the contribution of the energy economy sector needs to be higher.\n",
    "\n",
    "Steps applied:\n",
    "* Perform a linear regression using statsmodels based on historical emissions (1990-2018) and years.\n",
    "* Apply historical values to all other possible emission pathways.\n",
    "* Add the target values as data points for the other pathways and interpolate linearly in between.\n",
    "\n",
    "Data sources:\n",
    "* Overall historical emissions: BMWi (2020). Energiedaten Gesamtausgabe. As of 23.10.2020, table 10, line 31 (Energiewirtschaft). https://www.bmwi.de/Redaktion/DE/Binaer/Energiedaten/energiedaten-gesamt-xls.xlsx?__blob=publicationFile&v=131, accessed 23.12.2020.\n",
    "* Bundesregierung (2010): Energiekonzept für eine umweltschonende, zuverlässige und bezahlbare Energieversorgung. 29. September 2010. https://www.bmwi.de/Redaktion/DE/Downloads/E/energiekonzept-2010.pdf?__blob=publicationFile&v=3, accessed 23.12.2020.\n",
    "* Bundesregierung (2016): Klimaschutzplan 2050. Klimaschutzpolitische Grundsätze und Ziele der Bundesregierung. https://www.bmu.de/fileadmin/Daten_BMU/Download_PDF/Klimaschutz/klimaschutzplan_2050_bf.pdf, accessed 23.12.2020.\n",
    "* KSG. Bundes-Klimaschutzgesetz vom 12. Dezember 2019 (BGBl. I S. 2513).\n",
    "\n",
    "> _NOTE: Setting an emissions limit may conflict resp. interrelates with setting carbon prices. Thus, it is recommended to use carbon prices in the dispatch model (if available) and annual emissions limits in the investment model were minimum loads (of power plant fleets) are considered in a simplified manner._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GHG_emissions = pd.read_csv(\n",
    "    main_path[\"inputs\"] + sub_path[\"assumptions\"] + input_file[\"GHG_emissions\"],\n",
    "    index_col=0, sep=\";\", decimal=\",\" \n",
    ").T\n",
    "GHG_emissions.drop(index=\"Basiswert+\", inplace=True)\n",
    "GHG_emissions.index = GHG_emissions.index.str.strip(\"*\").astype(int)\n",
    "GHG_emissions.columns = GHG_emissions.columns.str.strip()\n",
    "\n",
    "# Do a linear regression for the BAU prognosis\n",
    "X = GHG_emissions.index.values\n",
    "Y = GHG_emissions.Energiewirtschaft.values\n",
    "\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "GHG_regression = sm.OLS(Y, X).fit()\n",
    "\n",
    "prediction_horizon = range(GHG_emissions.index.values[-1] + 1, 2051)\n",
    "X_pred = sm.add_constant(prediction_horizon)\n",
    "GHG_prediction = GHG_regression.predict(X_pred)\n",
    "GHG_pred = pd.DataFrame(index=prediction_horizon, data={\"Energiewirtschaft\": GHG_prediction})\n",
    "\n",
    "GHG_df = GHG_emissions.append(GHG_pred)\n",
    "GHG_df.columns = ['BAU']\n",
    "GHG_df['80_percent_linear'], GHG_df['95_percent_linear'], GHG_df['100_percent_linear'] = [GHG_df['BAU'].values] * 3\n",
    "GHG_df.loc[2019:2050, ['80_percent_linear', '95_percent_linear', '100_percent_linear']] = np.nan\n",
    "\n",
    "# Assign values for other paths\n",
    "start_val = GHG_df.at[1990, 'BAU']\n",
    "\n",
    "emission_targets_KSP_80 = pd.DataFrame({2030: np.mean([175, 183]), \n",
    "                                        2050: start_val * (1 - 0.8)},\n",
    "                                      index=[\"values\"]).T\n",
    "emission_targets_KSP_95 = pd.DataFrame({2030: np.mean([175, 183]), \n",
    "                                        2050: start_val * (1 - 0.95)},\n",
    "                                      index=[\"values\"]).T\n",
    "emission_targets_KSG_100 = pd.DataFrame({2020: 280,\n",
    "                                         2022: 257,\n",
    "                                         2030: 108,\n",
    "                                         2045: 0,\n",
    "                                         2050: 0},\n",
    "                                       index=[\"values\"]).T\n",
    "\n",
    "GHG_df.loc[GHG_df['80_percent_linear'].isna(), '80_percent_linear'] = emission_targets_KSP_80['values']\n",
    "GHG_df.loc[GHG_df['95_percent_linear'].isna(), '95_percent_linear'] = emission_targets_KSP_95['values']\n",
    "GHG_df.loc[GHG_df['100_percent_linear'].isna(), '100_percent_linear'] = emission_targets_KSG_100['values']\n",
    "GHG_df.interpolate(method='linear', inplace=True)\n",
    "# convert million tons of CO2 to tons of CO2\n",
    "GHG_df = GHG_df.mul(1e6)\n",
    "\n",
    "GHG_df.to_csv(\n",
    "    main_path[\"outputs\"] + output_file[\"emission_limits\"] + \".csv\"\n",
    ")\n",
    "GHG_df.to_excel(writer, sheet_name='emission_limits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do some garbage collection\n",
    "try:\n",
    "    del (GHG_df, GHG_emissions, GHG_pred, GHG_prediction, GHG_regression,\n",
    "         X, X_pred, Y, emission_targets_KSG_100, emission_targets_KSP_80, emission_targets_KSP_95,\n",
    "         prediction_horizon, start_val)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shortage sources\n",
    "Steps applied:\n",
    "- Introduce one electricity shortage source per country modelled and one hydro shortage source for each country with hydro generation\n",
    "- Concat the two data sets and assign high penalty costs\n",
    "- Store the results to a csv as well as an Excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources_shortage_el = pd.DataFrame(data={'country': countries_modeled})\n",
    "sources_shortage_el = tools.nodes_to_oemof(sources_shortage_el, '_bus_el', '_source_el_shortage')\n",
    "\n",
    "sources_shortage_hydro = sources_hydro.copy()\n",
    "sources_shortage_hydro.index = [_+'_shortage' for _ in sources_hydro.index]\n",
    "sources_shortage_hydro.drop(columns='capacity', inplace=True)\n",
    "\n",
    "sources_shortage = pd.concat([sources_shortage_el, sources_shortage_hydro])\n",
    "sources_shortage['shortage_costs'] = 160\n",
    "\n",
    "sources_shortage.to_csv(\n",
    "    main_path[\"outputs\"] + output_file[\"sources_shortage\"] + \"_\" + str(year) + \".csv\"\n",
    ")\n",
    "sources_shortage.to_excel(writer, sheet_name='sources_shortage' + \"_\" + str(year))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Renewable sources\n",
    "In this subsection, **renewable energy sources** (RES) data is put together.<br>\n",
    "For Germany, a detailled RES modelling approach is used. A distinction between fluctuating and non-fluctuating RES is made.\n",
    "\n",
    "> _NOTE: Germany and other European bidding zones are treated differently:_\n",
    "> * _For **Germany**, the sources data set contains the **installed capacities** while the timeseries data set contains maximum values smaller than 1 considering simultaneity of RES infeed._\n",
    "> * _For other **European** bidding zones, the sources data set contains the **maximum infeed capacity** while the timeseries data set contains maximum values equal to 1._\n",
    "\n",
    "> _NOTE: The POMMES investment model depicts RES in a much less granular fashion since its only aim is to determine optimal capacity portfolios (as well as overall power system costs). Therefore, the approach laid down in the following is not applied in the investment model._\n",
    "\n",
    "### Fluctuating RES\n",
    "Fluctuating RES depicted in _POMMES_ are wind onshore, wind offshore and PV. Run of river (ROR) is modelled seperately (see [reservoir storages section](#Reservoir-storages) for timeseries extraction and [extensions for Germany](#Extensions-for-Germany) for data preparation).\n",
    "\n",
    "#### Extract timeseries data\n",
    "Steps applied:\n",
    "- Read in timeseries data from OPSD for Europe\n",
    "- Slice values for the year 2017 and do some column renaming for obtaining the oemof.solph terminology\n",
    "- Rename columns for Skandinavian countries for proper bidding zone terminology (e.g. 'NO1' instead of 'NO_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_eu = pd.read_csv(\n",
    "    main_path[\"inputs\"] + sub_path[\"timeseries\"] + input_file[\"opsd_timeseries\"],\n",
    "    index_col = 'utc_timestamp'\n",
    ")\n",
    "\n",
    "ts_eu = ts_eu['2017-01-01T00:00:00Z':'2017-12-31T23:00:00Z']\n",
    "ts_eu.index = pd.to_datetime(ts_eu.index)\n",
    "\n",
    "dict_rename_ts = {'_wind_onshore_generation_actual': '_source_windonshore',\n",
    "                  '_wind_offshore_generation_actual': '_source_windoffshore',\n",
    "                  '_solar_generation_actual': '_source_solarPV',\n",
    "                  '_load_actual_entsoe_transparency': '_sink_el_load'}\n",
    "\n",
    "for el in dict_rename_ts:\n",
    "    ts_eu.columns = ts_eu.columns.str.replace(el, dict_rename_ts[el])\n",
    "\n",
    "dict_names = dict(zip(ts_eu.filter(regex = 'DK|NO|SE').columns.values,\n",
    "                      ts_eu.filter(regex = 'DK|NO|SE').columns.str.replace(r\"[_]\", \"\", n = 1).values))\n",
    "ts_eu.rename(dict_names, axis = 1, inplace = True)\n",
    "\n",
    "ts_eu = ts_eu.tz_localize(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do manual data corrections\n",
    "Steps applied:\n",
    "- PV infeed in France is below zero for some (night) hours which is obviously a sign of a data error / inconsistency in data formatting &rarr; Hence, PV infeed is forced to 0 for the respective hours.\n",
    "- For some reason there is no windonshore feedin for the Netherlands. For a rough approximation, the TenneT profile (from DE) is used and scaled with the installed capacity obtained from IRENA (2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_eu.loc[ts_eu['FR_source_solarPV'] < 0, 'FR_source_solarPV'] = 0\n",
    "\n",
    "ts_eu['NL_source_windonshore'] = (\n",
    "    ts_eu['DE_tennet_wind_generation_actual'] / ts_eu['DE_tennet_wind_generation_actual'].max() * 3245)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare RES data for usage in oemof.solph\n",
    "Steps applied:\n",
    "- Filter the columns containing RES timeseries data\n",
    "- Concat the data with ROR generation (has been extracted in hydro (reservoir) data preparation above)\n",
    "- Extract maximum value for RES generation and set this as \"capacity\" for European RES\n",
    "> _Note: What is set as RES \"capacity\" here is actually the maximum feed in value, taking into account simultaneity of RES generation. Hence, a normalized generation time series with a maximum value of 1 can be derived from that. Actually installed RES generation capacity is higher, though. For Germany, a different approach is chosen and actually installed RES generation capacity is used for time series \"normalization\". Hence, for Germany, the maximum value of the \"normalized\" is smaller than 1._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [c+ts for c in countries_modeled for ts in dict_rename_ts.values()]\n",
    "ts_eu = ts_eu[ts_eu.columns[ts_eu.columns.isin(cols)]]\n",
    "\n",
    "ts_eu = pd.concat([ts_eu, generation_ror], axis=1, sort=False)\n",
    "# NO1 and NO5 have zero windonshore\n",
    "ts_eu.drop(columns=['NO1_source_windonshore', 'NO5_source_windonshore'], inplace=True)\n",
    "\n",
    "ts_res = ts_eu.filter(regex = 'wind|PV|ROR')\n",
    "\n",
    "sources_res = ts_res.max()\n",
    "sources_res = sources_res.to_frame().rename(columns={0: 'capacity'})\n",
    "\n",
    "sources_ts_res = ts_res / sources_res['capacity']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extensions for Germany\n",
    "Since Germany is in the focus of the analysis, some extensions are made for the modelling of renewables:\n",
    "- Non-fluctuating renewables are displayed in more detail\n",
    "- For flucutating renewables, the bidding strategy is approximated by simulating negative costs (see [this section](#EEG-transformers-with-negative-costs-(clustering)))\n",
    "\n",
    "**Approach for modelling marketing of RES**\n",
    "> In Germany, (fluctuating) RES generation is marketed within the so called \"market premium model\". Plant operators receive a market premium ($MP$) to compensate for the difference of their individual value applied (more or less equal to their full costs, i.e. LCOE) and the monthly average market revenue at the Day-ahead spot market. So their compensation is\n",
    "$$Earnings = Spot + MP$$\n",
    "Hence, it is economically rational to market the generation as long as profit is larger or equal to 0. For flucutating RES, it can be assumed that marginal costs are nearly 0 so that profits are nearly equal to earnings. This leads to:\n",
    "$$0 \\leq Spot + MP$$\n",
    "and\n",
    "$$-MP \\leq Spot$$\n",
    "The last inequation means that (rationally acting) plant operators are willing to market their generation as long as spot prices are at least as high as the negative market premium. This rational is modelled in _POMMES_.\n",
    "Since modelling each RES unit individually would blow up model complexity, power plant clusters for similar market premiums resp. values applicable within the same energy source are created.\n",
    "\n",
    "Steps applied:\n",
    "- Read in RES data and group by energy source. Primary data source is TSO data.\n",
    "- Do some renaming in order to be consistent with the remainder.\n",
    "\n",
    "Main data sources:\n",
    "* ÜNB (2018): EEG-Anlagenstammdaten zur Jahresabrechnung 2017, https://www.netztransparenz.de/EEG/Anlagenstammdaten.\n",
    "* ÜNB (2018): EEG-Bewegungsdaten zur Jahresabrechnung 2017, https://www.netztransparenz.de/EEG/Jahresabrechnungen.\n",
    "\n",
    "The detailled approach for RES modeling is described in the master's thesis of Yannick Werner (YW):\n",
    "\n",
    "Werner, Yannick (2020): Modellierung der Auswirkungen einer CO<sub>2</sub>-Bepreisung auf die EEG-Umlage, master's thesis at the chair of energy and resources economics at TU Berlin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usecols = ['eeg_id', 'capacity', 'energy_source', 'commissioning_date', 'decommissioning_date',\n",
    "           'support_scheme', 'value_applied']\n",
    "eeg_pps_de = pd.read_csv(\n",
    "    main_path[\"inputs\"] + sub_path[\"renewables\"] + input_file[\"eeg_powerplants\"],\n",
    "    usecols=usecols,\n",
    "    parse_dates=['commissioning_date', 'decommissioning_date']\n",
    ")\n",
    "\n",
    "eeg_pps_de_agg = eeg_pps_de.groupby('energy_source').agg({'capacity': 'sum'}) / 1000 # kW to MW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Powerplants, that are decommissioned before \"year\" are dropped.\n",
    "# We don't exclude powerplants that are commissioned after \"year\". Instead, we're not using\n",
    "# the German tender data.\n",
    "#eeg_pps_de.drop(index=eeg_pps_de[eeg_pps_de['commissioning_date'].dt.year > year].index, inplace=True)\n",
    "eeg_pps_de.drop(index=eeg_pps_de[eeg_pps_de['decommissioning_date'].dt.year < year].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_eeg_pps_names = {'Biomasse': 'biomassEEG',\n",
    "#                      'Deponiegas': 'landfillgas',\n",
    "#                      'Geothermie': 'geothermal',\n",
    "#                      'Grubengas': 'minegas',\n",
    "#                      'Klärgas': 'larga',\n",
    "#                      'Solar': 'solarPV',\n",
    "#                      'Wasser': 'ROR',\n",
    "#                      'Wind_Offshore': 'windoffshore',\n",
    "#                      'Wind_Onshore': 'windonshore'}\n",
    "\n",
    "dict_eeg_pps_names = {'biomass': 'biomassEEG',\n",
    "                      'landfill_gas': 'landfillgas',\n",
    "                      'geothermal': 'geothermal',\n",
    "                      'mine_gas': 'minegas',\n",
    "                      'sevage_gas': 'larga',\n",
    "                      'solar': 'solarPV',\n",
    "                      'hydro': 'ROR',\n",
    "                      'wind_offshore': 'windoffshore',\n",
    "                      'wind_onshore': 'windonshore'}\n",
    "\n",
    "eeg_pps_de_agg.rename(index=dict_eeg_pps_names, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run-Of-River\n",
    "Steps applied and data sources used:\n",
    "- For Germany, ROR data is extracted from OPSD data\n",
    "- ROR under the EEG is obtained from the Anlagenstammdaten and added\n",
    "- The capacity of individual plants is summed up in order to create one representative source\n",
    "- It is assumed, that (EEG and non-EEG) run-of-river plants are renewed 1:1 instead of being commissioned.\n",
    "- New-built plant(s) is / are added to the data set.\n",
    "- Read in ROR generation timeseries data, drop NA values and resample from quarter-hourly to hourly frequency\n",
    "- Normalize the ROR generation profile by dividing through overall installed capacity\n",
    "- Data source for European ROR plants is (conventional) power plant matching data (see [this section](#Prepare-European-conventional-PP-Raw-Data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ror_de.rename(columns={'capacity_net_bnetza': 'capacity',\n",
    "                       'eic_code_plant': 'eic_code',\n",
    "                       'name_bnetza': 'name'}, inplace=True)\n",
    "\n",
    "ror_de = pd.concat([ror_de, ror_de_new])\n",
    "\n",
    "ror_de_not_eeg = ror_de.loc[(ror_de['status'] == 'operating') \n",
    "                            & (ror_de['eeg'] == 'no'), ['name', 'country', 'capacity']]\n",
    "\n",
    "\n",
    "ror_de_agg = ror_de_not_eeg.groupby(by='country').agg({'capacity': 'sum'})\n",
    "\n",
    "# Add eeg based capacity\n",
    "ror_de_agg.loc['DE', 'capacity'] += eeg_pps_de_agg.loc['ROR'].values\n",
    "ror_de_agg.index = ['DE_source_ROR']\n",
    "\n",
    "ror_ts_de = pd.read_csv(\n",
    "    main_path[\"inputs\"] + sub_path[\"timeseries\"] + input_file[\"entsoe_generation_de\"],\n",
    "    usecols=['Hydro Run-of-river and poundage  - Actual Aggregated [MW]']\n",
    ")\n",
    "ror_ts_de.columns = ['DE_source_ROR']\n",
    "ror_ts_de.drop(index=ror_ts_de[ror_ts_de['DE_source_ROR'].isna()].index, \n",
    "               inplace=True)\n",
    "ror_ts_de.index = pd.date_range(start='2017-01-01 00:00:00', \n",
    "                                end='2017-12-31 23:45:00', \n",
    "                                freq='15min')\n",
    "ror_ts_de = ror_ts_de.resample('H').mean()\n",
    "\n",
    "# Calculate ROR profile for Germany\n",
    "ror_ts_de = ror_ts_de / ror_de_agg.loc['DE_source_ROR', 'capacity']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other non-fluctuating RES (EEG)\n",
    "Steps applied and data sources used:\n",
    "- Read in power plants capacity and monthly capacity factors which are taken from the master thesis of YW\n",
    "- Determine the number of hours per month for 2017\n",
    "- \"Roll out\" the monthly capacity factors (from 2017) over all hours of a month and assign the hourly time series profile\n",
    "- Transfer the data to an oemof.solph compatible format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources_nonfluc_monthly_capacity_factors = pd.read_csv(\n",
    "    main_path[\"inputs\"] + sub_path[\"renewables\"] + input_file[\"renewables_nonfluctuating\"], \n",
    "    index_col=0\n",
    ")\n",
    "\n",
    "sources_nonfluc_ts = sources_nonfluc_monthly_capacity_factors\n",
    "sources_nonfluc_ts.rename(columns={k: v for k, v in dict_eeg_pps_names.items() \n",
    "                                   if k in sources_nonfluc_ts.columns}, inplace=True)\n",
    "\n",
    "energy_sources = ['biomassEEG', 'landfillgas', 'geothermal', 'minegas', 'larga']\n",
    "sources_nonfluc = eeg_pps_de_agg.loc[energy_sources].reset_index().rename(\n",
    "    columns={'energy_source': 'technology'})\n",
    "\n",
    "sources_nonfluc['country'] = 'DE'\n",
    "\n",
    "sources_nonfluc = tools.nodes_to_oemof(\n",
    "    sources_nonfluc, to='_bus_el', component='_source_', component_suffix_var='technology')\n",
    "sources_nonfluc.drop(columns=['technology'], inplace=True)\n",
    "\n",
    "sources_nonfluc_ts.columns = sources_nonfluc.index\n",
    "sources_nonfluc_ts = sources_nonfluc_ts.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other adjustments for RES in Germany\n",
    "- Overwrite installed RES capacities for Germany with the _true_ ones to derive proper time series (overall capacity of market premium _and_ other (feed-in tariff) RES plants)\n",
    "- Overwrite German RES time series by dividing through the installed RES capacities (leads to a maximum value smaller than one for the \"normalized\" infeed time series, see [this section above](#Prepare-RES-data-for-usage-in-oemof.solph)); Result is assigned to RES buses, not sources (since it is provided by multiple units)\n",
    "- Do some column renaming and fill nan values through linear interpolation\n",
    "- Overwrite data for German RES power plants from the overall RES data set with the accurate data for Germany\n",
    "- Write the RES data for Germany to csv\n",
    "- Load market values from netztransparenz.de and do some data pre-preparations (see function `get_market_values` for details) in order to derive market value time series\n",
    "- Write the market values time series to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RES sources, buses and timeseries\n",
    "fluc_res_de = {'DE_source_solarPV': ('oh_solar', 'DE_bus_solarPV'), \n",
    "               'DE_source_windoffshore': ('oh_windoffshore', 'DE_bus_windoffshore'), \n",
    "               'DE_source_windonshore': ('oh_windonshore', 'DE_bus_windonshore')}\n",
    "\n",
    "for key, value in fluc_res_de.items():\n",
    "    ts_res.loc[:, key] = eeg.load_online_extrapolation(key, value[0])\n",
    "\n",
    "sources_res.loc[fluc_res_de, 'capacity'] = (\n",
    "    eeg_pps_de_agg.loc[[key.split(\"_\")[2] \n",
    "                        for key in fluc_res_de.keys()], 'capacity'].values)\n",
    "\n",
    "sources_ts_res[list(fluc_res_de.keys())] = (\n",
    "    ts_res[list(fluc_res_de.keys())] / sources_res.loc[list(fluc_res_de.keys()), 'capacity'])\n",
    "\n",
    "sources_ts_res.rename(columns={key: value[1] for key, value in fluc_res_de.items()}, \n",
    "                      inplace=True)\n",
    "\n",
    "# Interpolate NaNs\n",
    "print('Timeseries that contain NaNs: ', sources_ts_res.columns[sources_ts_res.isna().any()])\n",
    "sources_ts_res = sources_ts_res.interpolate(method='linear')\n",
    "\n",
    "sources_res.drop(index=list(fluc_res_de.keys()), inplace=True)\n",
    "\n",
    "sources_res_fluc_de = pd.DataFrame(index=list(fluc_res_de.keys()), \n",
    "                                   data=[value[1] for value in fluc_res_de.values()],      \n",
    "                                   columns=['to'])\n",
    "sources_res_fluc_de['country'] = 'DE'\n",
    "sources_res_fluc_de.index.name = 'label'\n",
    "sources_res_fluc_de.to_csv(\n",
    "    main_path[\"outputs\"] + output_file[\"sources_fluc_res\"] + \"_\" + str(year) + \".csv\"\n",
    ")\n",
    "\n",
    "# fname = i\n",
    "# if year != 2020:\n",
    "#     fname = 'netztransparenz_market_values_2017.csv'\n",
    "# else:\n",
    "#     fname = 'netztransparenz_market_values_2020.csv'\n",
    "\n",
    "# Market values\n",
    "market_values_ts = eeg.get_market_values(\n",
    "    netztransparenz=True,\n",
    "    filename=main_path[\"inputs\"] + sub_path[\"renewables\"] + input_file[\"market_values\"],\n",
    "    year=2017\n",
    ")\n",
    "\n",
    "market_values_ts = tools.reindex_time_series(market_values_ts, year)\n",
    "market_values_ts.to_csv(\n",
    "    main_path[\"outputs\"] + output_file[\"costs_market_values\"] + \"_\" + str(year) + \".csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare RES data for usage in oemof.solph\n",
    "\n",
    "Steps applied:\n",
    "- Put together fluctuating RES data\n",
    "- Add country information and electricity bus as well as label\n",
    "- Combine data sets for fluctuating and non-fluctuating RES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources_res = pd.concat([sources_res, ror_de_agg])\n",
    "sources_res['country'] = [c[0] for c in sources_res.index.str.split('_')]\n",
    "sources_res['to'] = sources_res['country'] + '_bus_el'\n",
    "sources_res.index.name = 'label'\n",
    "\n",
    "sources_RES = pd.concat([sources_res, sources_nonfluc, sources_hydro], sort=False)\n",
    "sources_RES_original = sources_RES.copy()\n",
    "\n",
    "sources_ts_res = pd.concat([sources_ts_res, ror_ts_de, sources_nonfluc_ts, rel_avg_hourly_inflow], \n",
    "                           axis=1, \n",
    "                           sort=False)\n",
    "sources_ts_res = np.round(sources_ts_res, 4).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do some garbage collection\n",
    "try:\n",
    "    del (cols, dict_names, dict_rename_ts, el, energy_sources, fluc_res_de, generation_ror,\n",
    "         key, market_values_ts, rel_avg_hourly_inflow, ror_de, ror_de_new, ror_de_not_eeg,\n",
    "         ror_ts_de, sources_nonfluc_monthly_capacity_factors, sources_nonfluc_ts,\n",
    "         sources_res_fluc_de, sources_shortage, sources_shortage_el, sources_shortage_hydro)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renewable capacities and infeed for the target year\n",
    "\n",
    "Basic steps:\n",
    "* For German and European RES, capacity expansion factors per RES source are calculated and installed resp. maximum infeed capacities are scaled accordingly.\n",
    "* European capacities are determined based on IRENA data. For Germany, a study by Prognos et al. is used for capacity projection.\n",
    "* For Germany, an in-depth analysis of new installations is made in order to derive future values applied. Thereby, the results from the German tender procedures are evaluated which will determine prices for the next few years. In addition to that, cost projections are put together based on literature projections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determine European capacity development\n",
    "\n",
    "Steps applied:\n",
    "* Read in capacity data for the status quo from IRENA\n",
    "* Determine capacity development from TYNDP for Europe.\n",
    "* Keep capacity for ROR, hydro and other non-fluctuating resources constant; The data is not included in TYNDP projections, but it seems unlikely that there is a capacity decline.\n",
    "* For 2017 and 2018, use 2019 data.\n",
    "* Determine a scaling factor for the capacity values of RES.\n",
    "    * Use quotient between TYNDP data and capacities from IRENA where applicable.\n",
    "    * Assign values of 1 for reservoir and ROR plants. (no other information available &rarr; assumed to remain constant)\n",
    "    * Use quotient between TYNDP and ENTSOE data from above (peak infeed capacities) where no other information is available in IRENA data set.\n",
    "\n",
    "Data sources:\n",
    "- Data on capacity of fluctuating RES (except denmark), IRENA (2020): https://www.irena.org/Statistics/Download-Data, data downloaded 09.11.2020; all sources, data for 2017 used.\n",
    "- Data on fluctuating RES for denmark: ENTSO-E (2019): Generation - Installed Capacity per Production Type, BZN DK1 / DK2, https://transparency.entsoe.eu/dashboard/show, accessed 20.11.2019. Data for recent years seems to underestimate capacities slightly compared to IRENA data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources_RES_cap = pd.read_csv(\n",
    "    main_path[\"inputs\"] + sub_path[\"renewables\"] + input_file[\"REcap_eu\"],\n",
    "    sep=\";\", decimal=\",\"\n",
    ")\n",
    "DK_cap = pd.read_csv(\n",
    "    main_path[\"inputs\"] + sub_path[\"renewables\"] + input_file[\"REcap_DK\"],\n",
    "    sep=\";\", decimal=\",\"\n",
    ")\n",
    "\n",
    "sources_RES_cap.loc[:,'Technology'] = sources_RES_cap.loc[:,'Technology'].fillna(method='ffill')\n",
    "\n",
    "sources_RES_cap.rename(columns = {'Country/area': 'country', 'Technology': 'technology'}, \n",
    "                       inplace = True)\n",
    "sources_RES_cap.drop(columns = 'Indicator', inplace = True)\n",
    "sources_RES_cap['country'].replace({'Europe': 'EU', 'Austria': 'AT', 'Belgium': 'BE', \n",
    "                                    'Czechia': 'CZ', 'Denmark': 'DK', 'Finland': 'FI', \n",
    "                                    'France': 'FR', 'Germany': 'DE', 'Hungary': 'HU', \n",
    "                                    'Luxembourg': 'LU', 'Netherlands': 'NL', 'Norway': 'NO', \n",
    "                                    'Poland': 'PL', 'Portugal': 'PO', 'Slovakia': 'SK',\n",
    "                                    'Spain': 'ES', 'Sweden': 'SE', 'Switzerland': 'CH', \n",
    "                                    'UK': 'GB'}, \n",
    "                                   inplace = True)\n",
    "\n",
    "sources_RES_cap['technology'].replace({'Onshore wind energy': 'windonshore', \n",
    "                                       'Offshore wind energy': 'windoffshore',\n",
    "                                       'Solar photovoltaic': 'solarPV'}, \n",
    "                                      inplace = True)\n",
    "sources_RES_cap = sources_RES_cap[(sources_RES_cap['technology'].isin(['windonshore', 'windoffshore', 'solarPV']))\n",
    "                                  & (sources_RES_cap['country'].isin(countries_modeled))]\n",
    "sources_RES_cap.columns= sources_RES_cap.columns.astype(str)\n",
    "\n",
    "DK_cap['technology'].replace({'Wind Onshore': 'windonshore', \n",
    "                              'Wind Offshore': 'windoffshore', \n",
    "                              'Solar': 'solarPV'}, \n",
    "                             inplace = True)\n",
    "DK_cap = DK_cap[DK_cap['technology'].isin(['windonshore', 'windoffshore', 'solarPV'])]\n",
    "DK_cap[['2016', '2017', '2018']] = DK_cap[['2016', '2017', '2018']].astype('int32')\n",
    "\n",
    "sources_RES_cap = pd.concat([sources_RES_cap, DK_cap], sort = True)\n",
    "\n",
    "sources_RES_cap.drop(columns=['2016','2018'], inplace=True)\n",
    "sources_RES_cap.drop(index=sources_RES_cap[sources_RES_cap['2017'].isna()].index, inplace=True)\n",
    "sources_RES_cap.rename(columns={'2017': 'capacity', \n",
    "                                'technology': 'fuel'}, inplace=True)\n",
    "\n",
    "sources_RES_cap['label'] = sources_RES_cap['country'] + '_source_' + sources_RES_cap['fuel']\n",
    "sources_RES_cap.set_index('label', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TYNDP data on RES\n",
    "renewables_eu_2025_BEST['label'] = (renewables_eu_2025_BEST['country'].values\n",
    "                                    + '_source_' + renewables_eu_2025_BEST['fuel'].values)\n",
    "renewables_eu_2025_BEST.set_index('label', inplace=True)\n",
    "renewables_eu_2025_BEST.loc[:,'to'] = renewables_eu_2025_BEST.loc[:,'country'] + '_bus_el'\n",
    "renewables_eu_2025_BEST.drop(columns='fuel', inplace=True)\n",
    "renewables_eu_2025_BEST = renewables_eu_2025_BEST.sort_values(by='country')\n",
    "\n",
    "renewables_eu_2030_DG['label'] = (renewables_eu_2030_DG['country'].values\n",
    "                                  + '_source_' + renewables_eu_2030_DG['fuel'].values)\n",
    "renewables_eu_2030_DG.set_index('label', inplace=True)\n",
    "renewables_eu_2030_DG.loc[:,'to'] = renewables_eu_2025_BEST.loc[:,'country'] + '_bus_el'\n",
    "renewables_eu_2030_DG.drop(columns='fuel', inplace=True)\n",
    "renewables_eu_2030_DG = renewables_eu_2030_DG.sort_values(by='country')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sources_res = pd.concat([sources_res, sources_nonfluc, sources_hydro, sources_RES])\n",
    "all_sources_res = all_sources_res.loc[~(all_sources_res.index.duplicated(keep=\"first\"))]\n",
    "\n",
    "renewables_eu_capacities = pd.DataFrame(\n",
    "    index=all_sources_res.index.union(renewables_eu_2025_BEST.index).union(renewables_eu_2030_DG.index),\n",
    "    columns=list(range(2019, 2031)), dtype=\"float64\")\n",
    "\n",
    "renewables_eu_capacities[2019] = all_sources_res[\"capacity\"].astype(\"float64\")\n",
    "renewables_eu_capacities[2025] = renewables_eu_2025_BEST[\"capacity\"]\n",
    "renewables_eu_capacities[2030] = renewables_eu_2030_DG[\"capacity\"]\n",
    "\n",
    "renewables_eu_capacities.loc[:, [2019, 2025, 2030]] = (\n",
    "    renewables_eu_capacities.loc[:, [2019, 2025, 2030]].fillna(0))\n",
    "renewables_eu_capacities = renewables_eu_capacities.interpolate(axis=\"columns\").round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_constant_ix = [ix for ix in renewables_eu_capacities.index if '_ROR' in ix \n",
    "                    or ix in sources_nonfluc.index or ix in sources_hydro.index]\n",
    "renewables_eu_capacities.loc[keep_constant_ix,:] = (\n",
    "    renewables_eu_capacities.loc[keep_constant_ix, 2019].values.repeat(len(\n",
    "        renewables_eu_capacities.columns)).reshape(len(keep_constant_ix), len(renewables_eu_capacities.columns)))\n",
    "\n",
    "# Create subset for which capacities are extracted directly; no status quo capacity values given\n",
    "diff_ix = renewables_eu_capacities.index.difference(sources_RES.index)\n",
    "renewables_eu_abs_capacities = renewables_eu_capacities.loc[diff_ix]\n",
    "renewables_eu_abs_capacities[[2017, 2018]] = 0.0\n",
    "renewables_eu_capacities = renewables_eu_capacities.loc[[ix for ix in renewables_eu_capacities.index \n",
    "                                                         if ix not in diff_ix]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine capacity expansion factors for RES:\n",
    "* As quotient between capacity for 2030 from TYNDP and status quo of installed capacities (IRENA)\n",
    "* Values are corrected:\n",
    "    * For the status quo year (2019), values are set to 1. The same is made for 2017 and 2018.\n",
    "    * For wind onshore in DK1, the status quo values given were comparatively high whereas the TYNDP projection was moderate. Values were adjusted to match the moderate TYNDP projection.\n",
    "* Capacities are assumed to remain constant for the remaining RES incl. hydro and ROR (=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in renewables_eu_capacities.columns:\n",
    "    renewables_eu_capacities[col] = renewables_eu_capacities[col].div(sources_RES_cap['capacity'])\n",
    "    renewables_eu_capacities[col] = np.where(renewables_eu_capacities[col] < 1, 1.0, \n",
    "                                             renewables_eu_capacities[col])\n",
    "\n",
    "renewables_eu_capacities[2019] = np.where(renewables_eu_capacities[2019] > 1, 1.0, \n",
    "                                          renewables_eu_capacities[2019]) \n",
    "\n",
    "# Correct values for wind onshore in DK1\n",
    "DK_val_2030 = renewables_eu_capacities.at[\"DK1_source_windonshore\", 2030]\n",
    "renewables_eu_capacities.loc[\"DK1_source_windonshore\"] = np.nan\n",
    "renewables_eu_capacities.at[\"DK1_source_windonshore\", 2030] = DK_val_2030\n",
    "renewables_eu_capacities.at[\"DK1_source_windonshore\", 2019] = 1.0\n",
    "renewables_eu_capacities.interpolate(axis=1)\n",
    "\n",
    "# Fill na values and assume capacities for 2017 and 2018 to be the same than 2019 values\n",
    "renewables_eu_capacities = renewables_eu_capacities.fillna(1)\n",
    "renewables_eu_capacities[[2017, 2018]] = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add RES projection for Germany\n",
    "\n",
    "Add capacity assumption for RES capacity development for Germany:\n",
    "* Capacity values for 2030 can be either based on\n",
    "    * a study by Prognos, Öko-Institut and Wuppertal-Institut on behalf of Agora Energiewende, Agora Verkahreswende and Stiftung Klimaneutralitä or\n",
    "    * EEG 2021 capacity expansion targets.\n",
    "* Values for water comprise ROR, reservoir as well as phes with natural inflow (same as in BMWi statistics from AGEB). &rarr; ROR part needed here is probably assumed to reamin constant and so is proceeded here.\n",
    "\n",
    "Steps applied:\n",
    "* Read in and select data source to be used (Prognos or EEG_2021)\n",
    "* Filter capacity values for 2030\n",
    "* Determine capacity expansion factors by dividing the capacities for 2030 through those of the status quo \n",
    "\n",
    "Full citations:\n",
    "* Prognos, Öko-Institut and Wuppertal-Institut (2020): Klimaneutrales Deutschland. In drei Schritten zu null Treibhausgasen bis 2050 über ein Zwischenziel von -65 % im Jahr 2030 als Teil des EU-Green-Deals. Study on behalf of Agora Energiewende, Agora Verkehrswende and Stiftung Klimaneutralität (full version), https://www.agora-energiewende.de/veroeffentlichungen/klimaneutrales-deutschland/, p. 53, accessed 09.11.2020.\n",
    "* EEG 2021: Gesetz für den Ausbau erneuerbarer Energien (Erneuerbare-Energien-Gesetz - EEG 2021). Erneuerbare-Energien-Gesetz vom 21. Juli 2014 (BGBl. I S. 1066), das zuletzt durch Artikel 1 des Gesetzes vom21. Dezember 2020 (BGBl. I S. 3138) geändert worden ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_DE_EEG_2021 = pd.read_csv(\n",
    "    main_path[\"inputs\"] + sub_path[\"renewables\"] + input_file[\"RES_DE_EEG_2021\"],\n",
    "    sep=\";\", decimal=\",\"\n",
    ")\n",
    "\n",
    "res_DE_Prognos = pd.read_csv(\n",
    "    main_path[\"inputs\"] + sub_path[\"renewables\"] + input_file[\"RES_DE_Prognos\"],\n",
    "    sep=\";\", decimal=\",\"\n",
    ")\n",
    "\n",
    "# Drop 2018 capacities\n",
    "res_DE_Prognos.drop(index=res_DE_Prognos[res_DE_Prognos[\"year\"] == 2018].index, inplace=True)\n",
    "\n",
    "res_capacity_projections = {\"Prognos\": res_DE_Prognos,\n",
    "                            \"EEG_2021\": res_DE_EEG_2021}\n",
    "\n",
    "if res_capacity_projection not in res_capacity_projections.keys():\n",
    "    raise ValueError(\"`res_capacity_projection` has to be either 'Prognos' or 'EEG_2021'.\")\n",
    "\n",
    "res_de_projection = res_capacity_projections[res_capacity_projection]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_de_projection.set_index(\"fuel\", inplace=True)\n",
    "\n",
    "res_de_capacity_development = pd.DataFrame(\n",
    "    index=res_de_projection.index.unique(),\n",
    "    columns=range(2020, 2031), dtype=\"float64\")\n",
    "\n",
    "years_for_iteration = []\n",
    "if res_capacity_projection == \"Prognos\":\n",
    "    years_for_iteration = [2025, 2030]\n",
    "elif res_capacity_projection == \"EEG2021\":\n",
    "    years_for_iteration = [2022, 2024, 2026, 2028,2030]\n",
    "\n",
    "for iter_year in years_for_iteration:\n",
    "    try:\n",
    "        res_de_capacity_development[iter_year] = (\n",
    "            res_de_projection.loc[res_de_projection[\"year\"] == iter_year, \"capacity [GW]\"].astype(\"float64\"))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "#if res_capacity_projection == \"EEG2021\":\n",
    "eeg_pps_de_agg[\"label\"] = \"DE_source_\" + eeg_pps_de_agg.index\n",
    "#res_de_capacity_development[2020] = eeg_pps_de_agg.set_index(\"label\").loc[:,\"capacity\"]\n",
    "res_de_capacity_development[2020] = eeg_pps_de_agg.loc[res_de_capacity_development.index, 'capacity']/1000\n",
    "    \n",
    "res_de_capacity_development = res_de_capacity_development.interpolate(axis=\"columns\").round(1)\n",
    "res_de_capacity_development = res_de_capacity_development.mul(1000).div(\n",
    "    eeg_pps_de_agg[\"capacity\"], axis=0).fillna(1)\n",
    "res_de_capacity_development.loc[\"biomassEEG\"] = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scale (up) capacities for renewable infeed\n",
    "\n",
    "Renewable infeed for the target year is determined by a simple scaling the renewable capacities by the capacity expansion factors (capacity in 2030 / capacity in 2017 resp. 2019) determined above:\n",
    "* The capacities of the RES sources are multiplied by the capacity expansion factors.<br>\n",
    "_NOTE: While they represent installed capacities for Germany, for other European countries they represent the peak infeed capacity._\n",
    "* The infeed timeseries themselves are not modified, but taken as they are.\n",
    "* For RES not contained in the status quo, \n",
    "    * the absolute capacity projections from the TYNDP are used and assigned and\n",
    "    * timeseries of neighbouring countries are assigned as a proxy.\n",
    "    * _Note: A scaling is not made due to the inconsistencies of the IRENA / ENTSOE data.<br>Actually, this would be useful since one is installed capacities while the other is maximum outputs._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_de_capacity_development[\"label\"] = \"DE_source_\" + res_de_capacity_development.index\n",
    "res_de_capacity_development.set_index(\"label\", inplace=True)\n",
    "\n",
    "res_capacities = pd.concat([renewables_eu_capacities, res_de_capacity_development])\n",
    "res_capacities = res_capacities.loc[~res_capacities.index.duplicated()]\n",
    "res_capacities[2020] = 1.0\n",
    "\n",
    "# Determine capacity in target year\n",
    "sources_RES[\"capacity\"] = sources_RES[\"capacity\"].astype(\"float64\").mul(res_capacities[year], axis=0).round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "renewables_eu_abs_capacities = renewables_eu_abs_capacities[[year]]\n",
    "renewables_eu_abs_capacities[\"index_label\"] = renewables_eu_abs_capacities.index\n",
    "renewables_eu_abs_capacities[\"country\"] = (\n",
    "    renewables_eu_abs_capacities[\"index_label\"].str.split(\"_\", n=1, expand=True)[0])\n",
    "renewables_eu_abs_capacities[\"to\"] = renewables_eu_abs_capacities[\"country\"] + \"_bus_el\"\n",
    "renewables_eu_abs_capacities = renewables_eu_abs_capacities.drop(\n",
    "    columns=\"index_label\").rename(columns={year: \"capacity\"})\n",
    "\n",
    "sources_RES = sources_RES.append(renewables_eu_abs_capacities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some bidding zones for which no capacity information for certain RES for the status quo is available.\n",
    "Thus, no timeseries information exists.\n",
    "\n",
    "As a first proxy, these market zones will simply be assigned timeseries from neighbouring countries:\n",
    "* FR offshore &rarr; DE offshore\n",
    "* NO2-5 & SE1-4 solarPV &rarr; DK solarPV\n",
    "* PL solarPV and offshore &rarr; DE solarPV and offshore\n",
    "* SE4 offshore &rarr; DK offshore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_res_ts_dict = {\n",
    "    'FR_source_windoffshore': 'DE_bus_windoffshore',\n",
    "    'NO2_source_solarPV': 'DK1_source_solarPV',\n",
    "    'NO3_source_solarPV': 'DK1_source_solarPV',\n",
    "    'NO4_source_solarPV': 'DK1_source_solarPV',\n",
    "    'NO5_source_solarPV': 'DK1_source_solarPV',\n",
    "    'NO5_source_windonshore': 'DK1_source_windonshore',\n",
    "    'PL_source_solarPV': 'DE_bus_solarPV',\n",
    "    'PL_source_windoffshore': 'DE_bus_windoffshore',\n",
    "    'SE1_source_solarPV': 'DK2_source_solarPV',\n",
    "    'SE2_source_solarPV': 'DK2_source_solarPV',\n",
    "    'SE3_source_solarPV': 'DK2_source_solarPV',\n",
    "    'SE4_source_solarPV': 'DK2_source_solarPV',\n",
    "    'SE4_source_windoffshore': 'DK2_source_windoffshore'}\n",
    "\n",
    "# Assign missing timeseries\n",
    "for k, v in missing_res_ts_dict.items():\n",
    "    sources_ts_res[k] = sources_ts_res.loc[:,v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources_RES.to_csv(\n",
    "    main_path[\"outputs\"] + output_file[\"sources_renewables\"] + \"_\" + str(year) + \".csv\"\n",
    ")\n",
    "\n",
    "sources_ts_res = tools.reindex_time_series(sources_ts_res, year)\n",
    "\n",
    "sources_ts_res.to_csv(\n",
    "    main_path[\"outputs\"] + output_file[\"sources_renewables_ts\"] + \"_\" + str(year) + \".csv\"\n",
    ")\n",
    "sources_RES.to_excel(writer, sheet_name='sources_renewables' + \"_\" + str(year))\n",
    "sources_ts_res.tz_localize(None).to_excel(writer, sheet_name='sources_renewables_ts' + \"_\" + str(year))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extension for Germany: Determine new-built RES capacities and values applied\n",
    "General approach:\n",
    "* Add plants commissioned until 2030 and introduce an assumption for the values applied by year.\n",
    "* For commissionings until 2022 (2025 for offshore) the current results from the EEG tender procedures (capacities and prices) are used.\n",
    "* For commissionings from 2023 on (2026 for offshore), an assumption for the capacities to be commissioned is made. Therefore, the target capacity to be build is split equally onto the years from 2023 to 2030.\n",
    "* Thereby, decommissionings based on unit age have to be taken into account. It is assumed that plants are decommissioned after 20 years of operation.\n",
    "\n",
    "#### Determine decommissioning years and calculate (gross) capacity expansion needed\n",
    "\n",
    "Determine amount of RES capacity to be installed for every year:\n",
    "* Decommissioning of EEG plants will happen from 2021 on after 20 years of operation.\n",
    "    * For the sake of simplicity, use commissioning year only and only stick to the rules from prior EEG versions (until EEG 2014) which granted payments for 20 years + remainder of the commissioning year. &rarr; Assume 21 years lifetime.\n",
    "    * Determine the remaining capacities by energy source and store them in a dictionary indexed by years.\n",
    "* Determine the amount of capacity commissioned until 2022 by already finished RES tenders.\n",
    "* Determine the amount of (gross) capacity to be added to fill up the gap up to the installed capacities for 2030 given in the study from Prognos et al. (2020) or the EEG 2021."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eeg_pps_de['commissioning_year'] = eeg_pps_de['commissioning_date'].astype(str).str.slice(start=0, stop=4).astype(int)\n",
    "eeg_pps_de['decommissioning_year'] = np.where(eeg_pps_de['commissioning_year'] + 21 > 2020, \n",
    "                                              eeg_pps_de['commissioning_year'] + 21, 2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the amount of installed capacity remaining for every year\n",
    "eeg_pps_de_agg_by_year = pd.DataFrame(index=eeg_pps_de.energy_source.unique(), \n",
    "                                      columns=range(2020, 2031), dtype=\"float64\")\n",
    "\n",
    "for iter_year in range(2020, 2031):\n",
    "    to_group = eeg_pps_de[eeg_pps_de['decommissioning_year'] > iter_year]\n",
    "    eeg_pps_de_agg_by_year[iter_year] = (to_group.groupby('energy_source').agg({'capacity': 'sum'}) \n",
    "                                         / 1000).round(1)  # convert kW to MW\n",
    "    del to_group\n",
    "    \n",
    "eeg_pps_de_agg_by_year.rename(index=dict_eeg_pps_names, inplace=True)\n",
    "\n",
    "cap_needed_until_2030 = res_de_projection.loc[res_de_projection[\"year\"] == 2030, 'capacity [GW]'].mul(1000).sub(\n",
    "    eeg_pps_de_agg_by_year[2030]).fillna(0).round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap_needed_until_2030"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Include data from German RES tenders\n",
    "\n",
    "* Read in the data for the different energy sources.\n",
    "* Decrease the amount of capacity needed by\n",
    "    * capacity to be commissioned from the EEG tender procedures and\n",
    "    * capacity installed elsewhise (only for solarPV).\n",
    "* Obtain values applied from the tender data.\n",
    "\n",
    "_NOTE: For the sake of simplicity, the deviating deadlines for Bürgerenergiegesellschaften are neglected for wind onshore._\n",
    "\n",
    "RES tenders for wind onshore\n",
    "\n",
    "**@JK**: TODO: add/update tenders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onshore_tenders = pd.read_excel(\n",
    "    main_path[\"inputs\"] + sub_path[\"renewables\"] + input_file[\"onshore_tenders\"],\n",
    "    sheet_name='Alle Runden', skiprows=5, nrows=23, index_col=0, \n",
    "    usecols=[0, 8, 10, 11, 12, 20]\n",
    ")\n",
    "\n",
    "dict_tender_cols = {'Zuschlagsmenge (kW)': 'capacity_awarded',\n",
    "                    'Zuschlagswerte (ct/kWh)': 'min',\n",
    "                    'Unnamed: 11': 'max',\n",
    "                    'Unnamed: 12': 'weighted_ave',\n",
    "                    'Frist zur Inbetriebnahme ohne Fördersatzreduktion': 'due_date'}\n",
    "onshore_tenders.rename(columns=dict_tender_cols, inplace=True)\n",
    "onshore_tenders = onshore_tenders[~(onshore_tenders.index.isna())]\n",
    "\n",
    "# Due to the EEG, deadline for commissioning is 24 months for the tenders in 2019\n",
    "onshore_tenders.loc[onshore_tenders.due_date.isna(), \n",
    "                    'due_date'] = (onshore_tenders.loc[onshore_tenders.due_date.isna()].index\n",
    "                                   + pd.DateOffset(months=24) + pd.tseries.offsets.MonthEnd(1))\n",
    "\n",
    "# Determine weigthed averages per commissioning year\n",
    "onshore_tenders['tender_year'] = onshore_tenders.index.year\n",
    "onshore_tenders['commissioning_year'] = pd.DatetimeIndex(onshore_tenders.due_date).year\n",
    "\n",
    "wm = lambda x: np.average(x, weights=onshore_tenders.loc[x.index, 'capacity_awarded'])\n",
    "\n",
    "onshore_tenders.loc[:,['min', 'max', 'weighted_ave']] = (\n",
    "    onshore_tenders.loc[:,['min', 'max', 'weighted_ave']].astype(float))\n",
    "\n",
    "windonshore_commissionings = onshore_tenders.groupby('commissioning_year').aggregate({\n",
    "    'capacity_awarded':'sum',\n",
    "    'min': wm, 'max': wm, 'weighted_ave': wm})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RES tenders for PV and common RES tender (wind + PV):\n",
    "* So far, only PV plants have been successful within the common tender procedure.\n",
    "* Hence, the capacities are added onto the PV bids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solarPV_tenders = pd.read_excel(\n",
    "    main_path[\"inputs\"] + sub_path[\"renewables\"] + input_file[\"solar_tenders\"],\n",
    "    sheet_name='Alle Runden', skiprows=6, nrows=26, index_col=0, \n",
    "    usecols=[1, 9, 11, 12, 13, 19])\n",
    "\n",
    "common_tenders = pd.read_excel(\n",
    "    main_path[\"inputs\"] + sub_path[\"renewables\"] + input_file[\"common_tenders\"],\n",
    "    sheet_name='Alle Runden', skiprows=5, nrows=7, index_col=0, \n",
    "    usecols=[0, 8, 10, 11, 12, 18])\n",
    "\n",
    "dict_tender_cols['Unnamed: 12'] = 'max'\n",
    "dict_tender_cols['Unnamed: 13'] = 'weighted_ave'\n",
    "dict_tender_cols['Frist zum Antrag auf Zahlungsberechtigung2 ohne Fördersatzreduktion'] = 'due_date'\n",
    "solarPV_tenders.rename(columns=dict_tender_cols, inplace=True)\n",
    "\n",
    "dict_tender_cols[\"Zuschlagsmenge (MW)\"] = 'capacity_awarded'\n",
    "dict_tender_cols[\"Unnamed: 11\"] = 'max'\n",
    "dict_tender_cols[\"Unnamed: 12\"] = 'weighted_ave'\n",
    "dict_tender_cols['Frist zum Antrag auf Zahlungsberechtigung2 ohne Fördersatzreduktion\\n - bei Solar -'] = 'due_date'\n",
    "common_tenders.rename(columns=dict_tender_cols, inplace=True)\n",
    "\n",
    "solarPV_tenders = solarPV_tenders[~(solarPV_tenders.index.isna())]\n",
    "common_tenders = common_tenders[~(common_tenders.index.isna())]\n",
    "solarPV_tenders = solarPV_tenders.append(common_tenders)\n",
    "\n",
    "solarPV_tenders.loc[solarPV_tenders.due_date.isna(), \n",
    "                    'due_date'] = (solarPV_tenders.loc[solarPV_tenders.due_date.isna()].index\n",
    "                                   + pd.DateOffset(months=24) + pd.tseries.offsets.MonthEnd(1))\n",
    "\n",
    "# Determine weigthed averages per commissioning year\n",
    "solarPV_tenders['tender_year'] = solarPV_tenders.index.year\n",
    "solarPV_tenders['commissioning_year'] = pd.DatetimeIndex(solarPV_tenders.due_date).year\n",
    "\n",
    "wm2 = lambda x: np.average(x, weights=solarPV_tenders.loc[x.index, 'capacity_awarded'])\n",
    "solarPV_tenders.loc[:,['min', 'max', 'weighted_ave']] = (\n",
    "    solarPV_tenders.loc[:,['min', 'max', 'weighted_ave']].astype(float))\n",
    "\n",
    "solarPV_commissionings = solarPV_tenders.groupby('commissioning_year').aggregate({\n",
    "    'capacity_awarded':'sum',\n",
    "    'min': wm2, 'max': wm2, 'weighted_ave': wm2})\n",
    "solarPV_commissionings.drop([2016, 2017], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RES tenders for wind offshore (transitional model):\n",
    "\n",
    "Data sources:\n",
    "* BNetzA (2017): BK6-17-001, Ergebnisse der 1. Ausschreibung vom 01.04.2017, https://www.bundesnetzagentur.de/DE/Service-Funktionen/Beschlusskammern/1_GZ/BK6-GZ/2017/BK6-17-001/Ergebnisse_erste_Ausschreibung.pdf?__blob=publicationFile&v=3, Abruf am 17.11.2020.\n",
    "* BNetzA (2018): BK6-18-001, Ergebnisse der 2. Ausschreibung vom 01.04.2018, https://www.bundesnetzagentur.de/DE/Service-Funktionen/Beschlusskammern/1_GZ/BK6-GZ/2018/BK6-18-001/Ergebnisse_zweite_ausschreibung.pdf?__blob=publicationFile&v=3, Abruf am 17.11.2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windoffshore_commissionings = pd.DataFrame(\n",
    "    index=['2024', '2025'],\n",
    "    data={'capacity_awarded': [1490000, 1610000], \n",
    "          'min': [0, 0],\n",
    "          'max': [6, 9.83],\n",
    "          'weighted_ave': [0.44, 4.66]})\n",
    "\n",
    "windoffshore_commissionings.index.name='commissioning_year'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decrease capacity needed until 2030 by the amount of capacity already (to be) installed from the tender procedures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap_needed_until_2030.loc['windonshore'] -= windonshore_commissionings.loc[2021:2022, \n",
    "                                                                           'capacity_awarded'].div(1000).sum()\n",
    "cap_needed_until_2030.loc['windoffshore'] -= windoffshore_commissionings['capacity_awarded'].div(1000).sum()\n",
    "cap_needed_until_2030.loc['solarPV'] -= solarPV_commissionings.loc[2021:2020, \n",
    "                                                                   'capacity_awarded'].div(1000).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add capacity from PV build outside the tendering procedures:\n",
    "* rooftop PV plants, tenant electricity plants (\"Mieterstrom\") and smaller open space plants &rarr; The value applied is determined by the gross capacity expansion of those plants for which in turn the corrected expansion values are used.\n",
    "* The capacity values and remuneration values are read in, whereby the values are adjusted every quarter based on the rolling half year before.\n",
    "* The capacity values are aggregated per year and subtracted from the capacity exapansion need for solarPV until 2030\n",
    "\n",
    "Data sources:\n",
    "* BNetzA (2020): EEG-Registerdaten und Fördersätze, https://www.bundesnetzagentur.de/DE/Sachgebiete/ElektrizitaetundGas/Unternehmen_Institutionen/ErneuerbareEnergien/ZahlenDatenInformationen/EEG_Registerdaten/EEG_Registerdaten_node.html;jsessionid=C9A499BB48D4F32668B73B6769FBBD84, Abruf am 19.11.2020.\n",
    "* BNetzA (2020): Archiv EEG-Vergütungssätze und Datenmeldungen, https://www.bundesnetzagentur.de/DE/Sachgebiete/ElektrizitaetundGas/Unternehmen_Institutionen/ErneuerbareEnergien/ZahlenDatenInformationen/EEG_Registerdaten/ArchivDatenMeldgn/ArchivDatenMeldgn_node.html, Abruf am 19.11.2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_cap_names_dict = {#'2018_01': (input_file[\"PV_2018_01\"], 6),\n",
    "                     #'2019_01': (input_file[\"PV_2019_01\"], 6),\n",
    "                     #'2019_02': (input_file[\"PV_2019_02\"], 6),\n",
    "                     #'2020_01': (input_file[\"PV_2020_01\"], 6),\n",
    "                     #'2020_02': (input_file[\"PV_2020_02\"], 6),\n",
    "                     '2021_01': (input_file[\"PV_2021_01\"], 6),\n",
    "                     '2021_02': (input_file[\"PV_2021_02\"], 3),\n",
    "                     '2021_03': (input_file[\"PV_2021_03\"], 3)}\n",
    "\n",
    "pv_cap_dict = {}\n",
    "\n",
    "pv_capacities = pd.DataFrame(columns=['capacity'])\n",
    "pv_capacities.index.name = 'year'\n",
    "\n",
    "pv_col_names = {'Leistung (kWp)': 'capacity',\n",
    "                'Leistung (kWp) 1': 'capacity',\n",
    "                'Leistung (kW) 1': 'capacity'}\n",
    "\n",
    "for k, v in pv_cap_names_dict.items():\n",
    "    # Extract capacity installations info\n",
    "    pv_cap_dict[k] = pd.read_excel(\n",
    "        main_path[\"inputs\"] + sub_path[\"renewables\"] + v[0],\n",
    "        skiprows=6, usecols=\"B:C\", nrows=v[1], index_col=0\n",
    "    )\n",
    "\n",
    "    if not isinstance(pv_cap_dict[k].index, pd.DatetimeIndex):\n",
    "        pv_cap_dict[k].index = pv_cap_dict[k].index.str.strip('*')\n",
    "        pv_cap_dict[k]['year'] = pv_cap_dict[k].index.str[-4:]\n",
    "    else:\n",
    "        pv_cap_dict[k]['year'] = pv_cap_dict[k].index.year\n",
    "    \n",
    "    pv_cap_dict[k].rename(columns=pv_col_names, inplace=True)\n",
    "    pv_capacities = pv_capacities.append(pv_cap_dict[k].groupby('year').agg('sum'))\n",
    "    \n",
    "pv_capacities.reset_index(inplace=True)\n",
    "pv_capacities['year'] = pv_capacities['year'].astype(str)\n",
    "pv_capacities_agg = pv_capacities.groupby('year').agg('sum').div(1000)\n",
    "\n",
    "# Drop values for 2017 (already given)\n",
    "pv_capacities_agg.drop(index='2020', inplace=True)\n",
    "pv_capacities_agg.at['2021', 'capacity'] = pv_capacities_agg.at['2021', 'capacity']\n",
    "\n",
    "cap_needed_until_2030.loc['solarPV'] -= pv_capacities_agg['capacity'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribute the capacity needed (gross) installations:\n",
    "* Gross capacity installations have to take place between 2023 and 2030 for wind and solarPV\n",
    "* For windoffshore, gross installations equal the net installations and take place between 2026 and 2030\n",
    "* Installations from tender procedures are given for PV &rarr; I.e., remaining rooftop installations needed for PV for 2021 and 2022 are given by the annual capacity expansion need minus the installations from the tender procedures (and already installed rooftop capacity for 2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annual_cap_expansion = pd.DataFrame(dtype=float, index=['windonshore', 'solarPV', 'windoffshore'],\n",
    "                                    columns=['capacity', 'start_year'])\n",
    "\n",
    "annual_cap_expansion.loc['windonshore', \n",
    "                         'solarPV', \n",
    "                         'windoffshore'] = [(cap_needed_until_2030.loc['windonshore'] / (2030 - 2022), 2023), \n",
    "                                            (cap_needed_until_2030.loc['solarPV'] / (2030 - 2021), 2021),\n",
    "                                            (cap_needed_until_2030.loc['windoffshore'] / (2030 - 2025), 2026)]\n",
    "\n",
    "pv_capacities_agg.loc['2021'] = (annual_cap_expansion.at['solarPV', 'capacity'] \n",
    "                                 - solarPV_commissionings.at[2021, 'capacity_awarded'] / 1000)\n",
    "pv_capacities_agg.loc['2022'] = (annual_cap_expansion.at['solarPV', 'capacity'] \n",
    "                                 - solarPV_commissionings.at[2022, 'capacity_awarded'] / 1000)\n",
    "pv_capacities_agg.index = pv_capacities_agg.index.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add capacities and values applied from the RES tender scheme\n",
    "Determine remaining capacities in target year:\n",
    "* Filter out plants with a decommissioning year prior to the target year.\n",
    "* Include DataFrames holding artificial new-built units. Artificial new-built units hereby means a collection of units to be newly installed which are grouped by their values applied due to a given capacity distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out plants to be decommissioned\n",
    "eeg_pps_de = eeg_pps_de[eeg_pps_de['decommissioning_year'] > year]\n",
    "\n",
    "eeg_pps_de_solarPV = pd.DataFrame(columns=eeg_pps_de.columns)\n",
    "eeg_pps_de_windonshore = pd.DataFrame(columns=eeg_pps_de.columns)\n",
    "eeg_pps_de_windoffshore = pd.DataFrame(columns=eeg_pps_de.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply a function to determine capacity distribution as well as another function to create new-built units:\n",
    "* The data sets each contain a lower, a middle and an upper value for the value applied.\n",
    "* A beta distribution function for the capacity shares is introduced and the corresponding equally sliced values applied are attributed to the corresponding capacity shares.\n",
    "* Add power plants based on capacity distribution to the new-built DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create capacity distributions and corresponding values applied\n",
    "PV_cap_distribution = eeg.create_cap_distribution_from_beta(\n",
    "    solarPV_commissionings,\n",
    "    cols=['min', 'weighted_ave', 'max'])\n",
    "                                              \n",
    "onshore_cap_distribution = eeg.create_cap_distribution_from_beta(\n",
    "    windonshore_commissionings,\n",
    "    cols=['min', 'weighted_ave', 'max'])\n",
    "                                              \n",
    "offshore_cap_distribution = eeg.create_cap_distribution_from_beta(\n",
    "    windoffshore_commissionings,\n",
    "    cols=['min', 'weighted_ave', 'max'])\n",
    "\n",
    "# Create new-built units with the corresponding market values\n",
    "new_eeg_pps_de_solarPV = eeg.add_new_built_res(PV_cap_distribution, eeg_pps_de_solarPV, \n",
    "                                               solarPV_commissionings, 'Solar', \n",
    "                                               capacity_col='capacity_awarded', indexed_by_years=True)\n",
    "\n",
    "new_eeg_pps_de_windonshore = eeg.add_new_built_res(onshore_cap_distribution, eeg_pps_de_windonshore, \n",
    "                                                   windonshore_commissionings, 'Wind_Onshore', \n",
    "                                                   capacity_col='capacity_awarded', indexed_by_years=True)\n",
    "\n",
    "new_eeg_pps_de_windoffshore = eeg.add_new_built_res(offshore_cap_distribution, eeg_pps_de_windoffshore, \n",
    "                                                    windoffshore_commissionings, 'Wind_Offshore', \n",
    "                                                    capacity_col='capacity_awarded', indexed_by_years=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add exogeneous assumptions for development of RES value applied\n",
    "\n",
    "The estimates are based on the latest study from Fraunhofer ISE:\n",
    "\n",
    "Fraunhofer ISE (2021): Stromgestehungskosten Erneuerbare Energien , Freiburg, June 2021.\n",
    "\n",
    "The study in turn uses a learning rate approach based on the following sources for wind capacity projections:\n",
    "* GWEC (2016): Global Wind Energy Outlook 2016. Global Wind Energy Council.\n",
    "* IRENA (2021b): World Energy Transitions Outlook: 1.5°C Pathway. International Atomic Energy Agency. Abu Dhabi.\n",
    "\n",
    "The following learning curve formula is used to derive capex for future years:\n",
    "$$ C(x_t)=C(x_0)(\\frac{x_t}{x_0})^{-b} $$\n",
    "with\n",
    "$$ LR = 1-2^{-b} $$\n",
    "\n",
    "The corresponding capacities have been extracted from the plot in the ISE study on p. 22 using the [WebPlotDigitizer](https://automeris.io/WebPlotDigitizer/) open source tool.\n",
    "\n",
    "Steps applied:\n",
    "* Read in worldwide wind and PV capacity installation projections\n",
    "* Read in the cost information for RES LCOE calculation\n",
    "* Calculate exponents for the learning curve cost development projection\n",
    "* Determine factors for costs by applying the learning curve formulat\n",
    "* Calculate capex and opex using the learning curve quotient\n",
    "* Derive LCOEs by combining time-dependent capex with opex and other parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_capacity_development = pd.read_csv(\n",
    "    main_path[\"inputs\"] + sub_path[\"renewables\"] + input_file[\"RES_market_development\"],\n",
    "    index_col=0, header=[0, 1], encoding=\"ISO 8859-1\", sep=\";\", decimal=\".\").drop(\n",
    "    columns=\"source\").T\n",
    "\n",
    "res_capacity_development.columns = res_capacity_development.columns.astype(str)\n",
    "res_capacity_development.index.names = [\"energy_carrier\", \"parameter\"]\n",
    "\n",
    "res_costs = pd.read_csv(\n",
    "    main_path[\"inputs\"] + sub_path[\"renewables\"] + input_file[\"RES_costs_ISE21\"],\n",
    "    index_col=None, header=0, encoding=\"ISO 8859-1\", sep=\";\").drop(\n",
    "    columns={\"unit\", \"source\"})\n",
    "\n",
    "res_costs = res_costs.fillna(method=\"ffill\")\n",
    "res_costs = res_costs.set_index([\"energy_carrier\", \"parameter\"])\n",
    "res_costs.rename(index={\"solarPV_open_area\": \"solarPV\"}, inplace=True)\n",
    "res_costs = res_costs.loc[res_costs.index.get_level_values(0).isin(RES_to_sep)]\n",
    "\n",
    "for ec in RES_to_sep:\n",
    "    res_costs.at[(ec, \"exponent\"), \"2021\"] = -np.log(1 - res_costs.at[(ec, \"LR\"), \"2021\"] / 100) / np.log(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_cost_data = res_capacity_development.append(res_costs).T\n",
    "res_cost_data = res_cost_data.fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
    "\n",
    "for ec in RES_to_sep:\n",
    "    res_cost_data[(ec, \"cost_factor_low\")] = (\n",
    "        (res_cost_data[(ec, \"high\")].div(res_cost_data.at[\"2021\", (ec, \"high\")])) \n",
    "         ** -res_cost_data[(ec, \"exponent\")])\n",
    "    res_cost_data[(ec, \"cost_factor_middle\")] = (\n",
    "        (res_cost_data[(ec, \"middle\")].div(res_cost_data.at[\"2021\", (ec, \"middle\")])) \n",
    "         ** -res_cost_data[(ec, \"exponent\")])\n",
    "    res_cost_data[(ec, \"cost_factor_high\")] = (\n",
    "        (res_cost_data[(ec, \"low\")].div(res_cost_data.at[\"2021\", (ec, \"low\")])) \n",
    "         ** -res_cost_data[(ec, \"exponent\")])\n",
    "    res_cost_data[(ec, \"capex_low\")] = (\n",
    "        res_cost_data[(ec, \"capex_low\")].mul(res_cost_data[(ec, \"cost_factor_low\")]))\n",
    "    res_cost_data[(ec, \"capex_middle\")] = (\n",
    "        res_cost_data.at[\"2021\", (ec, \"capex_low\")] * res_cost_data[(ec, \"cost_factor_middle\")])\n",
    "    res_cost_data[(ec, \"capex_high\")] = (\n",
    "        res_cost_data[(ec, \"capex_high\")].mul(res_cost_data[(ec, \"cost_factor_high\")]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LCOE_ISE = eeg.estimate_lcoe_ise21(res_cost_data, RES_to_sep)\n",
    "\n",
    "LCOE_windonshore_ISE = LCOE_ISE.loc[(LCOE_ISE.index >= annual_cap_expansion.at['windonshore', 'start_year']), \n",
    "                                    LCOE_ISE.columns.get_level_values(0) == \"windonshore\"].droplevel(0, axis=1)\n",
    "LCOE_windoffshore_ISE = LCOE_ISE.loc[(LCOE_ISE.index >= annual_cap_expansion.at['windoffshore', 'start_year']), \n",
    "                                    LCOE_ISE.columns.get_level_values(0) == \"windoffshore\"].droplevel(0, axis=1)\n",
    "LCOE_solarPV_ISE = LCOE_ISE.loc[(LCOE_ISE.index >= annual_cap_expansion.at['solarPV', 'start_year']), \n",
    "                                 LCOE_ISE.columns.get_level_values(0) == \"solarPV\"].droplevel(0, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine the split of the annual capacity to be installed for PV, i.e. the amount installed in tenders under the market premium scheme resp. the remainder installed under the FIT regime:\n",
    "* For 2021 and 2022, the capacities from the tender procedure are given.\n",
    "* For the remaining years, the volumes are taken from the [EEG 2021](http://www.gesetze-im-internet.de/eeg_2014/index.html).\n",
    "    * In § 28a EEG 2021 the tender volumes for solarPV are given.\n",
    "    * The volumes for 2022 have been increased by a [decision from the German Bundestag from 24.06.2021](https://www.clearingstelle-eeg-kwkg.de/sites/default/files/2021-06/210625_BR_Entwurf_Gesetzesbeschluss_Wasserstoff_BR-Drs_578-21.pdf).\n",
    "    * These will be separated into two segments. A second segments of larger rooftop and buildings PV will be eligible for market premium payments as well.\n",
    "    * For solarPV tenders, it is assumed that plants will be operational two years from the tender year. Hence, there is a shift by twor years in the dict defining the tender capacities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capacity_split_solarPV = pd.DataFrame(index=pv_capacities_agg.index.union(LCOE_solarPV_ISE.index), \n",
    "                                      columns=['MP', 'FIT'])\n",
    "capacity_split_solarPV.loc[:,'FIT'] = annual_cap_expansion.at['solarPV', 'capacity'] * 1000\n",
    "\n",
    "EEG_2021_tenders_dict = {2023: 1850+300,\n",
    "                         2024: 3600+2300,\n",
    "                         2025: 1650+350,\n",
    "                         2026: 1650+350,\n",
    "                         2027: 1650+400,\n",
    "                         2028: 1550+400,\n",
    "                         2029: 1550+400,\n",
    "                         2030: 1550+400}\n",
    "\n",
    "capacity_split_solarPV.loc[2023:2030, 'MP'] = pd.Series(EEG_2021_tenders_dict).values * 1000\n",
    "capacity_split_solarPV.loc[2023:2030, 'FIT'] = capacity_split_solarPV.loc[2023:2030, 'FIT'].sub(\n",
    "    capacity_split_solarPV.loc[2023:2030, 'MP'])\n",
    "\n",
    "for iter_year in pv_capacities_agg.index.values:\n",
    "    capacity_split_solarPV.at[iter_year, 'FIT'] = pv_capacities_agg.at[iter_year, 'capacity'] * 1000\n",
    "    capacity_split_solarPV.at[iter_year, 'MP'] = solarPV_commissionings.at[iter_year, 'capacity_awarded']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the new-built plants to the data set:\n",
    "* Create a capacity distribution determining the (capacity) distribution for the LCOE estimates.\n",
    "* Add the plants groups to the DataFarme containing new-built RES units and their corresponding values applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onshore_cap_distribution = eeg.create_cap_distribution_from_beta(\n",
    "    LCOE_windonshore_ISE, cols=['LCOE_low', 'LCOE_middle', 'LCOE_high'])\n",
    "offshore_cap_distribution = eeg.create_cap_distribution_from_beta(\n",
    "    LCOE_windoffshore_ISE, cols=['LCOE_low', 'LCOE_middle', 'LCOE_high'])\n",
    "solarPV_cap_distribution = eeg.create_cap_distribution_from_beta(\n",
    "    LCOE_solarPV_ISE, cols=['LCOE_low', 'LCOE_middle', 'LCOE_high'])\n",
    "\n",
    "annual_cap_expansion[\"capacity\"] = annual_cap_expansion[\"capacity\"].mul(1000)  # Convert MW to kW\n",
    "\n",
    "new_eeg_pps_de_windonshore = eeg.add_new_built_res(onshore_cap_distribution, new_eeg_pps_de_windonshore, \n",
    "                                                    annual_cap_expansion, 'Wind_Onshore', indexed_by_years=False)\n",
    "new_eeg_pps_de_windoffshore = eeg.add_new_built_res(offshore_cap_distribution, new_eeg_pps_de_windoffshore, \n",
    "                                                    annual_cap_expansion, 'Wind_Offshore', indexed_by_years=False)\n",
    "new_eeg_pps_de_solarPV = eeg.add_new_built_res(solarPV_cap_distribution, new_eeg_pps_de_solarPV, \n",
    "                                               capacity_split_solarPV, 'Solar', capacity_col='MP',\n",
    "                                               indexed_by_years=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add solarPV units which are to be installed under the FIT scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iter_year in capacity_split_solarPV.index.values:\n",
    "    new_eeg_pps_de_solarPV.loc['solar_PV_FIT_'+str(iter_year),\n",
    "                                ['capacity', 'energy_source', \n",
    "                                'support_scheme', 'commissioning_year']] = (\n",
    "            [capacity_split_solarPV.at[iter_year, 'FIT'], 'Solar', 'FIT', iter_year])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EEG transformers with negative costs (clustering)\n",
    "German EEG power plants within the market premium model are modeled as transformers to display negative power prices. (see [section above](#Extensions-for-Germany) for a description of the underlying logic)\n",
    "\n",
    "Steps applied here:\n",
    "- Create a common data set by appending the artificial new built units to the RES data set.\n",
    "- Drop new-plants that are to be commissioned after the target year.\n",
    "- Cluster RES power plants by market values and prepare them for usage in _POMMES_ (see function `create_ee_transformers` for details)\n",
    "- Add RES buses needed for the modelling approach\n",
    "- Set RES operation costs as negative value of average market premium of cluster\n",
    "- Write RES data to a csv and to and Excel file\n",
    "- Write the costs to a separate file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all data sets\n",
    "eeg_pps_de = pd.concat([eeg_pps_de, new_eeg_pps_de_solarPV,\n",
    "                        new_eeg_pps_de_windonshore, new_eeg_pps_de_windoffshore])\n",
    "\n",
    "# Drop plants that are commissioned prior to the target year\n",
    "eeg_pps_de[\"commissioning_year\"] = eeg_pps_de[\"commissioning_year\"].astype(\"int32\")\n",
    "eeg_pps_de = eeg_pps_de.loc[eeg_pps_de[\"commissioning_year\"] <= year]\n",
    "\n",
    "ee_agg = eeg.create_ee_transformers(eeg_pps_de, cluster_no=eeg_clusters_per_technology)\n",
    "ee_agg['country'] = 'DE'\n",
    "ee_agg['to_el'] = 'DE_bus_el'\n",
    "ee_agg['efficiency_el'] = 1\n",
    "buses.extend(ee_agg['from'].unique())\n",
    "\n",
    "# seperate costs and nodes\n",
    "costs_operation_RES = -ee_agg['value_applied'].to_frame().rename(columns={'value_applied': 'costs'})\n",
    "\n",
    "ee_agg.to_csv(\n",
    "    main_path[\"outputs\"] + output_file[\"transformers_renewables\"] + \"_\" + str(year) + \".csv\"\n",
    ")\n",
    "ee_agg.to_excel(writer, sheet_name='transformers_renewables' + \"_\" + str(year))\n",
    "costs_operation_RES.to_csv(\n",
    "    main_path[\"outputs\"] + output_file[\"costs_operation_renewables\"] + \"_\" + str(year) + \".csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "installed_vres_capacities = (\n",
    "    eeg_pps_de.loc[eeg_pps_de[\"energy_source\"].isin(\n",
    "        [\"Solar\", \"Wind_Onshore\", \"Wind_Offshore\"])].groupby(\"energy_source\").agg({\"capacity\": \n",
    "                                                                                   \"sum\"}).div(1000).round(1)\n",
    ")\n",
    "\n",
    "installed_vres_capacities.rename(dict_eeg_pps_names, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_TODO: Find and add missing PV capacity!_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "installed_vres_capacities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do some garbage collection\n",
    "try:\n",
    "    del (DK_cap, DK_val_2030, EEG_2021_tenders_dict, LCOE_ISE, LCOE_solarPV_ISE, LCOE_windoffshore_ISE,\n",
    "         LCOE_windonshore_ISE, RES_to_sep, PV_cap_distribution,\n",
    "         all_sources_res, annual_cap_expansion,\n",
    "         capacity_split_solarPV, cap_needed_until_2030, col, common_tenders,\n",
    "         costs_operation_RES, dict_eeg_pps_names, dict_tender_cols,\n",
    "         diff_ix, ec, eeg_clusters_per_technology, eeg_de, eeg_pps_de,\n",
    "         eeg_pps_de_agg, eeg_pps_de_agg_by_year, eeg_pps_de_solarPV,\n",
    "         eeg_pps_de_windoffshore, eeg_pps_de_windonshore, \n",
    "         iter_year, k, keep_constant_ix, missing_res_ts_dict, new_eeg_pps_de_solarPV,\n",
    "         new_eeg_pps_de_windoffshore, new_eeg_pps_de_windonshore, \n",
    "         onshore_cap_distribution, offshore_cap_distribution,\n",
    "         onshore_tenders, pv_cap_dict, pv_cap_names_dict, pv_capacities,\n",
    "         pv_capacities_agg, pv_col_names, renewables_eu_2025_BEST, renewables_eu_2030_DG,\n",
    "         renewables_eu_abs_capacities, renewables_eu_capacities, res_DE_EEG_2021,\n",
    "         res_DE_Prognos, res_de_projection, res_capacities,\n",
    "         res_capacity_projection, res_capacity_projections, res_de_capacity_development,\n",
    "         res_cost_data, solarPV_cap_distribution, solarPV_commissionings, \n",
    "         solarPV_tenders, sources_RES_cap, sources_RES_original, sources_hydro,\n",
    "         sources_nonfluc, sources_res, sources_ts_res, ts_res, usecols, v, value,\n",
    "         windoffshore_commissionings, windonshore_commissionings, years_for_iteration)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers and commodity sources for the target year\n",
    "\n",
    "## Add new-built power plants by assumption\n",
    "**Background and approach**:\n",
    "\n",
    "* A capacity estimation for 2030 showed that large amounts of capacities are about to be decommissioned whereas there is a limited number of plans for new-built power plants and the estimate of the TSOs from the NEP 2018 is rather conservative (i. e. not considering a coal phase-out and assuming fewer plants to go offline).\n",
    "* In order not to provoke situations with a loss of load (LOL), the idea is to introduce backup power plants for levelling out the system balance for future target years.\n",
    "* The scheme of the capacity balance (sheet) is used here. The latest TSO estimations covering 2018-2022 are used as a rough guideline. The scenario with a coal phase out is used. It shows that for 2021 with around 72 GW of installed capacity not including network reserve and security backup (of coal power plants), the remaining margin of 2 GW is small but sufficient. Hence, it is assumed that installed capacity has to add up to 72 GW in order to meet the peak demand.\n",
    "* Accordignly, the difference between the overall capacity given here for 2030 (roughly 57 GW + 80% of phes and 28% of ROR capacities, 60% of biomass capacities and 1% of wind capacities) and the needed 72 GW is filled up with gas power plants.\n",
    "* The lacking capacity is evenly distributed between gas turbines and CCGT power plants.\n",
    "\n",
    "ÜNB (2020): Bericht zur Leistungsbilanz, https://www.netztransparenz.de/portals/1/Bericht_zur_Leistungsbilanz_2019.pdf, accessed 14.11.2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap_requirement = 72000\n",
    "\n",
    "# Add all capacity credits to calculate secured capcity\n",
    "secured_cap_target_year = remaining_capacity_conv + conv_de_new.capacity.sum()\n",
    "secured_cap_target_year += 0.28 * ror_de_agg.at['DE_source_ROR', 'capacity'] + 0.8 * phes_de.capacity.sum()\n",
    "secured_cap_target_year += 0.6 * sources_RES.at['DE_source_biomassEEG', 'capacity']\n",
    "secured_cap_target_year += 0.01 * installed_vres_capacities.loc[['windonshore', 'windoffshore'], 'capacity'].sum()\n",
    "\n",
    "missing_cap_target_year = np.max([cap_requirement - secured_cap_target_year, 0])\n",
    "print(f'Capacity to be newly provided by additional gas power plants in {year}: {missing_cap_target_year:,.2f} MW')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers_new_built = pd.read_csv(\n",
    "    main_path[\"inputs\"] + sub_path[\"pp_er\"] + input_file[\"new_built\"], \n",
    "    sep=\";\", decimal=\",\", index_col=0\n",
    ")\n",
    "\n",
    "transformers_new_built['capacity'] = missing_cap_target_year/2\n",
    "\n",
    "if transformers_new_built.capacity.sum() == 0:\n",
    "    pass\n",
    "else:\n",
    "    conv = conv.append(transformers_new_built)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster transformers (optionally)\n",
    "> _NOTE: This section has been added to create a reduced transformers data for Germany set for quicker, but less precise simulation runs._\n",
    "\n",
    "Basic steps (only if cluster control variable is True):\n",
    "* Slice subset for German power plants\n",
    "* Cluster by electrical efficiency\n",
    "* Group data based on defined rules\n",
    "* Assign a new index to clustered data\n",
    "* Remove detailled data from transformer input sheet and append clustered one instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cluster_transformers_DE:\n",
    "    conv_de = conv.loc[conv['country'] == \"DE\"]\n",
    "    conv_de_clustered = tf_agg.cluster_transformers(\n",
    "        conv_de, \n",
    "        by='efficiency_el', \n",
    "        share_clusters=0.1)\n",
    "    \n",
    "    grouping_cols = ['cluster']\n",
    "    mean_cols = ['efficiency_el', 'max_load_factor', 'part_load_losses',\n",
    "                 'min_uptime', 'min_downtime', 'cold_startup_time', 'warm_startup_time',\n",
    "                 'hot_startup_time', 'cold_startup_costs', 'warm_startup_costs', \n",
    "                 'hot_startup_costs', 'grad_pos', 'grad_neg']\n",
    "    sum_cols = ['capacity', 'outages']\n",
    "    cols_considered = grouping_cols + mean_cols + sum_cols\n",
    "\n",
    "    other_cols = [col for col in conv.columns if col not in cols_considered]\n",
    "\n",
    "    conv_de_clustered = tf_agg.group_power_plants(\n",
    "        conv_de_clustered, \n",
    "        grouping_cols, \n",
    "        mean_cols, \n",
    "        sum_cols, \n",
    "        other_cols)\n",
    "\n",
    "    conv_de_clustered['label'] = ('transformer_' \n",
    "                                  + conv_de_clustered['fuel'] \n",
    "                                  + '_' + conv_de_clustered['type']\n",
    "                                  + '_cluster_' \n",
    "                                  + conv_de_clustered['cluster'].apply(str))\n",
    "    conv_de_clustered.set_index('label', inplace=True)\n",
    "    \n",
    "    conv_clustered = conv.drop(conv_de.index)\n",
    "    conv_clustered = conv_clustered.append(conv_de_clustered[conv.columns])\n",
    "    conv_clustered['identifier'] = np.nan\n",
    "    \n",
    "    conv_clustered = conv_clustered.round(4)\n",
    "    conv_clustered.to_csv(\n",
    "        main_path[\"outputs\"] + 'transformers_clustered' + \"_\" + str(year) + \".csv\"\n",
    "    )\n",
    "    conv_clustered.to_excel(writer, sheet_name='conv_clustered' + \"_\" + str(year))\n",
    "    \n",
    "    del (cols_considered, conv_clustered, conv_de_clustered,\n",
    "         grouping_cols, mean_cols, other_cols, sum_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write power plants data to csv and Excel\n",
    "\n",
    "Steps applied:\n",
    "* Write data to csv\n",
    "* Write data to Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = conv.round(4)\n",
    "conv.to_csv(\n",
    "    main_path[\"outputs\"] + output_file[\"transformers\"] + \"_\" + str(year) + \".csv\"\n",
    ")\n",
    "conv.to_excel(writer, sheet_name='conv' + \"_\" + str(year))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commodity sources\n",
    "\n",
    "Steps applied:\n",
    "- Read in emission factors data\n",
    "- Extract necessary information from conventional power plants data set, i.e. country, fuel and information on commodity bus (commodity source feeds into that bus; transformer receives energy from that bus)\n",
    "- Add emission factor information and rename\n",
    "- Store the results to csv and to Excel\n",
    "- Add commodity bus information on the buses data set\n",
    "- Set costs for carbon (hard-coded) and write that cost data\n",
    "\n",
    "Data sources used:\n",
    "- Emission factors: UBA (2019). Entwicklung der spezifischen Kohlendioxid-Emissionen des deutschen Strommix in den Jahren 1990 - 2018, Dessau-Roßlau, pp. 16, 28\n",
    "- Natural gas: BAFA (2019). Aufkommen und Export von Erdgas sowie die Entwicklung der Grenzübergangspreise ab 1991"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emission_factors = pd.read_csv(\n",
    "    main_path[\"inputs\"] + sub_path[\"assumptions\"] + input_file[\"emf\"],\n",
    "    sep=\";\", decimal=\",\"\n",
    ")\n",
    "\n",
    "sources_commodity = conv.reset_index()[['country', 'fuel', 'from']].drop_duplicates(subset='from')\n",
    "sources_commodity['label'] = sources_commodity['country'] + '_source_' + sources_commodity['fuel']\n",
    "sources_commodity = sources_commodity.merge(emission_factors, on='fuel').drop(columns='fuel').set_index('label')\n",
    "sources_commodity.rename(columns={'from': 'to'}, inplace=True)\n",
    "\n",
    "sources_commodity.to_csv(\n",
    "    main_path[\"outputs\"] + output_file[\"sources_commodity\"] + \"_\" + str(year) + \".csv\"\n",
    ")\n",
    "sources_commodity.to_excel(writer, sheet_name='sources_commodity' + \"_\" + str(year))\n",
    "\n",
    "buses.extend(sources_commodity['to'].values)\n",
    "\n",
    "costs_carbon = pd.DataFrame(index=sources_commodity.index, columns=range(2015,2050), data=5.8)\n",
    "costs_carbon.to_csv(\n",
    "    main_path[\"outputs\"] + output_file[\"costs_carbon\"] + \"_\" + str(year) + \".csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do some garbage collection\n",
    "try:\n",
    "    del (cap_requirement, cluster_transformers_DE,\n",
    "         conv, conv_de, conv_de_new, costs_carbon, emission_factors, \n",
    "         installed_vres_capacities, missing_cap_target_year, \n",
    "         phes_de, remaining_capacity_conv, ror_de_agg, secured_cap_target_year,\n",
    "         sources_RES, transformers_new_built)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sinks\n",
    "\n",
    "In this section, **demand** data is put together.<br>\n",
    "In [oemof.solph](https://github.com/oemof/oemof-solph) demand can be represented through (generic) so called \"Sink\" elements which have one input and no output. In addition to the demand sinks per bidding zone, excess sinks are created to model an overall generation surplus which cannot be consumed or stored within the area considered. In reality this would be exports to other European countries or result in curtailments. The excess sinks are needed for ensuring model feasibility.\n",
    "\n",
    "## Electricity Demand\n",
    "\n",
    "Currently the time series for electricity demand is created in the RES section, since the time series are loaded there.\n",
    "\n",
    "Steps applied:\n",
    "- Add bus information and label for demand\n",
    "- Normalize demand time series by determining peak demand and dividing the demand timeseries through that value\n",
    "- Write demand values (i.e. maximum demands) and demand time series to csv and to Excel files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sinks_demand_el = pd.DataFrame(data={'country': countries_modeled})\n",
    "sinks_demand_el['from'] = sinks_demand_el['country'] + '_bus_el'\n",
    "sinks_demand_el['label'] = sinks_demand_el['country'] + '_sink_el_load'\n",
    "sinks_demand_el.set_index('label', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_eu_el_demand = ts_eu.filter(regex = 'sink_el')\n",
    "peak_demand = ts_eu_el_demand.max()\n",
    "sinks_demand_el.loc[peak_demand.index, 'maximum'] = peak_demand\n",
    "\n",
    "sinks_demand_el_ts = ts_eu_el_demand/peak_demand\n",
    "\n",
    "sinks_demand_el['maximum'] = sinks_demand_el['maximum'].astype(int)\n",
    "sinks_demand_el_ts = sinks_demand_el_ts.round(4).astype(np.float32)\n",
    "\n",
    "# interpolate NaN values (Czech and France)\n",
    "sinks_demand_el_ts = sinks_demand_el_ts.interpolate(method='linear')\n",
    "sinks_demand_el_ts = tools.reindex_time_series(sinks_demand_el_ts, year)\n",
    "\n",
    "sinks_demand_el.to_csv(\n",
    "    main_path[\"outputs\"] + output_file[\"sinks_demand_el\"] + \"_\" + str(year) + \".csv\"\n",
    ")\n",
    "sinks_demand_el_ts.to_csv(\n",
    "    main_path[\"outputs\"] + output_file[\"sinks_demand_el_ts\"] + \"_\" + str(year) + \".csv\"\n",
    ")\n",
    "sinks_demand_el.to_excel(writer, sheet_name='sinks_demand_el' + \"_\" + str(year))\n",
    "sinks_demand_el_ts.tz_localize(None).to_excel(writer, sheet_name='sinks_demand_el_ts' + \"_\" + str(year))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excess Sinks\n",
    "Steps applied:\n",
    "- Define and parameterize electricity excess sink per bidding zone\n",
    "- Introduce high excess costs (penalty) costs in order not to produce to much excess\n",
    "- Concatenate the electricity excess Sinks with the reservoir spillage Sinks\n",
    "- Drop the German electricity excess Sink since it is explicitly modeled in _POMMES_\n",
    "- Write the Sinks data to a csv and an Excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sinks_excess = pd.DataFrame(data = {'country': countries_modeled})\n",
    "sinks_excess['label'] = sinks_excess['country'] + '_sink_el_excess'\n",
    "sinks_excess['from'] = sinks_excess['country'] + '_bus_el'\n",
    "sinks_excess['excess_costs'] = 1000000\n",
    "sinks_excess.set_index('label', inplace = True)\n",
    "\n",
    "sinks_excess = pd.concat([sinks_excess, sinks_reservoir_spillage])\n",
    "# The German electricity excess Sink is explicitly modeled in the model\n",
    "sinks_excess.drop(index='DE_sink_el_excess', inplace=True)\n",
    "sinks_excess.to_csv(\n",
    "    main_path[\"outputs\"] + output_file[\"sinks_excess\"] + \"_\" + str(year) + \".csv\"\n",
    ")\n",
    "sinks_excess.to_excel(writer, sheet_name='sinks_excess' + \"_\" + str(year))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do some garbage collection\n",
    "try:\n",
    "    del (countries_modeled, peak_demand, sinks_demand_el, sinks_demand_el_ts, sinks_excess,\n",
    "         sinks_reservoir_spillage, ts_eu, ts_eu_el_demand)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Buses\n",
    "\n",
    "ave all buses elements which were already created above to csv and to Excel\n",
    "Links to sections were buses were created resp. adde:\n",
    "- [electricity buses](#Introduce-electricity-buses) &rarr; instanciation of buses data set\n",
    "- [hydro buses](#Prepare-data-for-usage-in-oemof.solph)\n",
    "- [commodity buses](#Commodity-sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buses_df = pd.DataFrame(index=buses)\n",
    "buses_df['country'] = [_[0] for _ in buses_df.index.str.split('_')]\n",
    "buses_df.index.name = 'label'\n",
    "buses_df.to_csv(\n",
    "    main_path[\"outputs\"] + output_file[\"buses\"] + \"_\" + str(year) + \".csv\"\n",
    ")\n",
    "buses_df.to_excel(writer, sheet_name='buses' + \"_\" + str(year))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save all DataFrames into one Excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the Pandas Excel writer and output the Excel file.\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Costs\n",
    "Costs are read in from raw sheets and just spreaded here in order to fit the countries used.\n",
    "\n",
    "Cost categories considered:\n",
    "- OPEX for conventional power plants\n",
    "- Ramping costs\n",
    "- Fuel costs (commodity only)\n",
    "- Storage OPEX\n",
    "\n",
    "Steps applied for each of the cost categories:\n",
    "- Read in the raw data\n",
    "- Match index with data set (in most cases conventional power plants)\n",
    "- Round values\n",
    "- Save results to a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources_commodity['fuel'] = [_[1] for _ in sources_commodity['to'].str.rsplit('_', 1)]\n",
    "sources_commodity['technology'] = [_[1] for _ in sources_commodity['to'].str.rsplit('_', 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opex = pd.read_csv(\n",
    "    main_path[\"inputs\"] + sub_path[\"costs\"] + input_file[\"operation_costs\"], \n",
    "    index_col=0, sep=\";\", decimal=\",\"\n",
    ")\n",
    "\n",
    "costs_opex = opex.loc[sources_commodity['fuel'].values]\n",
    "costs_opex.index = sources_commodity['to'].values\n",
    "costs_opex.index.name = 'label'\n",
    "\n",
    "costs_opex = costs_opex.round(2).astype(np.float32)\n",
    "\n",
    "costs_opex.to_csv(\n",
    "    main_path[\"outputs\"] + output_file[\"costs_operation\"] + \"_\" + str(year) + \".csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ramping = pd.read_csv(\n",
    "    main_path[\"inputs\"] + sub_path[\"costs\"] + input_file[\"ramping_costs\"], \n",
    "    index_col=0, sep=\";\", decimal=\",\"\n",
    ")\n",
    "\n",
    "costs_ramping = ramping.loc[sources_commodity['fuel'].values]\n",
    "costs_ramping.index = sources_commodity['to'].values\n",
    "costs_ramping.index.name = 'label'\n",
    "costs_ramping.columns = costs_ramping.columns.astype(int)\n",
    "\n",
    "# add fRES buses\n",
    "costs_ramping = pd.concat([costs_ramping, \n",
    "                           pd.DataFrame(index=ee_agg['from'].unique(), columns=range(2015,2051), data=0)])\n",
    "\n",
    "costs_ramping.to_csv(\n",
    "    main_path[\"outputs\"] + output_file[\"costs_ramping\"] + \"_\" + str(year) + \".csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fuel = pd.read_csv(\n",
    "    main_path[\"inputs\"] + sub_path[\"costs\"] + input_file[\"middle_fuel_costs\"], \n",
    "    index_col=0, sep=\";\", decimal=\",\"\n",
    ")\n",
    "\n",
    "costs_fuel = fuel.loc[sources_commodity['technology'].values]\n",
    "costs_fuel.index = sources_commodity.index\n",
    "costs_fuel.index.name = 'label'\n",
    "\n",
    "costs_fuel = costs_fuel.round(2).astype(np.float32)\n",
    "costs_fuel.to_csv(\n",
    "    main_path[\"outputs\"] + output_file[\"costs_fuel_middle\"] + \"_\" + str(year) + \".csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opex_storages = pd.DataFrame(index=storages_el.index, columns=range(2015,2051), data=1)\n",
    "opex_storages.to_csv(\n",
    "    main_path[\"outputs\"] + output_file[\"costs_operation_storages\"] + \"_\" + str(year) + \".csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do some garbage collection\n",
    "try:\n",
    "    del (buses, buses_df, costs_fuel, costs_opex, costs_ramping, ee_agg, fuel,\n",
    "         opex, opex_storages, ramping, sources_commodity, storages_el)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "1086px",
    "left": "34px",
    "top": "111.133px",
    "width": "610.533px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 923.85,
   "position": {
    "height": "945.85px",
    "left": "1932px",
    "right": "20px",
    "top": "129px",
    "width": "583px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
